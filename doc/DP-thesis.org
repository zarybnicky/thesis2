#+BEGIN_COMMENT
Normostrana = 1800c ≈ 250 slov
org-word-count
org-wc-display

16.4. 2303w
17.4. 3023w
18.4. 3600w? (Σ36p)
19.4. 4000w? (Σ40p)
22.4. 5200w
24.4. 6700w
25.4. 7200w (Σ56p)
26.4. 7660w (Σ59p)
27.4. 8800w (Σ64p)
28.4. 10050w (Σ67p)
29.4. 11700w (Σ68p)
30.4. 12500w (Σ74p, 52p content)
3.5. 14000w (Σ81p, Σ58p content)
5.5. 14500w (Σ79p)
6.5. 15200w (Σ77p)
10.5. 18800w (Σ81p)
11.5. 20900w

minimum: 12,500
target: 17,500
maximum: 30,000
#+END_COMMENT
* (front matter)                                              :ignoreheading:
#+LANGUAGE: en
#+OPTIONS: texht:nil toc:nil author:nil ':t H:6 num:3
#+LATEX_CLASS: fitthesis
#+LATEX_CLASS_OPTIONS: [english,zadani,cprint,odsaz]
#+EXCLUDE_TAGS: noexport
# print = B&W links and logo
# cprint = B&W links, color logo
#+BIND: org-latex-title-command ""
#+BIND: org-latex-prefer-user-labels t
#+BIND: org-latex-default-figure-position "hbt"
#+BEGIN_EXPORT latex
\maketitle
\setlength{\parskip}{0pt}
{\hypersetup{hidelinks}\tableofcontents}
\iftotalfigures\listoffigures\fi
\iftotaltables\listoftables\fi
\iftotallistings\listoflistings\fi
\iftwoside\cleardoublepage\fi
\setlength{\parskip}{0.5\bigskipamount}n
#+END_EXPORT
* Bleh                                                             :noexport:
- cite:altenkirch08_pisigma, cite:altenkirch10_pisigma - "I had discovered the ΠΣ paper when finishing my thesis: too late, unfortunately"
- cite:eisenberg20_stitch is an interesting tutorial of a dependent interpreter of dependent languages
- cite:juan20_unif_thesis - well described contexts + language specification - can I take as inspiration?
- well investigated in cite:lindley05_nbe_sml where there is a comprehensive of
  NbE techniques as applied to ML
- also in cite:lindley05_nbe_sml there is a treatment of η reduction/expansion - READ

GHC, Clojure, Scala, OCaml, ML, Eta, Frege, ?

hyperfine to benchmark - measures speed (what about ~prof~?)

Krun benchmark runner + its warmup_stats functionality for statistical analysis
of steady states, number of iterations it took to stabilize.

memory profile from GHC's RTS for agda/idris/smalltt (+RTS -p) (what about
coq? - https://github.com/coq/coq/blob/master/dev/doc/profiling.txt)

Graal's default memory profiler

* Introduction
#+COMMENT: Motivation: why this project? Why should anyone care?

Dependently-typed languages allow programmers to write correct-by-construction
code, and they can be used as theorem provers, or proof assistants. Programs
written in such languages can encode more properties than those written without
dependent types, and they are considered to be one approach to formal software
verification cite:norell08_agda_tutorial: a well-known example is the
computational proof of the four-color theorem in the theorem prover Coq in 2005
cite:gonthier08_fourcolor.

However, dependently-typed languages rely on their compilers or interpreters to
verify, or prove, all invariants (properties) encoded in a program, which
involves significant computational effort. When applied to problems or systems
on a large-enough scale, type-checking performance becomes the primary obstacle
to their use cite:gross14_coq_experience,gross21_performance. While many of the
performance issues are fundamentally algorithmic cite:nawaz19_survey_provers, a
better runtime system would improve the rest.

In recent years, there have been several investigations into the performance of
dependently-typed languages: Jason Gross's work on improving the performance of
Coq cite:gross14_coq_experience,gross21_performance; the work of András Kovács
on performant Haskell-based interpreters cite:kovacs_norm,kovacs20_implicit;
Edwin Brady's work on the Idris 2 runtime system based on Chez Scheme
cite:brady21_idris. Kovacs, in particular, manages to outperform both Coq and
Agda by a large margin in the SmallTT project cite:smalltt.

However, custom runtime systems or capable optimizing compilers are
time-consuming to build and maintain. This thesis seeks to answer the question
of whether just-in-time compilation can help to improve the performance of such
systems. Moving from custom runtime systems to general language platforms like
e.g., the Java Virtual Machine (JVM) or RPython cite:bolz14_meta, has improved
the performance of several dynamic languages: projects like TruffleRuby, FastR,
or PyPy. It has allowed these languages to re-use the optimization machinery
provided by these platforms, improve their performance, and simplify their
runtime systems.

The platform to be evaluated is GraalVM and the Truffle language implementation
framework, which reuse and improve upon the JIT capabilities of the Java Virtual
Machine. Truffle has been used to implement an improved runtime system for a
number of general-purpose languages, the most prominent of which are TruffleRuby
and FastR. In both cases, replacing a custom runtime system with a JIT-based one
resulted in significant performance improvements[fn:7][fn:8].

In the final stages of this thesis, I have encountered a single project that
attempts to apply JIT compilation to dependent types, there were no other before
this one to the best of my knowledge. This project is Enso cite:enso, a visual
programming language with multi-language polyglot capabilities, that uses
dependent types at its core. I have been able to incorporate and evaluate some
of its improvements into the practical parts of this thesis, despite the time
constraints, as it would otherwise serve as one of my primary sources.

Other than Enso, closest to this project is Cadenza cite:kmett_2019 by Edward
Kmett, who also suggested the topic of this thesis. Cadenza is an implementation
of the simply-typed lambda calculus on the Truffle framework. While it is
unfinished and did not show as promising performance compared to other
simply-typed lambda calculus implementations as its author hoped, this thesis
applies similar ideas to the dependently-typed lambda calculus, where the
presence of compile-time computation should lead to larger gains.

In this thesis, I will evaluate the effect of JIT compilation on the runtime
performance of the type-checking (elaboration) of a dependently-typed language
based on the typed λ-calculus. In particular, the goal is to investigate the
performance of β-normalization and βη-conversion checking. Those are among the
main computational tasks in the elaboration process and they are also the tasks
can most likely benefit from JIT compilation. The obtained results will be
compared between JIT and non-JIT implementations, but also against
state-of-the-art proof assistants: Coq, Agda, Idris.

As there are no standard benchmarks for dependently-typed languages, the first
task is to design a small, dependently-typed core language, followed by an
implementation of an interpreter for this language. Proof assistants use
languages based on the typed λ-calculus at their core, so it is a sufficient
basis for the goals of this thesis.

Truffle makes it possible to incrementally add JIT compilation to an existing
interpreter, using /partial evaluation/ to turn slow interpreter code into
efficient machine code cite:wurthinger13_graal. During partial evaluation, an
interpreter is specialized together with the source code of a program, yielding
executable code: parts of the interpreter could be specialized, some optimized,
and some could be left off entirely, which often results in performance gains of
several orders of magnitude. Having a language interpreter based on Truffle also
brings other benefits: seamless interoperability with Java or JVM-based
languages cite:sipek19_polyglot, or automatically derived language tooling
cite:stolpe19_environment. Regardless of the outcome of the performance
evaluation, using Truffle would benefit dependently-typed or experimental
languages.

Starting from basic λ-calculus theory and building up to the systems of the
λ-cube, we specify the syntax and semantics of a small language
(Chapter ref:lambda). Continuing with the principles of λ-calculus evaluation,
type-checking and elaboration, we implement an interpreter for Montuno in a
functional style (Chapter ref:interpreter). In the second part of the thesis, we
evaluate the capabilities offered by Truffle and the peculiarities of Truffle
languages, and implement a Truffle interpreter for Montuno (Chapter
ref:jit-interpreter). After designing a set of benchmarks to evaluate the
language's performance and discussing then results, we close with a large list of
possible follow-up work (Chapter ref:evaluation).

* Language specification: λ⋆-calculus with extensions
  :PROPERTIES:
  :CUSTOM_ID: lambda
  :END:
#+INCLUDE: "01-spec.org::#lambda" :only-contents t

* Language implementation: Montuno
  :PROPERTIES:
  :CUSTOM_ID: interpreter
  :END:
#+INCLUDE: "02-pure.org::#interpreter" :only-contents t

* Adding JIT compilation to Montuno: MontunoTruffle
  :PROPERTIES:
  :CUSTOM_ID: jit-interpreter
  :END:
#+INCLUDE: "03-truffle.org::#jit-interpreter" :only-contents t

* Evaluation
  :PROPERTIES:
  :CUSTOM_ID: evaluation
  :END:
#+INCLUDE: "04-evaluation.org::#evaluation" :only-contents t

* Conclusion
The first part of this thesis presents the concepts necessary for understanding
and specifying type systems based on the systems of the λ-cube. I have used
these concepts to specify a small, dependently-typed language. The second part
contains an overview of state-of-the-art algorithms involved in creating an
interpreter for a dependently-typed language, and presents my implementation of
such an interpreter in Kotlin.

In the third part of the thesis, I have presented GraalVM, the Truffle language
implementation framework, and the optimization possibilities they provide. I
have implemented a second interpreter for the language using the Truffle
framework, also written in Kotlin. The fourth part contains the compilation of a
small set of benchmarks for investigating the elaboration performance of
dependently-typed languages, and uses them to evaluate the effect of JIT
compilation on the performance of the interpreters, and to coarsely compare
their performance with the performance of state-of-the-art languages.

The results are, however, unsatisfactory. The benefits brought by JIT
compilation are outweighed by the overhead of the implementation on the JVM
platform. Its performance is lacking, compared to platforms like GHC Haskell, or
Chez Scheme that are used in other dependently-typed languages. I believe
further investigation would manage to eliminate most causes of inefficiency, but
compared to the initial expectations, such conclusions disprove the entire
premise of my thesis.

Overall, despite the negative conclusion, I believe this thesis has fulfilled a
large part of its goals. It presents a concise introduction to the concepts
required for implementing a dependently-typed language, and an overview of
the optimization opportunities offered by Truffle, which can both form the
starting point for other projects in this area.


* (bibliography, start of appendix)                           :ignoreheading:

#+BEGIN_EXPORT latex
\makeatletter
\def\@openbib@code{\addcontentsline{toc}{chapter}{Bibliography}}
\makeatother
\begin{flushleft}
#+END_EXPORT

bibliographystyle:bibstyle
bibliography:bibliography.bib

#+BEGIN_EXPORT latex
\end{flushleft}
\iftwoside\cleardoublepage\fi
\appendix
\appendixpage
\iftwoside\cleardoublepage\fi
\startcontents[chapters]
% \setlength{\parskip}{0pt}
% \printcontents[chapters]{l}{0}{\setcounter{tocdepth}{2}}
% \setlength{\parskip}{0.5\bigskipamount}
\iftwoside\cleardoublepage\fi
#+END_EXPORT

* Language specification
   :PROPERTIES:
   :CUSTOM_ID: spec
   :END:
** Syntax
#+label:syntax-recap2
#+caption:Terms and values in Montuno (revisited)
#+attr_latex: :options [htb]
#+begin_figure latex
\[\begin{array}{rclclcl}
term & ≔ & v     & | & constant & & \\
     & | & a b   & | & a \{b\}  &   & \\
     & | & a→b   & | & (a:A)→b  & | & \{a:A\}→b \\
     & | & a × b & | & (l:A)×b  & | & a.l \\
     & | & \text{let} x=v \text{in} e && && \\
     & | & \_ &&&& \\
value& ≔ & constant & | & neutral && \\
     & | & λx:A.b & | & Πx:A.b && \\
     & | & (a₁,⋯,aₙ) &&&& \\
     & | & \_ &&&& \\
neutral & ≔ & var & | & neutral a₁ ...aₙ & | & neutral.lₙ\\
\end{array}\]
#+end_figure

** Grammar
#+include: "../montuno/src/main/antlr/montuno/Montuno.g4" src antlr -n

** Built-in constructs
- Unit : Type
- unit : Unit
- Nat : Type
- zero : Nat
- succ : Nat → Nat
- natElim : {A} → Nat → A → (Nat → A) → A
- Bool : Type
- true : Bool
- false : Bool
- cond : {A} → Bool → A → A → A
- fix : {A} → (A→A) → A
- the : (A) → A → A
** Pre-terms
   :PROPERTIES:
   :CUSTOM_ID: montuno-data
   :END:
#+include: "../montuno/src/main/montuno/syntax/presyntax.kt" src kotlin -n
* Footnotes
[fn:12] https://github.com/AndrasKovacs/normalization-bench 

[fn:8] FastR is between 50 to 85x faster than GNU R, depending on the source. cite:fumero17_jit_gpu

[fn:7] Unfortunately, there are no officially published benchmarks, but a number of articles claim that TruffleRuby is 10-30x faster than the official C implementation. cite:shopify2020

