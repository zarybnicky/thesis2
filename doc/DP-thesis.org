* (front matter)                                              :ignoreheading:
#+LANGUAGE: en
#+OPTIONS: texht:nil toc:nil author:nil ':t
#+LATEX_CLASS: fitthesis
#+LATEX_CLASS_OPTIONS: [english,zadani,odsaz]
# print = B&W links and logo
# cprint = B&W links, color logo
#+BIND: org-latex-title-command ""
#+BIND: org-latex-default-figure-position "tbh"
#+LATEX_HEADER: \usepackage{minted}
#+LATEX_HEADER: \usepackage{tikz}
#+LATEX_HEADER: \usepackage{tikz-cd}
#+LATEX_HEADER: \usepackage{bussproofs}
#+LaTeX_HEADER: \usepackage[figure,table,listing]{totalcount}
#+LaTeX_HEADER: \input{metadata}
#+BEGIN_EXPORT latex
\maketitle
\setlength{\parskip}{0pt}
{\hypersetup{hidelinks}\tableofcontents}
\iftotalfigures\listoffigures\fi
\iftotaltables\listoftables\fi
\iftotallistings\listoflistings\fi
\iftwoside\cleardoublepage\fi
\setlength{\parskip}{0.5\bigskipamount}
#+END_EXPORT

* Změnit název: "Montuno: Efficient elaboration of dependent types using JIT compiler"

* Introduction
** (Old)
When creating small experimental or research languages, writing a compiler may
be too much effort for the expected gain. On the other hand an interpreter is
usually not as performant as its creators may require for more computationally
intensive tasks.

There is a potential third way, proposed by Yoshihiko Futamura in the 1970s,
called the Futamura projection (or partial program evaluation), wherein an
interpreter is specialized in conjunction with the source code of a program,
yielding an executable. Some parts of the interpreter may be specialized, some
optimized, some left off entirely. Depending on the quality of the specializer,
the gains may be several orders of magnitude.

The goal of my thesis is to evaluate whether the GraalVM/Truffle platform is
suitable enough to act as a specializer for functional languages, in particular
for the dependently-typed lambda calculus.  To illustrate in Figure
\ref{fig:futamora}, the question is whether the path \textit{Native
Image\textrightarrow Result} is fast enough compared to the path
\textit{Executable\textrightarrow Result}.

#+BEGIN_EXPORT latex
\begin{figure}
\centering
\begin{tikzcd}
{} & Program
 \arrow[ld, "Compiler" description, bend right]
 \arrow[dd, "Interpreter" description, bend right=67]
 \arrow[rd, "Partial\ Evaluation" description, bend left]
 \arrow[dd, "JIT" description, bend left=67] & {} \\
Executable \arrow[rd, "Run" description, bend right] & {} & Native\ Image \arrow[ld, "Run", bend left]
 \\ {} & Result & {}
\end{tikzcd}
\caption{Methods of program execution}
\label{fig:futamora}
\end{figure}
#+END_EXPORT

Truffle has already been used rather successfully for the (mostly) imperative
languages Ruby, Python, R, Java, and WebAssembly, but (purely-)functional
languages differ in their evaluation model and in particular the required
allocation throughput, so it is still an open question whether GraalVM is a good
enough fit.

The desired outcome---at least, of the first part of my thesis---is a set of
implementations, and a set of benchmarks demonstrating a positive or a negative
result.  If the result is positive, there are many potential follow-up tasks:
implementing a different, more complex language, maybe a language to be
interpreted into the dependently-typed lambda calculus to attempt the approach
implemented in the /Collapsing Tower of Interpreters/ cite:amin2017collapsing,
or experimenting with different runtime models - all depending on the results of
this preliminary proof of concept.

In the best case, the JIT-compiled program would be as close in performance to a
program processed by a hand-crafted compiler as possible (not including JIT
warm-up), and I would spend the second half of my thesis on different topics
(like provably-correct program transformations) instead of hand-optimizing the
primitive operations - I should find out which it is going to be as soon in the
second term as possible.

As far as I am aware, there are no other native just-in-time compiled
implementations of the dependently-typed lambda calculus, with the exception of
the preliminary investigations done by the originator of this idea
cite:kmett_2019, although there are a few projects implementing a lambda
calculus directly to the Java Virtual Machine byte code..

** New
--- see Excel@FIT start

Follow-up to Cadenza (STLC) on behalf of its creator

Combination of JIT and dependent types is natural (computation on the type-level) but not found in literature

Motivation: JIT for elaboration!!! plus easy prototyping, stepping block for
    other projects (efficient LF/Twelf, ...)

smalltt a step in another direction


* GraalVM and the Truffle Framework
** Intro: ecosystem, purpose, benefits
*GraalVM* is a just-in-time optimizing compiler for the Java bytecode. *Truffle* is
a set of libraries that expose the internals of the GraalVM compiler, intended
for easy implementation of other languages. So far JavaScript, Python, Ruby, R,
and WebAssembly have Truffle implementations, and therefore can run on the JVM.

GraalVM is also intended to allow creating /polyglot applications/ easily,
applications that have their parts written in different languages. It is
therefore easy to e.g. call R to create visualizations for the results of a
Python program, or to call any Truffle language from Java.

There is also the option to compile a /Native Image/ to eliminate most program
start-up costs associated with a just-in-time compiler, pre-compiling the
program partially (ahead-of-time).

From the point of view of a programmer, Truffle makes it possible to write an
interpreter, and then slowly add optimizations like program graph rewriting,
node specializations, inline instruction caching or others. This seems like a
good middle ground between spending large amounts of time on an optimized
compiler, and just specifying the semantics of a program in an interpreter that,
however, will likely not run quickly.

While GraalVM/Truffle is open-source and released under GPL v2, an
enterprise edition that claims large performance improvements is released
commercially.

#+ATTR_LaTeX: :placement [!htb]
#+CAPTION: GraalVM and Truffle (source: oracle.com)
[[./img/graalvm.jpg]]

JIT options - specialization, deoptimization
Use cases
Potential optimizations
** GraalVM
marketing texts

diagrams

Hotspot's JIT vs Graal's

** Truffle
Intro + motivating multilanguage snippet

features with code samples

benchmarks for other languages

* Language specification
** Dependent types
Intro dependent types + motivating example

hot ML research area (cubical, path, ...) (look in Kovacs' materials)

** Lambda Calculus Theory
*** Untyped Lambda Calculus
The untyped lambda calculus is a simple language consisting of just three kinds
of forms: variables, function application, and abstraction.

#+CAPTION: Untyped lambda calculus
#+ATTR_LaTeX: :options [!htpb]
#+begin_figure latex
\[\begin{array}{ccll}
e & ::= & x            & \text{variable} \\
  & |   & e_1~e_2      & \text{application} \\
  & |   & \lambda x. e & \text{abstraction}
\end{array}\]
#+end_figure

syntax, semantics, usage

*** Simply-Typed Lambda Calculus
The simply-typed lambda calculus adds a fourth kind of a term, type annotation,
and its type language:

#+CAPTION: Simply typed lambda calculus
#+ATTR_LaTeX: :options [!htpb]
#+begin_figure latex
\[\begin{array}{ccll}
e & ::= & x           & \text{variable} \\
  & |   & e_1~e_2      & \text{application} \\
  & |   & \lambda x. e & \text{abstraction} \\
  & |   & x:\tau     & \text{annotation}
\end{array}\]
\[\begin{array}{ccll}
\tau & ::= & \alpha           & \text{base type} \\
     & |   & \tau\rightarrow\tau' & \text{composite type}
\end{array}\]
#+end_figure

syntax, semantics, type checking, type inference

*** Dependently-Typed Lambda Calculus

Lambda cube

... image

The dependently typed lambda calculus merges these two languages together,
simplifying the grammar.

#+CAPTION: Dependently typed lambda calculus
#+ATTR_LaTeX: :options [!htpb]
#+begin_figure latex
\[\begin{array}{ccll}
e & ::= & x           & \text{variable} \\
  & |   & e_1~e_2      & \text{application} \\
  & |   & \lambda x. e & \text{abstraction} \\
  & |   & x:\tau      & \text{annotation} \\
  & |   & *           & \text{the type of types} \\
  & |   & \forall x:\rho.\rho' & \text{dependent function space}
\end{array}\]
#+end_figure
...

directions, decidability, where is DTLC

implicits, holes, metacontext

** Algorithms
type-checking (bidi, NbE) + motivation

elaboration (what is it, why is it slow, glued, nondet, cite Kovacs)

evaluation techniques (by-need, ...)

eval/apply (in implementation section)

** Specification
#+ATTR_LaTeX: :placement [!htpb]
#+CAPTION: The constant function in LambdaPi
#+begin_src text
let const = (\ a b x y -> x) :: forall (a :: *) (b :: *) . a -> b -> a
#+end_src

grammar

semantics (inference rules, evaluation rules)

* Language Implementations
** Pure Interpreter
*** Parser
ANTLR is the parser to use in JVM

Several ways to consume: listener, visitor - I've used AST transformation which
is the most compact and most familiar to other DTLC implementations which are
usually in functional languages

(listener - enter/exit function calls, visitor is similar, toAst uses recursive calls)

#+include: "../montuno/src/main/antlr/montuno/Montuno.g4" src antlr -n

*** Data shape

*** De Bruijn
Given an environment stack, indices count from the start of the stack, levels
count form the bottom. Two ways of indexing the environment, indices useful for
a stable context, levels without a stable context.

Both remote the problem of substitution, can be converted by subtraction.

We use both, indices when quoting evaluated values.

*** Algorithms
elaboration

** Truffle Interpreter
I have implemented a dependently typed lambda calculus called LambdaPi based on
the prior work /A tutorial implementation of a dependently typed lambda calculus/
cite:loh2010tutorial. The parser and interpreter are written in Kotlin, where
I will also need to write the JIT implementation. This is a pure interpreter
that will serve as a baseline for future benchmarks.

parser shared with the previous implementation

slightly impractical compared to the functional version, subclasses with methods
instead of plain functions -> required for truffle annotations

data classes

truffle specifics

inline cache, tail call, trampoline (continuations)

!! show their effect on program graphs

eval/uneval(quote)

evaluation phases - translate to Code, run typecheck, run eval vs glued, ???

native image

*** Closure abstraction
HOAS vs Closure

HOAS impossible in Truffle, functions and function calls need to be objects
HOAS = using the host language's support functions
therefore explicit closures

*** Evaluation model of function application
push-enter - arguments are pushed onto the stack, the function then takes as
many as it requires

eval-apply - the caller sees the arity of the function and then decides whether
it is over-applied (evaluates the function and creates a continuation), appllied
exactly (EVAL), or under-applied (creates a PAP, a closure-like value)

-- exactly describe the rules from eval-apply paper KNOWNCALL, EXACT, CALLK, PAP
-- known application ( = known arity), unknown function


** Similar implementations
"I've considered LLVM, WASM backends, but we need to evaluate type-checking
perf especially, so not relevant - better to compare with existing and/or
experimental systems"

SmallTT, Coq, Agda, GHC - for comparison

* Evaluation
** Benchmarks
  - from SmallTT project, from Idris project
  - memory usage (curve)
  - compilation speed (type-heavy test)
  - evaluation speed (compute-heavy test)

*** Specific test cases:
- Nats - large type elaboration, call-by-need test
- Nats - type-level calculation
- Nats - value-level calculation
- Nats - equality/forcing
- pairs - large type elaboration, call-by-need test
- pairs - nested accessors
- function types - embed STLC?

** Results
---

** Future work
good enough?

LF, techniques, extensions, real language

* Conclusion




* (bibliography, start of appendix)                           :ignoreheading:

#+BEGIN_EXPORT latex
\makeatletter
\def\@openbib@code{\addcontentsline{toc}{chapter}{Bibliography}}
\makeatother
\begin{flushleft}
#+END_EXPORT

bibliographystyle:bibstyle
bibliography:bibliography.bib

#+BEGIN_EXPORT latex
\end{flushleft}
\iftwoside\cleardoublepage\fi
\appendix
\appendixpage
\iftwoside\cleardoublepage\fi
\startcontents[chapters]
% \setlength{\parskip}{0pt}
% \printcontents[chapters]{l}{0}{\setcounter{tocdepth}{2}}
% \setlength{\parskip}{0.5\bigskipamount}
\iftwoside\cleardoublepage\fi
#+END_EXPORT

* Contents of the attached data storage
...
