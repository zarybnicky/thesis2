#+BEGIN_COMMENT
Normostrana = 1800c ≈ 250 slov
org-word-count
org-wc-display

16.4. 2303w
17.4. 3023w
18.4. 3600w? (Σ36p)
19.4. 4000w? (Σ40p)
22.4. 5200w
24.4. 6700w
25.4. 7200w (Σ56p)
26.4. 7660w (Σ59p)
27.4. 8800w (Σ64p)
28.4. 10050w (Σ67p)
29.4. 11700w (Σ68p)
30.4. 12500w (Σ74p, 52p content)
3.5. 14000w (Σ81p, Σ58p content)
5.5. 14500w (Σ79p)
6.5. 15200w (Σ77p)
10.5. 18800w (Σ81p)
11.5. 20900w

minimum: 12,500
target: 17,500
maximum: 30,000
#+END_COMMENT
* (front matter)                                              :ignoreheading:
#+LANGUAGE: en
#+OPTIONS: texht:nil toc:nil author:nil ':t H:6 num:3
#+LATEX_CLASS: fitthesis
#+LATEX_CLASS_OPTIONS: [english,zadani,odsaz]
#+EXCLUDE_TAGS: noexport
# print = B&W links and logo
# cprint = B&W links, color logo
#+BIND: org-latex-title-command ""
#+BIND: org-latex-prefer-user-labels t
#+BIND: org-latex-default-figure-position "hbt"
#+BEGIN_EXPORT latex
\maketitle
\setlength{\parskip}{0pt}
{\hypersetup{hidelinks}\tableofcontents}
\iftotalfigures\listoffigures\fi
\iftotaltables\listoftables\fi
\iftotallistings\listoflistings\fi
\listoftodos
\iftwoside\cleardoublepage\fi
\setlength{\parskip}{0.5\bigskipamount}
#+END_EXPORT
* (Recommended thesis structure)                                   :noexport:
1. "Úvod do knížečky", jedna strana, bez podkapitol
2. "Co bylo třeba vystudovat?" - "Je tato informace potřebná k pochopení toho, co jsem udělal?"
3. "Nové myšlenky" - rozhodl jsem, vymyslel, rozvrhl, odvodil, vylepšil, navrhl, ...
4. "Implementace a vyhodnocení" - udělal jsem, posbíral data, výsledky
5. "Závěr" - žádné nové poznatky, možnosti pokračování, "naladit oponenta"

* (Casual outline)                                                 :noexport:
*Story:*
- Let's build a fast Agda!
- Why Agda, what's at its core?
- Oh, dependent types? What's that good for, how does it work?
- Why are all of these languages functional?
- Lambda calculus and everything - that's a lot of formalisms...
- How to build it? Fast λs, that's a start, then build it up.
- So, does it work? What can I use it for, does it fulfill the design goals?
- Why is it so slow? Asymptotics...
- So, Truffle should make it fast, what specifically? What's the inspiration?
- How do we start, what needs to be changed?
- Wow, that's a lot of concepts, slow down, explain!
- So, does this changed work function at all? What about polyglots? What UIs?
- What can we make improve, was this just the start?
- Did we fulfill our goals, is this useful at all?
- If this is somebody's starting point, what will they need to know?

*Original goal:*
- efficient βη-normalization runtime
- Show the asymptotics on Peano, Church numbers

*Side outcome:* compilation of all the relevant concepts.

- Citations: Werthinger et al. [45] have developed [...]

eqref:eq:1
#+begin_export latex
\begin{equation}\label{eq:1}
 ≡ Π_{x:A}B(x)
\end{equation}
#+end_export

* Bleh                                                             :noexport:
- cite:altenkirch08_pisigma, cite:altenkirch10_pisigma - "I had discovered the ΠΣ paper when finishing my thesis: too late, unfortunately"
- cite:eisenberg20_stitch is an interesting tutorial of a dependent interpreter of dependent languages
- cite:juan20_unif_thesis - well described contexts + language specification - can I take as inspiration?
- well investigated in cite:lindley05_nbe_sml where there is a comprehensive of
  NbE techniques as applied to ML
- also in cite:lindley05_nbe_sml there is a treatment of η reduction/expansion - READ

GHC, Clojure, Scala, OCaml, ML, Eta, Frege, ?

hyperfine to benchmark - measures speed (what about ~prof~?)

Krun benchmark runner + its warmup_stats functionality for statistical analysis
of steady states, number of iterations it took to stabilize.

memory profile from GHC's RTS for agda/idris/smalltt (+RTS -p) (what about
coq? - https://github.com/coq/coq/blob/master/dev/doc/profiling.txt)

Graal's default memory profiler

* Introduction
#+COMMENT: Motivation: why this project? Why should anyone care?

Proof assistants like Coq, F*, Agda or Idris, or other languages with dependent
types like Cayenne or Epigram, allow programmers to write provably
correct-by-construction code in a manner similar to a dialog with the compiler
cite:norell08_agda_tutorial. They also face serious performance issues when
applied to problems or systems on a large-enough scale
cite:gross14_coq_experience
cite:gross21_performance. Their performance grows exponentially with the number
of lines of code in the worst case cite:nawaz19_survey_provers, which is a
significant barrier to their use. While many of the performance issues are
fundamentally algorithmic, a better runtime system would improve the
rest. However, custom runtime systems or more capable optimizing compilers are
time-consuming to build and maintain. This thesis seeks to answer the question
of whether just-in-time compilation can help to improve the performance of such
systems.

Moving from custom runtime systems to general language platforms like e.g., the
Java Virtual Machine (JVM) or RPython cite:bolz14_meta, has improved the
performance of several dynamic languages: project like TruffleRuby, FastR, or
PyPy. It has allowed these languages to re-use the optimization machinery
provided by these platforms, improve their performance, and simplify their
runtime systems.

#+COMMENT: Problem definition: What exactly are you solving? What is the core and what is a bonus? What parameters should a proper solution to the problem have?

As there are no standard benchmarks for dependently typed languages, we design a
small, dependently-typed core language to see if using specific just-in-time
(JIT) compilation techniques produces asymptotic runtime improvements in the
performance of β-normalization and βη-conversion checking, which are among the
main computational tasks in the elaboration process and is also the part that
can most likely benefit from JIT compilation. The explicit non-goals of this
thesis are language completeness and interoperability, as neither are required
to evaluate runtime performance.

#+COMMENT: Existing solutions: be fair in identifying their strengths and weaknesses. Cite important works from the field of your topic. Try to define well what is the state of the art.

State-of-the-art proof assistants like Coq, Agda, Idris, or others is what we
can compare our results with. There are also numerous actively developed
research projects in this area; Lean is a notable one that I found too late in
my thesis to incorporate its ideas. However, the primary evaluation will be
against the most well established proof assistants.

inline:Reformulate As for the languages that use Truffle, the language implementation framework
that allows interpreters to use the JIT optimization capabilities of GraalVM, an
alternative implementation of the Java Virtual Machine: there are numerous
general-purpose functional languages, the most prominent of which are
TruffleRuby and FastR. Both were reimplemented on the Truffle platform,
resulting in significant performance improvements[fn:7][fn:8]. We will
investigate the optimization techniques they used, and reuse those that are
applicable to our language.

There is also a number of functional languages on the Java Virtual Platform that
do not use the Truffle platform, like Clojure, Scala or Kotlin, as well as
purely functional languages like Eta or Frege. All of these languages compile
directly to JVM byte code: we may compare our performance against their
implementation, but we would not be able to use their optimization
techniques. To the best of my knowledge, neither meta-tracing nor partial
evaluation have been applied to the dependently-typed lambda calculus.

The closest project to this one is Cadenza cite:kmett_2019, which served as the
main inspiration for this thesis. Cadenza is an implementation of the
simply-typed lambda calculus on the Truffle framework. While it is unfinished
and did not show as promising performance compared to other simply-typed lambda
calculus implementations as its author hoped, this project applies similar ideas
to the dependently-typed lambda calculus, where the presence of type-level
computation should lead to larger gains.

#+COMMENT: Our solution: Make a quick outline of your approach, pitch your solution

inline:Rephrase In this thesis, I will use the Truffle framework to evaluate how
well are the optimizations provided by the just-in-time compiler GraalVM
suitable to the domain of dependently-typed languages. GraalVM helps to turn
slow interpreter code into efficient machine code by means of /partial evaluation/
cite:wurthinger13_graal. During partial evaluation, specifically the second
Futamura projection cite:latifi19_futamura, an interpreter is specialized
together with the source code of a program, yielding executable code. Parts of
the interpreter could be specialized, some optimized, and some could be left off
entirely. Depending on the quality of the specializer, this may result in
performance gains of several orders of magnitude.

Truffle makes this available to language creators, they only need to create an
interpreter for their language. It also allows such interpreters to take
advantage of GraalVM's /polyglot/ capabilities, and directly interoperate with
other JVM-based languages, their code and values
cite:sipek19_polyglot. Development tooling can also be derived for Truffle
languages quite easily cite:stolpe19_environment. Regardless of whether Truffle
can improve their performance, both of these features would benefit
dependently-typed or experimental languages.

#+COMMENT: Contributions: Sell your solution. Pinpoint your achievements. Be fair and objective.

While this project was originally intended just as a λΠ calculus compiler and an
efficient runtime, it has ended up much larger due to a badly specified
assignment. I also needed to study type theory and type checking and elaboration
algorithms that I have used in this thesis, and which form a large part of
chapters ref:lambda and ref:interpreter.

Starting from basic λ-calculus theory and building up to the systems of the
lambda cube, we specify the syntax and semantics of a small language that I
refer to as Montuno (Chapter ref:lambda). We go through the principles of
λ-calculus evaluation, type checking and elaboration, implement an interpreter
for Montuno in a functional style (Chapter ref:interpreter).  In the second part
of the thesis, we evaluate the capabilities offered by Truffle and the
peculiarities of Truffle languages, and implement an interpreter for Montuno
using the Truffle framework (Chapter ref:jit-interpreter), and apply various JIT
optimizations to it (Chapter ref:optimizations). After designing and using a set
of benchmarks to evaluate the language's performance, we close with a large list
of possible follow-up work (Chapter ref:evaluation).

* Language specification: λ⋆-calculus with extensions
  :PROPERTIES:
  :CUSTOM_ID: lambda
  :END:
#+INCLUDE: "01-spec.org::#lambda" :only-contents t

* Language implementation: Montuno
  :PROPERTIES:
  :CUSTOM_ID: interpreter
  :END:
#+INCLUDE: "02-pure.org::#interpreter" :only-contents t

* Adding JIT compilation to Montuno: MontunoTruffle
  :PROPERTIES:
  :CUSTOM_ID: jit-interpreter
  :END:
#+INCLUDE: "03-truffle.org::#jit-interpreter" :only-contents t

* Evaluation
  :PROPERTIES:
  :CUSTOM_ID: evaluation
  :END:
#+INCLUDE: "04-evaluation.org::#evaluation" :only-contents t

* Conclusion
We tried X to do Y. It went well and we fulfilled the assignment.

As a side effect, I produced a reference book for functional/dependent language implementation.

Original goal was X, it grew to encompass Y, Z as well.
* (bibliography, start of appendix)                           :ignoreheading:

#+BEGIN_EXPORT latex
\makeatletter
\def\@openbib@code{\addcontentsline{toc}{chapter}{Bibliography}}
\makeatother
\begin{flushleft}
#+END_EXPORT

bibliographystyle:bibstyle
bibliography:bibliography.bib

#+BEGIN_EXPORT latex
\end{flushleft}
\iftwoside\cleardoublepage\fi
\appendix
\appendixpage
\iftwoside\cleardoublepage\fi
\startcontents[chapters]
% \setlength{\parskip}{0pt}
% \printcontents[chapters]{l}{0}{\setcounter{tocdepth}{2}}
% \setlength{\parskip}{0.5\bigskipamount}
\iftwoside\cleardoublepage\fi
#+END_EXPORT

* Contents of the attached data storage
* Language specification
   :PROPERTIES:
   :CUSTOM_ID: spec
   :END:
** Syntax
#+include: "../montuno/src/main/antlr/montuno/Montuno.g4" src antlr -n

** Semantics

** Built-in constructs
- Unit : Type
- unit : Unit
- Nat : Type
- zero : Nat
- succ : Nat → Nat
- natElim : {A} → Nat → A → (Nat → A) → A
- Bool : Type
- true : Bool
- false : Bool
- if : {A} → Bool → A → A → A
- fix : {A} → (A→A) → A
- the : (A) → A → A
- eval : {A} → String → A
- typeOf : {A} → A → Type

* Montuno
  :PROPERTIES:
  :CUSTOM_ID: montuno-data
  :END:
** Pre-terms
#+include: "../montuno/src/main/montuno/syntax/presyntax.kt" src kotlin -n
* Footnotes

[fn:12] https://github.com/AndrasKovacs/normalization-bench 

[fn:8] FastR is between 50 to 85x faster than GNU R, depending on the source. cite:fumero17_jit_gpu

[fn:7] Unfortunately, there are no officially published benchmarks, but a number of articles claim that TruffleRuby is 10-30x faster than the official C implementation. cite:shopify2020

