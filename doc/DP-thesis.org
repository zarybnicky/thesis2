#+BEGIN_COMMENT
Normostrana = 1800c ≈ 250 slov
org-word-count
org-wc-display

16.4. 2303w
17.4. 3023w
18.4. 3600w? (Σ36p)
19.4. 4000w? (Σ40p)
22.4. 5200w
24.4. 6700w
25.4. 7200w (Σ56p)
26.4. 7660w (Σ59p)
27.4. 8800w (Σ64p)
28.4. 10050w (Σ67p)
29.4. 11700w (Σ68p)
30.4. 12500w (Σ74p, 52p content)

minimum: 12,500
target: 17,500
maximum: 30,000
#+END_COMMENT
* (front matter)                                              :ignoreheading:
#+LANGUAGE: en
#+OPTIONS: texht:nil toc:nil author:nil ':t H:4 num:3
#+LATEX_CLASS: fitthesis
#+LATEX_CLASS_OPTIONS: [english,zadani,odsaz]
#+EXCLUDE_TAGS: noexport
# print = B&W links and logo
# cprint = B&W links, color logo
#+BIND: org-latex-title-command ""
#+BIND: org-latex-prefer-user-labels t
#+BIND: org-latex-default-figure-position "htb"
#+BEGIN_EXPORT latex
\maketitle
\setlength{\parskip}{0pt}
{\hypersetup{hidelinks}\tableofcontents}
\iftotalfigures\listoffigures\fi
\iftotaltables\listoftables\fi
\iftotallistings\listoflistings\fi
\listoftodos
\iftwoside\cleardoublepage\fi
\setlength{\parskip}{0.5\bigskipamount}
#+END_EXPORT
* (Recommended thesis structure)                                   :noexport:
1. "Úvod do knížečky", jedna strana, bez podkapitol
2. "Co bylo třeba vystudovat?" - "Je tato informace potřebná k pochopení toho, co jsem udělal?"
3. "Nové myšlenky" - rozhodl jsem, vymyslel, rozvrhl, odvodil, vylepšil, navrhl, ...
4. "Implementace a vyhodnocení" - udělal jsem, posbíral data, výsledky
5. "Závěr" - žádné nové poznatky, možnosti pokračování, "naladit oponenta"

* (Casual outline)                                                 :noexport:
*Story:*
- Let's build a fast Agda!
- Why Agda, what's at its core?
- Oh, dependent types? What's that good for, how does it work?
- Why are all of these languages functional?
- Lambda calculus and everything - that's a lot of formalisms...
- How to build it? Fast λs, that's a start, then build it up.
- So, does it work? What can I use it for, does it fulfill the design goals?
- Why is it so slow? Asymptotics...
- So, Truffle should make it fast, what specifically? What's the inspiration?
- How do we start, what needs to be changed?
- Wow, that's a lot of concepts, slow down, explain!
- So, does this changed work function at all? What about polyglots? What UIs?
- What can we make improve, was this just the start?
- Did we fulfill our goals, is this useful at all?
- If this is somebody's starting point, what will they need to know?

*Original goal:*
- efficient βη-normalization runtime
- Show the asymptotics on Peano, Church numbers

*Side outcome:* compilation of all the relevant concepts.

- Citations: Werthinger et al. [45] have developed [...]

* Introduction
#+COMMENT: Motivation: why this project? Why should anyone care?

Proof assistants like Coq, F*, Agda or Idris, or other languages with dependent
types like Cayenne or Epigram, allow programmers to write provably
correct-by-construction code in a manner similar to a dialog with the compiler
cite:norell08_agda_tutorial. They also face serious performance issues when
applied to problems or systems on a large-enough scale
cite:gross14_coq_experience
cite:gross21_performance. Their performance grows exponentially with the number
of lines of code in the worst case cite:nawaz19_survey_provers, which is a
significant barrier to their use. While many of the performance issues are
fundamentally algorithmic, a better runtime system would improve the
rest. However, custom runtime systems or more capable optimizing compilers are
time-consuming to build and maintain. This thesis seeks to answer the question
of whether just-in-time compilation can help to improve the performance of such
systems.

Moving from custom runtime systems to general language platforms like e.g., the
Java Virtual Machine (JVM) or RPython [[inline:Cite]], has improved the performance
of several dynamic languages: project like TruffleRuby, FastR, or PyPy. It has
allowed these languages to re-use the optimization machinery provided by these
platforms, improve their performance, and simplify their runtime systems.

#+COMMENT: Problem definition: What exactly are you solving? What is the core and what is a bonus? What parameters should a proper solution to the problem have?

As there are no standard benchmarks for dependently typed languages, we design a
small, dependently-typed core language to see if using specific just-in-time
(JIT) compilation techniques produces asymptotic runtime improvements in the
performance of β-normalization and βη-conversion checking, which are among the
main computational tasks in the elaboration process and is also the part that
can most likely benefit from JIT compilation. inline:Nongoals

#+COMMENT: Existing solutions: be fair in identifying their strengths and weaknesses. Cite important works from the field of your topic. Try to define well what is the state of the art.

State of the art proof assistants like Coq, Agda, Idris, or others is what we
can compare our results with. There is also a large number of research projects
being actively developed in this area; Lean is a notable one that I found too
late in my thesis to incorporate its ideas. However, the primary evaluation will
be against the most well established proof assistants.

inline:reformulate As for the languages that use Truffle, the language implementation framework
that allows interpreters to use the JIT optimization capabilities of GraalVM, an
alternative implementation of the Java Virtual Machine: there are numerous
general-purpose functional languages, the most prominent of which are
TruffleRuby and FastR. Both were reimplemented on the Truffle platform,
resulting in significant performance improvements[fn:7][fn:8]. We will
investigate the optimization techniques they used, and reuse those that are
applicable to our language.

There is also a number of functional languages on the Java Virtual Platform that
do not use the Truffle platform, like Clojure, Scala or Kotlin, as well as
purely functional languages like Eta or Frege. All of these languages compile
directly to JVM byte code: we may compare our performance against their
implementation, but we would not be able to use their optimization
techniques. To the best of my knowledge, neither meta-tracing nor partial
evaluation have been applied to the dependently-typed lambda calculus.

The closest project to this one is Cadenza cite:kmett_2019, which served as the
main inspiration for this thesis. Cadenza is an implementation of the
simply-typed lambda calculus on the Truffle framework. While it is unfinished
and did not show as promising performance compared to other simply-typed lambda
calculus implementations as its author hoped, this project applies similar ideas
to the dependently-typed lambda calculus, where the presence of type-level
computation should lead to larger gains.

- cite:kleeblatt11_strongly_normalizing_stg - STG compiler JITed?
- cite:schilling13_tracing_jit - trace-based interpreter for GHC, a different approach

#+COMMENT: Our solution: Make a quick outline of your approach, pitch your solution

inline:Rephrase In this thesis, I will use the Truffle framework to evaluate how
well are the optimizations provided by the just-in-time compiler GraalVM
suitable to the domain of dependently-typed languages. GraalVM helps to turn
slow interpreter code into efficient machine code by means of /partial evaluation/
cite:wurthinger13_graal. During partial evaluation, specifically the second
Futamura projection cite:latifi19_futamura, an interpreter is specialized
together with the source code of a program, yielding executable code. Parts of
the interpreter could be specialized, some optimized, and some could be left off
entirely. Depending on the quality of the specializer, this may result in
performance gains of several orders of magnitude.

Truffle makes this available to language creators, they only need to create an
interpreter for their language. It also allows such interpreters to take
advantage of GraalVM's /polyglot/ capabilities, and directly interoperate with
other JVM-based languages, their code and values
cite:sipek19_polyglot. Development tooling can also be derived for Truffle
languages rather easily cite:stolpe19_environment. Regardless of whether Truffle
can improve their performance, both of these features would benefit
dependently-typed or experimental languages.

#+COMMENT: Contributions: Sell your solution. Pinpoint your achievements. Be fair and objective.

While this project was originally intended just as a λΠ calculus compiler and an
efficient runtime, it has ended up much larger due to a badly specified
assignment. I also needed to study type theory and type checking and elaboration
algorithms that I have used in this thesis, and which form a large part of
chapters ref:lambda and ref:interpreter.

Starting from basic λ calculus theory and building up to the systems of the
lambda cube, we specify the syntax and semantics of a small language that I
refer to as Montuno (Chapter~ref:lambda). We go through the principles of λ
calculus evaluation, type checking and elaboration, implement an interpreter for
Montuno in a functional style (Chapter ref:interpreter), and design a set of
benchmarks to evaluate the language's performance (Chapter ref:bench).  In the
second half of the thesis, we evaluate the capabilities offered by Truffle and
the peculiarities of Truffle languages (Chapter ref:truffle), implement an
interpreter for Montuno on the Truffle framework (Chapter ref:jit-interpreter),
and apply various JIT optimizations to it (Chapter ref:optimizations). After
evaluating our overall results, we close with a large list of possible follow-up
work (Chapter ref:evaluation).

* Language specification: λΠ calculus with extensions
  :PROPERTIES:
  :CUSTOM_ID: lambda
  :END:
** Introduction
Proof assistants like Agda or Idris are built around a fundamental principle
called the Curry-Howard correspondence that connects type theory and
mathematical logic, demonstrated in Figure ref:ch-logic. In simplified terms it
says that given a language with a self-consistent type system, writing a
well-typed program is equivalent to proving its correctness
cite:baez10_rosetta. It is often shown on the correspondence between natural
deduction and the simply-typed λ calculus, as in Figure ref:ch-deduction. Proof
assistants often have a small core language around which they are build:
e.g. Coq is build around the Calculus of Inductive Constructions, which is a
higher-order typed λ calculus.

-- Dependent languages are not only proof checkers but also general programming
languages (Cayenne, Epigram), and Eisenberg's Stitch cite:eisenberg20_stitch is
an interesting tutorial of a dependent interpreter of dependent languages

#+label: idris-vect
#+caption: Vectors with explicit length in the type, source: the Idris base library
#+begin_src idris
  data Vect : (len : Nat) -> (elem : Type) -> Type where
    Nil  : Vect Z elem
    (::) : (x : elem) -> (xs : Vect len elem) -> Vect (S len) elem

  -- Definitions elided

  head : Vect (S len) elem -> elem
  index : Fin len -> Vect len elem -> elem

  (++) : (xs : Vect m elem) -> (ys : Vect n elem) -> Vect (m + n) elem
  proofConcatLength
    : {m, n : Nat} -> {A : Type} -> (xs : Vect n A) -> (ys : Vect m A)
      -> length (xs ++ ys) = length xs + length ys
#+end_src

#+label: ch-logic
#+CAPTION: Curry-Howard correspondence between mathematical logic and type theory
#+ATTR_LaTeX: :options [!htpb]
#+begin_figure latex
\begin{center}
\begin{tabular}{c|c}
Mathematical logic & Type theory \\\hline\\[-1em]
\shortstack{$⊤$ \\ true} &
\shortstack{$()$ \\ unit type} \\[7pt]
\shortstack{$⊥$ \\ false} &
\shortstack{$∅$ \\ empty type} \\[7pt]
\shortstack{$p ∧ q$ \\ conjunction} &
\shortstack{$a × b$ \\ sum type} \\[7pt]
\shortstack{$p ∨ q$ \\ disjunction} &
\shortstack{$a + b$ \\ product type} \\[7pt]
\shortstack{$p ⇒ q$ \\ implication} &
\shortstack{$a → b$, $a^b$ \\ exponential (function) type} \\[7pt]
\shortstack{$∀x ∈ A, p$ \\ universal quantification} &
\shortstack{$Π_{x : A}B(x)$ \\ dependent product type} \\[7pt]
\shortstack{$∃x ∈ A, p$ \\ existential quantification} &
\shortstack{$Σ_{x : A}B(x)$ \\ dependent sum type} \\[7pt]
\end{tabular}
\end{center}
#+end_figure

#+label: ch-deduction
#+CAPTION: Curry-Howard correspondence between natural deduction and the simply-typed λ calculus
#+ATTR_LaTeX: :options [!htpb]
#+begin_figure latex
\begin{center}
\begin{tabular}{c|c}
Natural deduction & λ→ calculus \\\hline\\[-1em]
\shortstack{
\AxiomC{}
\UnaryInfC{$Γ₁, α, Γ₂ ⊢ α$}
\DisplayProof \\ axiom} &
\shortstack{
\AxiomC{}
\UnaryInfC{$Γ₁, x : α, Γ₂ ⊢ x : α$}
\DisplayProof \\ variable} \\[7pt]

\shortstack{
\AxiomC{$Γ, α ⊢ β$}
\UnaryInfC{$Γ ⊢ α → β$}
\DisplayProof \\ implication introduction} &
\shortstack{
\AxiomC{$Γ, x : α ⊢ t : β$}
\UnaryInfC{$Γ ⊢ λx. t: α → β$}
\DisplayProof \\ abstraction} \\[7pt]

\shortstack{
\AxiomC{$Γ ⊢ α → β$}
\AxiomC{$Γ ⊢ α$}
\BinaryInfC{$Γ ⊢ β$}
\DisplayProof \\ modus ponens} &
\shortstack{
\AxiomC{$Γ ⊢ t : α → β$}
\AxiomC{$Γ ⊢ u : α$}
\BinaryInfC{$Γ ⊢ t u : β$}
\DisplayProof \\ application} \\[7pt]
\end{tabular}
\end{center}
#+end_figure

Compared to the type systems in languages like Java, systems like this enable
encoding much more information in the types. We can see the usual example of a
list with a known length in Listing ref:idris-vect: the type ~Vect~ has two
parameters, one is the length of the list as a Peano number, the other is the type of its
elements. Using this we can define safe indexing operators like ~head~, which
is only applicable to non-empty lists, or ~index~, where the index must be given as a
finite number between zero and the length of the list. List concatenation uses
arithmetic on the type level, and it is possible to explicitly prove that
concatenation preserves list length.

On the other hand, these languages are often restricted in some ways. General
Turing-complete languages allow non-terminating programs which leads to a
inconsistent type system, and so proof assistants use various ways of keeping
the logic sound and consistent. Idris, for example, is requires that functions
are total. It uses a termination checker, checking that recursive functions use
only structural or primitive recursion, in order to ensure that type-checking
stays decidable.

Our goal in this chapter is to specify a minimal dependently-typed core
language, so that we can create an interpreter for is, and write some
computationally interesting programs in it. To do that, we first need to cover
the concepts that we will need in later parts of the thesis.

The goal of this chapter is to introduce the concepts relevant to the language
specification, and not to create a complete reference to the large field of type
theory.

** Languages/Systems
*** λ-calculus
We will start from the untyped lambda calculus, as it is the language that all
following ones will build upon. Introduced in the 1930s by Alonzo Church as a
model of computation. It is a very simple language that consists of only three
constructions: abstraction, application, and variables, written as in Figure
ref:untyped.

#+label: untyped
#+CAPTION: λ-calculus written in Church and de Bruijn notation
#+ATTR_LaTeX: :options [b]
#+begin_figure latex
\captionsetup[subfigure]{aboveskip=-1pt}
\begin{subfigure}[t]{.5\textwidth}\centering
  \[\begin{array}{ccll}
  e & ::= & v   & \text{variable} \\
    & |   & M~N & \text{application} \\
    & |   & λv.~M & \text{abstraction}
  \end{array}\]
  \caption{Standard (Church) notation}
\end{subfigure}
\begin{subfigure}[t]{.5\textwidth}\centering
  \[\begin{array}{ccll}
  e & ::= & v     \\
    & |   & (N)~M \\
    & |   & [v]~M
  \end{array}\]
  \caption{De Bruijn notation}
\end{subfigure}
#+end_figure

The λ-abstraction $λx.~t$ represents a program that, when applied to the
expression $x$, returns the term $t$. For example, the expression $(λx.x x) t$
produces the expression $t t$. This step, applying a λ-abstraction to an term,
is called /β-reduction/, and it is the basic /rewrite rule/ of λ-calculus. Another
way of saying that is that the x is assigned/replaced with the expression T, and
it is written as the substitution $M[x≔T]$

todo:ReductionRules

We however need to ensure that the variables don't overlap. If they do, we need
to rename them. (Free variables, bound variables) This is called /α-conversion/ or
α renaming. (The expression $t[s/x]$ denotes the result of taking a term $t$ and
replacing every free occurence of the variable $x$ by the term $s$.)

Reduction of a term in the form of $λx.T(x)$ to $T$ --that is, a function that receives
an argument and directly applies $T$ to it--is called /η-reduction/.

There are also other reduction rules: δ-reduction to unfold constants;
ζ-reduction to unfold local ~let-in~ definitions; μ-, ν-, and ι-reductions to
unfold inductive definitions; but α-conversion and β-reduction are sufficient to
fully specify the untyped λ-calculus, as we can see in Figure
ref:untyped-reduce.

There are also other reductions, though not usually used in simple λ calculus definitions:
- δ-reduction = unfolding of a constant
- μ-reduction = ?
- ζ-reduction = ?
- ι-reduction = ? (fixpoint unfolding)
- ν-reduction = ?
- see this for more: cite:sacerdoti07_reductions

- β-reduction is the introduction rule followed by the elimination rule (computation)
- η-reduction is the elimination rule followed by the introduction rule (uniqueness)
- (also η-conversion, uniqueness principle)

#+label: untyped-reduce
#+CAPTION: λ-calculus reduction rules
#+begin_figure latex
\begin{subfigure}[t]{.5\textwidth}\centering
  \[\AxiomC{}
  \UnaryInfC{}
  \DisplayProof\]\caption{Standard (Church) notation}
\end{subfigure}
#+end_figure

By repeatedly reducing an expression--applying functions to arguments and α
renaming variables if necessary, we get a /β-normal form/. This normal form is
unique (up to α-conversion), according to the Church-Rossier theorem.

[[todo:Normal forms - nf, hnf, whnf (3 items, describe)]]

\missingfigure{Show off a normal form derivation - which operation is used when}

More definitions in cite:garcia18_simple_lambda, simply explained

todo:ExamplesOfReductions

There are /normalization strategies/ that specify the order in which
sub-expressions are reduced. Common ones are /applicative order/ in which we first
reduce sub-expressions left-to-right, and then apply functions to them; and
/normal order/ in which we first apply the leftmost function, and only then reduce
its arguments. These closely correspond to the call-by-need and call-by-value
evaluation strategies, and we will need this notion in later chapters when we
are creating an interpreter.

We will also need to compare two λ-terms to see if they are equal. There are two
general notions of equality, intensional and extensional. By intensional we mean
that they are written in the same way, whereas extensional equality means that
they behave in the same way with regards to an observer. While in general
programming languages, it is impractical to check whether two functions are
equal, whether intensionally or extensionally, we will need this later when we
are talking about type-checking: to compare two types expressed as λ-terms.

The usual notion of equality in λ-calculus is /α-equivalence of β-normal forms/,
meaning that we first reduce both expressions as much as possible and then
compare the symbols they are made of, ignoring differences in variable naming;
this is formally expressed as variable permutation.

As simple as λ-calculus may seem, it is a Turing-complete system, and we can
encode logic, arithmetic, or data structures in it: some examples include /Church
encoding/ of booleans, pairs, or natural numbers (Figure ref:church).

#+label: church
#+CAPTION: Church encoding of various concepts
#+ATTR_LaTeX: :options [!htpb]
#+begin_figure latex
\begin{subfigure}[t]{.5\textwidth}\centering
  \[\begin{array}{ccl}
  0 & = & λf.λx.~x \\
  1 & = & λf.λx.~f~x
  \end{array}\]
  \caption{Natural numbers}
\end{subfigure}
\begin{subfigure}[t]{.5\textwidth}\centering
  \[\begin{array}{ccll}
  succ & = & λn.λf.λx.f~(n~f~x) \\
  plus & = & λm.λn.m~succ~n
  \end{array}\]
  \caption{Simple arithmetic}
\end{subfigure}
\begin{subfigure}[t]{.5\textwidth}\centering
  \[\begin{array}{ccll}
  true & = & λx.λy.x \\
  false & = & λx.λy.y \\
  not & = & λp.p~false~true \\
  and & = & λp.λq.p~q~p \\
  ifElse & = & λp.λa.λb.p~a~b
  \end{array}\]
  \caption{Logic}
\end{subfigure}
\begin{subfigure}[t]{.5\textwidth}\centering
  \[\begin{array}{ccll}
  cons & = & λf.λx.λy.f~x~y \\
  fst & = & λp.p~true \\
  snd & = & λp.p~false \\
  \end{array}\]
  \caption{Pairs}
\end{subfigure}
#+end_figure

There are also expressions that do not have a normal form; when reduced, they do
not get smaller, they /diverge/. The ω combinator \[ω = λx.x~x\] is one such
example that produces an infinite term. Applying it to itself produces a
divergent term whose reduction cannot terminate: \[ω~ω ⇒_β ω~ω\].

Also notable is the fixed-point function, or the Y combinator.

\[Y = λf.(λx.f (x x)) (λx.f (x x))\]

This is one possible way of encoding general recursion in λ calculus, as it
reduces by applying $f$ to itself:

 \[Y~f ⇒ f(Y~f) ⇒ f(f(Y~f)) ⇒ ...\]

This, as we will see in the following chapter, is impossible to encode in the
typed λ calculus without additional extensions.

*** λ→-calculus
It is often useful, though, to describe the kinds of objects we work
with. Already, in the previous examples we could see that reading such code can
get confusing: a boolean is a function of two parameters, a pair is a function
of three arguments, but the first one should be a boolean and the other two
contents of the pair...

- cite:guallart15_overview_types - overview

The simply typed λ calculus, also written λ→ as "→" is the connector used in
types, introduces the concept of types. We have a set of basic types that are
connected into terms using →, and type annotation or assignment $x : A$.  We now
have two languages connected by a type judgment, the language of terms, and the
language of types.

There are two ways of specifying the simply-typed λ calculus: λ→-Church, and
λ→-Curry. Church-style is also called system of typed terms, or the explicitly
typed λ calculus as we have terms that include type information, and we say

\[λx : A.x : A → A,\]

or using parentheses to specify the precedence

\[λ(x : A).x : (A → A).\]

Curry-style is also called the system of typed assignment, or the implicitly
type λ calculus as we assign types to untyped λ-terms that don't carry type
information by themselves, and we say $λx.x : A → A$. cite:barendregt92_typed.

There are systems that are not expressible in Curry-style, and vice versa.
Curry-style is interesting for programming, we want to omit type information;
and we will see how to manipulate programs specified in this way in following
chapters re:type elaboration. We will use Church-style in this chapter, but our
language will be Curry-style, so that we incorporate elaboration into the
interpreter.

- type checking ( = determining whether a program is well-typed)
- type inference ( = the process of obtaining the type of an expression from its
  parts or implicits; Hindley-Milner is well-known)
- elaboration = convert a partially specified expression into a complete,
  type-correct form (http://leodemoura.github.io/files/elaboration.pdf)
- cite:ferreira14_bidi - elaboration definition

Before we only needed evaluation rules to fully specify the system, but now we
will also need typing rules. We will also need to distinguish well-formed terms
that are syntactically valid from well-typed terms that are well-formed and
additionally obey typing rules.

- cite:abel17_decidability - contexts, normals, whnf, typed reduction rules (no subject reduction)

/Well-formed/ is not /well-typed/: /pre-syntax/terms/ turned into /syntax/terms/ by
type-checking.

 Type constructors:
 - formation, a way to construct new types (if A and B are types, then Bᴬ is a type)
 - introduction, ways to construct terms of these types (if (x:A)⊢(b:B), then λx.b:Bᴬ)
 - elimination, ways to use them to construct other terms (if a:A and f:Bᴬ, then f(a):B)
 - computation, what happens when intro&elim ((λx.b)(a) computes to b[x≔a])
 - η-conversion, neutrals rules?

 - positive type: defined by introductions: coproduct, empty type
 - negative type: defined by eliminations: function, product

The complete syntax of the λ→ is in Figure ref:typed. We now also have the
language of types that includes a set of /base types/ like naturals or booleans,
and functions between them. There is also the context that associates variables
to their types, which we will need in typing rules. Reduction operations are the
same as in the untyped lambda calculus

#+label: typed
#+CAPTION: Simply-typed λ calculus
#+ATTR_LaTeX: :options [!htpb]
#+begin_figure latex
\[\begin{array}{ccll}
e & & & (terms) \\
  & ≔ & v     & \text{variable} \\
  & | & M~N   & \text{application} \\
  & | & λx.~t & \text{abstraction} \\
  & | & x:τ   & \text{annotation}
\\[5pt]
τ & & & (types) \\
  & ≔ & β      & \text{base types} \\
  & | & τ → τ' & \text{composite type} \\[5pt]
Γ & & & (typing context) \\
  & ≔ & ∅     & \text{empty context} \\
  & | & Γ,x:τ & \text{type judgement} \\[5pt]
v & & & (values) \\
  & ≔ & λx. t & \text{closure} \\[5pt]
v_{Nf} & & & (normal forms) \\
  & ≔ & ??? \\[5pt]
v_{Ne} & & & (neutral values) \\
  & ≔ & ???
\end{array}\]
#+end_figure

Strongly normalizing = must terminate. STLC is strongly normalizing, adding
primitive recursion via an extension does not remove the property
cite:bove08_atwork, but general recursion as in the Y combinator is not
possible.

[[todo:Close the above, motivating example what is it good for and what it doesn't
support? (1 para + 1 figure)]]

*** λ-cube
Generalizations of the λ→ calculus can be organized into a cube called the λ
cube cite:barendregt92_typed (Figure ref:cube). The three dimensions of the cube
each represent a new type of dependency. In λ→ only terms can depend on terms,
but there are also three other combinations: types depending on types
(/higher-order/ in the figure, as it allows higher-order types, also called type
operators), terms depending on types (/polymorphism/ in the figure), and terms
depending on types (/dependence/ in the figure).

#+label: cube
#+CAPTION: The Barendregt cube, or λ-cube
#+begin_figure latex
\centering
\begin{tikzpicture}
\matrix (m) [matrix of math nodes,
row sep=3em, column sep=3em,
text height=1.5ex,
text depth=0.25ex]{
   & λω             &     & λC             \\
λ2 &                & λΠ2 &                \\
   & λ\underline{ω} &     & λΠ\underline{ω}\\
λ→ &                & λΠ  \\
};
\path[-{Latex[length=2.5mm, width=1.5mm]}]
(m-1-2) edge (m-1-4)
(m-2-1) edge (m-2-3) edge node[fill=white,pos=0.4]{higher-order} (m-1-2)
(m-3-2) edge (m-1-2) edge (m-3-4)
(m-4-1) edge node[fill=white]{polymorphism} (m-2-1)
(m-4-1) edge (m-3-2)
(m-4-1) edge node[fill=white]{dependence} (m-4-3)
(m-3-4) edge (m-1-4)
(m-2-3) edge (m-1-4)
(m-4-3) edge (m-3-4) edge (m-2-3);
\end{tikzpicture}
#+end_figure

Higher-order type operators simply generalize the concepts of functions to the
type level, adding λ-abstractions and applications to the language of types.

todo:SimpleHigher-orderSyntax

Polymorphism adds polymorphic types to the language of types (\[∀X:k.A(X)\]),
and type abstractions (Λ-abstractions) and applications to the language of
terms.

todo:SimplePolymorphicSyntax

Dependence allows the function type to depend on its arguments, adding the
Π-type (\[Πa:A.B(a)\]).

todo:SimpleDependentSyntax

- λ→ calculus with polymorphism is called the System F.
- λ→ calculus with dependent types is well-studied as the Logical Framework or LF.
- The system λω (λ→ with polymorphism and dependence) is the basis of type
  systems of languages like Haskell or ML.
- The system that combines all three of these features is the Calculus of
  Constructions. This system, together with some extensions, is the basis of
  Agda or Coq.

To formally describe the cube, we will need to introduce the notion of sorts. In
brief,

\[t : T : ⋆ : □,\]

where \[t\] is a term, \[T\] a type, \[⋆\] is a kind, the sort of types, and
\[□\] is the sort of kinds, The type of terms is a type, the type of types is a
kind, the type of kinds is a sort.

todo:KindFormation

All of these three dependencies can be described by one introduction rule, where
\[(s₁, s₂) ∈ R\] are pairs of sorts \[R = {⋆,□}\]:

#+BEGIN_EXPORT latex
\AxiomC{$Γ ⊢ A : s₁$}
\AxiomC{$Γ, A : s₁ ⊢ B : s₂$}
\BinaryInfC{$Γ ⊢ Π_{x : A}b : s₂$}
\DisplayProof
#+END_EXPORT

If we know that $Π_{a:A}B$ where $B$ does not use $a$ can be written as $A → B$,
then this is equivalent to the function type formation rule.

To think of it differently, these rules form a λ calculus "one order
higher". where the terms have the sort □ and not ⋆. We can generalize this even
more, and say that $⋆ ≡ Type₀$ and $□ ≡ Type₁$, and extend this to an infinite
hierarchy of universes:

\[Type₀ : Type₁ : Type₂ : ...\]

This introduces complications to the type theory. In the simple case of types
and kinds, we can define separate operators, but in the general case we need to
define /lifting/ and /lowering/ of terms between universes. In the pedagogic
implementations of dependently-typed λ calculi from which I took inspiration,
this was acknowledged and replaced with the rule *$Type:Type$* that flattens this
hierarchy. This leads to paradoxes similar to the Russel's paradox in set
theory. However, even if it means that our logic is inconsistent, omitting
universes simplifies the implementation enough that it is beneficial.

todo:GirardsParadox

In the rest of this thesis, we will use all three generalizations of the
simply-type λ calculus. We will formally define these constructs, and also
define several extensions to this system that should be useful in the context of
just-in-time compilation using Truffle: coproduct types, induction, booleans, and
natural numbers.

** Types
- cite:juan20_unif_thesis - well described contexts + language specification -
  can I take as inspiration?

*** Π-types
dependent product type, dependent function type

eqref:eq:1
#+begin_export latex
\begin{equation}\label{eq:1}
∀x:A,P(x) ≡ Π_{x:A}B(x)
\end{equation}
#+end_export

- \[x:A ⊢ B(x):Type\] introduces a universe type, its elements are types
- can apply Λ-abstraction, \[Λx.B(x) : Typeᴬ\]
- Type : Type leads to paradoxes

[[todo:generalization of the function space (2 paras)]]

\[Π(n:ℕ). Vec(ℝ, n)\] vs \[Π(n:ℕ). ℝ\]

[[todo:grammar, specific example (2 figures)]]

Allows inductive types, eliminators, Church

- formation: if (x:A)⊢(B(x):Type), then Π_{x:A}B(x):Type
- construction: if (x:A)⊢(b:B(x)), then λx.b:Π_{x:A}B(x)
- elimination: if a:A and f:Π_{x:A}B(x), then f(a):B(a)
- computation: (λx.b)(a) computes to b[x≔a]

**** Product types
non-dependent product type

\[p : A × B\], \[p ≔ (a, b)\]

projections \[π₁ : A × B → A\], \[π₁ (a, b) ≔ a\]

η rule: \[(π₁~p, π₂~p) ≔ p\]

\[curry : A × B → C → A → B → C\], \[curry f ≔ λx.λy.f (x, y)\]

\[uncurry : A → B → C → A × B → C\], \[curry f ≔ λx.f (π₁ x) (π₂ y)\]

- formation: A:⋆ and B:⋆ | A×B:⋆
- construction: a:A and b:B | (a, b):A×B
- elimination: if p:A×B, then fst(p):A and snd(p):B
- computation: fst(a, b) is a, snd(a, b) is b

*** Σ-types
[[todo:Dependent tuple/pair/product/sum (2 para)]]

- cite:abel11_sigma_unif - well-specified terms, sigma, ?

generalization of the coproduct type

\[∃x:A,P(x) ≡ Σ_{x:A}B(x)\]

\[(a, b) : Σ_{x:A}. B(x)\]

[[todo:grammar, specific example (2 figures)]]

Allows datatypes/records, Cat + Functor?

#+label: ch-deduction
#+CAPTION: Dependent sum type
#+ATTR_LaTeX: :options [!htpb]
#+begin_figure latex
\begin{subfigure}[t]{.32\textwidth}\centering
\AxiomC{$ ⊢ A : ⋆$}
\AxiomC{$, x : A ⊢ B : ⋆$}
\BinaryInfC{$ ⊢ Σ_{x : A}B : ⋆$}
\DisplayProof\caption{Type}
\end{subfigure}
\begin{subfigure}[t]{.68\textwidth}\centering
\AxiomC{$ ⊢ a : A$}
\AxiomC{$, x : A ⊢ B : ⋆$}
\AxiomC{$ ⊢ b : B[x ≔ a]$}
\TrinaryInfC{$ ⊢ (a, b) : Σ_{x : A}B$}
\DisplayProof\caption{Constructor}
\end{subfigure}
\begin{subfigure}[t]{.32\textwidth}\centering
\AxiomC{$Γ ⊢ p : Σ_{x : A}B$}
\UnaryInfC{$Γ ⊢ fst p : A$}
\DisplayProof\caption{\texttt{fst} Eliminator}
\end{subfigure}
\begin{subfigure}[t]{.32\textwidth}\centering
\AxiomC{$Γ ⊢ p : Σ_{x : A}B$}
\UnaryInfC{$Γ ⊢ snd p : B[x ≔ fst p]$}
\DisplayProof\caption{\texttt{snd} Eliminator}
\end{subfigure}
\begin{subfigure}[t]{.5\textwidth}\centering
\AxiomC{$Γ ⊢ a : A$}
\AxiomC{$Γ, x : A ⊢ B : ⋆$}
\AxiomC{$Γ ⊢ b : B[x ≔ a]$}
\TrinaryInfC{$Γ ⊢ fst(a, b) ≡ a : A$}
\DisplayProof\caption{\texttt{fst} β-reduction}
\end{subfigure}
\begin{subfigure}[t]{.5\textwidth}\centering
\AxiomC{$Γ ⊢ a : A$}
\AxiomC{$Γ, x : A ⊢ B : ⋆$}
\AxiomC{$Γ ⊢ b : B[x ≔ a]$}
\TrinaryInfC{$Γ ⊢ snd(a, b) ≡ b : B$}
\DisplayProof\caption{\texttt{snd} β-reduction}
\end{subfigure}
\begin{subfigure}[t]{.32\textwidth}\centering
\AxiomC{$Γ ⊢ p : Σ_{x : A}B$}
\UnaryInfC{$Γ ⊢ (fst p, snd p) : Σ_{x : A}B$}
\DisplayProof\caption{η-conversion}
\end{subfigure}
#+end_figure

**** Coproduct types
Sum types

Set magnitude

- formation: if A and B are types, A+B is a type
- construction: if a:A, then inl(a):A+B, is b:B, then inr(b):A+B
- elimination: if p:A+B, and (x:A)⊢(c_A:C) and (y:B)⊢(c_B:C), then case(p,c_A,c_B):C
- computation: case(inl(a),c_A,c_B) => c_A[x≔a], same for b

- (\[case_{A,B,C} : (A → C) → (B → C) → (A + B → C) by\]
- \[case f g (inl a) ≔ f a\]
- \[case f g (inr b) ≔ g b\] ?)

*** μ-types
[[todo:Recursive type, box/unbox (2 paras)]]

Equi-recursive types: generic recursion
Iso-recursive types: with explicit folding and unfolding

[[todo:Type-level recursion, fixpoint, grammar (2 paras)]]

\missingfigure{Motivating example - from PiSigma?}

*** Function types
Implicit arguments, same as \[Λx.t(x)\] or \[∀X.t\], we write \[{x: X} -> A(x)\]. Types that depend
on types, terms that depend on types. With the exception that these really are
implicit, we will be able to infer these arguments automatically instead of
manually passing type arguments to all polymorphic functions.

\AxiomC{$t:A→B$}\UnaryInfC{$t→_βλx^A.t x$}\DisplayProof

*Implicit arguments*
[[todo:In brief - arguments that the compiler will fill in for us, type arguments
Agda et al. have it. (2 paras)]]

- cite:kovacs20_implicit - implicits (example, algo), rules+grammar!!!,
  metacontext+weakening, issues!!!, telescopes, benchmarks!!!
- cite:lescanne95_levels - de Bruijn levels-only, substitution algorithm

[[todo:Figure - implicit/braced arguments of an ~id~]]

*** =-types (?)
- formation: if A:⋆, a:A, b:A, then (a=b):⋆
- construction: if a:A, then refl_a:(a=a)
- elimination:
  - if (x:A),(y:A),(p:(x=y)) ⊢ C(x,y,p) : ⋆
  - and (x:A) ⊢ d(x) : C(x,x,refl_x)
  - then (x:A),(y:A),(p:(x=y)) ⊢ J(d;x,y,p) : C(x,y,p)
- computation: J(d;a,a,refl_a) computes to d(a)

*** Naturals
System T of Gödel: STLC + naturals + booleans cite:bove08_atwork

is still strongly normalizing - introducing naturals and booleans does not
prevent termination

- formation: \[⊢ ℕ : ⋆\]
- construction: 0:ℕ, and (x:ℕ)⊢(s(x):ℕ)
- elimination: if \[c₀:C(0)\] and (x:ℕ),(r:C(x))⊢(cₛ:C(s(x))), then for p:ℕ we have \[rec(p,c₀,cₛ):C(p)\]
- computation: rec(0,c₀,cₛ) ≔ c₀, rec(s(n),c₀,cₛ) ≔ cₛ[x≔n], rec(n,c₀,cₛ) ≔ r.

\[elim : ΠA : ℕ → Type.A 0 → (Πn : ℕ.A n → A (suc n)) ! Πn : ℕ.A n
elim A z s 0 = z
elim A z s (suc 0) = s n (elim A z s n)
\]

\[zeros : Πn : ℕ. Vec ℕ n
zeros ≔ elim (Vec ℕ) [] (λn, x.0 ::ₙ x)
\]

Other eliminator examples at http://www.cs.nott.ac.uk/~psztxa/ntt/elim.pdf

*** Booleans
** Algorithms
*** Evaluation
- cite:eisenberg20_stitch - exact algorithm description

While reduction strategies mentioned (many pages back) are related to the
evaluation strategy and rewriting rules specify the behavior of an evaluator
exactly, "real-world" implementations of the λ-calculus do not use rewriting as
their primary method of evaluation.

What is commonly done is a translation of terms into another domain, which then
produces values, which are then compared or unified or whatever. This domain can
be an interpreter, compiled code, ...

First, however, we need to introduce evaluation models that connect reduction
strategies to terms used in implementation of programming
languages.

Call-by-value, otherwise called eager evaluation, corresponds to applicative
order reduction strategy cite:ariola97_cbn. Specifically, when executing a statement, its
sub-expressions are evaluated inside-out and immediately reduced to a value.
This leads to predictable program performance (the program will execute in the
order that the programmer wrote it, evaluating all expressions in order), but
this may lead to unnecessary computations performed: given an expression ~const
5 (ackermann 4 2)~, the value of ~ackermann 4 2~ will be computed but immediately
discarded, in effect wasting processor time.

Call-by-need, also lazy evaluation, is the opposite paradigm. An expression will
be evaluated only when it's result is first accessed, not when it is created or
defined. Using call-by-need, the previous example will terminate immediately as
the calculation ~ackermann 4 2~ will be deferred and then discarded. However, it
also has some drawbacks, as the performance characteristics of programs may be
less predictable or harder to debug.

Call-by-value is the prevailing paradigm, used in all commonly used languages
with the exception of Haskell. It is sometimes necessary to defer the evaluation
of an expression, however, and lazy evaluation is emulated using closures or
zero-argument functions: e.g., in Kotlin a variable can be initialized using the
syntax ~val x by lazy { ackermann(4, 2) }~, and it will be initialized only if is
ever needed.

There is also an alternative paradigm, called call-by-push-value, which subsumes
both call-by-need and call-by-value as they can be directly translated to
CBPV--in the context of λ-calculus specifically. It does this by defining additional
operators /delay/ and /force/, one to create a /thunk/ that contains a deferred
computation, one to evaluate the thunk. Also notable is that it distinguishes between values
and computations: values can be passed around, but computations can only be
executed, or deferred.

Levy's call-by-push-value cite:levy99_cbpv formalism subsumes both (by means of
a translation strategy): it defines a single evaluation order in terms of
operations with a stack using two additional operators. It *distinguishes values
and computations*, operator $U$ (delay) creates a /thunk/, a delayed computation,
and operator $F$ (force) that forces its evaluation.

#+CAPTION: Call-by-push-value values and computations
#+begin_figure latex
\[\begin{array}{ccll}
A & ::= & U B | Σ(i∈I)A_i | 1 | A × A \\
B & ::= & F A | Π(i∈I)B_i | A → B
\end{array}\]
#+end_figure

- value of type \[UB\] is a thunk producing a value of type \[B\]
- Σ is a pair (tag, Value)
- type A × A' is a value of type (V, V')
- type 1 is a 0-tuple
- computation of type \[FA\] produces a value of type A
- computation Π pops a tag \[i\] from operand stack, then is a computation of type \[B\]
- computation A → B pops a value of type A, then behaves as type B
[[todo:https://www.cs.bham.ac.uk/~pbl/papers/tlca99.pdf]]

*** Normalization
*NbE*

- well investigated in cite:lindley05_nbe_sml where there is a comprehensive of
  NbE techniques as applied to ML
- also in cite:lindley05_nbe_sml there is a treatment of η reduction/expansion - READ

- cite:altenkirch16_nbe - NbE for dependent types including sums, renamings!
- cite:christiansen19_nbe_haskell - Haskell simple NbE

- cite:gratzer19_modal_types - Tm, NfTm, NbTm; NfVal, NeVal, Val split, NbE well-written, normals, closures

Normalization-by-evaluation in the style of Abel. Reminiscent of partial
evaluation for λ calculus as we use "sticky" /neutral values/ instead of values we
currently do not have. Provably confluent, a viable normalization strategy.

As we use typed reduction rules, we do not need "subject reduction" algorithms(?)

Conversion checking = type (or expression) equivalence checking, includes
evaluation (NbE = full comparison of normal forms), checking equivalence "as
described in the previous section"

[[todo:Describe motivation for NbE, the process, what is a neutral, eval/quote (3 paras)]]

\missingfigure{neutral terms}

*** Type checking
*Bidirectional typing*
- cite:ferreira14_bidi - elaboration, bi-di
- cite:dunfield19_bidi - full rules, polarized??

Bidirectional typing (https://www.cl.cam.ac.uk/~nk480/bidir-survey.pdf) = now
standard approach, combines type-checking and inference, simpler to implement
even if inference is not required

[[todo:Intro, motivation, list alternatives, pros and cons (2 para)]]

[[todo:Sketch the process? Write out lambda + app rules? (half a page)]]

** Our syntax
[[todo:Putting this all together, we get...]]

#+begin_src antlr
  FILE
      : STMT (STMTEND STMT)* ;
  STMT
      : "{-#" PRAGMA "#-}"
      | ID ":" EXPR
      | ID ":" EXPR "=" EXPR
      | ID "=" EXPR
      | COMMAND EXPR
      ;
  EXPR
      : "let" ID ":" EXPR "=" EXPR "in" EXPR
      | "λ" LAM_BINDER "." EXPR
      | PI_BINDER+ "→" EXPR
      | ATOM ARG*
      ;
  LAM_BINDER
      : ID | "_"
      | "{" (ID | "_") "}"
      ;
  PI_BINDER
      : ATOM ARG*
      | "(" ID+ ":" EXPR ")"
      | "{" ID+ ":" EXPR "}"
      ;
  ARG
      : ATOM
      | "{" ID ("=" TERM)? "}"
      ;
  ATOM
      : "[" ID "|" FOREIGN "|" TERM "]"
      | EXPR "×" EXPR
      | "(" EXPR ("," EXPR)+ ")"
      | "(" EXPR ")"
      | ID "." ID
      | ID
      | NAT
      | "*"
      | "_"
      ;
  STMTEND : ("\n" | ";")+ ;
  ID : [a-zA-Z] [a-zA-Z0-9] ;
  SKIP : [ \t] | "--" [^\r\n]* | "{-" [^#] .* "-}" ;
  // pragma, command discussed in text
#+end_src

[[todo:The complete inference and evaluation rules are in Appendix X]]

This looks nice: https://homepages.inf.ed.ac.uk/wadler/papers/mpc-2019/unraveling.pdf

[[todo:Extend with builtins/primitives/wired-in types, FFI with truffle, hole?]]
* Language implementation: Montuno
  :PROPERTIES:
  :CUSTOM_ID: interpreter
  :END:
** Introduction
We will first create an interpreter for Montuno as specified in the assignment,
and also because evaluation and elaboration algorithms from the literature are
quite naturally translated to a functional-style program, which is not really
possible in Truffle, the target implementation, where annotated classes are the
main building block.

The Truffle implementation, which we will see in the following chapters, has a
much higher conceptual overhead as we will need to care about low-level
implementation details, e.g. implementing the actual function calls. In this
interpreter, though, we will simply use the features of our host language.

We will use Kotlin as our language of choice, as it is a middle ground between
plain Java and functional JVM-based languages like Scala or Clojure. While the
main target language of Truffle is Java, Kotlin also supports class and object
annotations on which the Truffle DSL is based [fn:1]. Functional style, on the other
hand, makes our implementation of the algorithms simpler and more
concise [fn:6].

The choice of the platform (JVM) and the language (Kotlin) also clarifies the
choice of supporting libraries. In general, we are focused on the algorithmic
part of the implementation, and not on speed or conciseness, which means that we
can simplify our choices by using the most widely used libraries:
- Gradle as the build system,
- JUnit as the testing framework,
- ANTLR as the parser generator,
- JLine as the command-line interface library.

Truffle authors recommend against using many external libraries in the
internals of the interpreter, as the techniques the libraries use may not
work well with Truffle. This means that we need to design our own supporting
data structures based on the primitive structures provided directly by Java.

The overall program flow of our interpreter will not be unusual for
interpreters:
- receive some input from the user, either from a file, or from an interactive
  prompt;
- parse the textual content into an intermediate representation;
- if we are in batch mode, sequentially process the top-level statements,
  accumulating the entries processed so far into a global table of names. Given
  a top-level definition containing a type and a term, we will:
  - /check/ that the type provided has the kind ⋆
  - /infer/ the type of the term
  - /check/ whether the provided and inferred types can be unified
  - /simplify/ both the type and term, and store them into the global name-table.
- if we are in interactive mode, normalize the expression given and print it
  out;
- if we encounter an ~elaborate~ or ~normalize~ command, print out the full or
  normalized form of the expression.

#+label: main-sigs
#+caption: Simplified signatures of the principal functions
#+begin_src kotlin
fun infer(pre: PreTerm): Pair<Term, Val>
fun check(pre: PreTerm, wanted: Val): Term
fun eval(term: Term): Val
fun quote(value: Val): Term
#+end_src

Throughout this process, we will evaluate and quote ~Terms~ from and into
~Values~. The signatures of the most important functions are mentioned in Listing
ref:main-sigs. These are the main parts of our interpreter--and the main
component of evaluation is function application, which is what we will focus on
first.

** Representing functions
The main requirement for a λ-calculus runtime system is fast function
evaluation, which is where we will start.

Closures as values of the λ-calculus

- cite:gratzer19_modal_types - normal values, closures

[[todo:Three constructs - lam, app, var (1 para)]]

A closure consists of an unapplied function, and an environment that /closes over/
the free variables used in the function.

HOAS is a specific representation of closures, wherein the function is
represented as function in the host language, using the host language's support
for closing over free variables.

While representing functions using HOAS produces very readable code and in some
cases e.g. on GHC produces code an order of magnitude faster than using explicit
closures, this is not possible in for us, where function calls need to be nodes
in the program graph and therefore objects, as we will see in Chapter
[[ref:truffle]], so we will represent closures using explicit closures in the pure
interpreter as well.

[[todo:Cite Kovacs benchmarks where HOAS on GHC wins]]

[[todo:HOAS vs Closure code example]]

[[todo:Single versus multi-argument (1 para)]]

[[todo:Resulting data structure (TLam, VLam, VCl) (1 figure)]]

** Representing environments and variables
The way we define functions leads us to environments (Γ), which is a context
where we will look for variables.

[[todo:Named versus nameless representation (2 paras, example)]]

As we saw in [[inline:Where's WHNF?]], evaluation in λ-calculus is defined in terms
of α- and β-reductions. Verifying expression equivalence also uses variable
renaming. However, traversing the entire expression in the course of function
application and substituting variables is not efficient and there are several
alternative ways.

In typed λ calculus implementations in "real world" applications, evaluation
in an abstract machine or into a different semantic domain are used
instead. Then we have a clear distinction between terms (as the source code) and
runtime objects (on which we perform reductions). Machines like STG, Zinc, SECD,
... Normalization-by-evaluation works in this way, we eval/quote, [...]

[[todo:Find de Bruijn motivating example]]
[[todo:Find side-by-side dB examples]]

All three of them mentioned here work by replacing variable names with their
indices in a variable stack, informally /counting the lambdas/.

- de Bruijn indices - well-known, starting from the current innermost λ
- de Bruijn levels - less well-known, starting from the outer lambda, stable
- locally-nameless - dB indices for bound variables,, names for free variables

Given an environment stack, indices count from the start of the stack, levels
count form the bottom. Two ways of indexing the environment, indices useful for
a stable context, levels without a stable context.

- cite:kamareddine01_de_bruijn (pros & cons)
- cite:eisenberg20_stitch - motivation for indices/levels

In values, we have a known context, so it is easier to carry a level counter
around, and use levels in variables = Val → Term uses levels

In terms, we do not have a known context, so it makes sense to refer to use
indices. If we used levels, we would have to renumber them anyway, if we
e.g. performed an unfolding or otherwise used the term in a bigger context. =
Term → Val uses indices.

It is trivial to convert between them: given a context of size equal to the
current depth level \[d\], level to index is \[d - lvl - 1\] and index to level is
\[d - ix - 1\].

[[todo:Environments - arrays, cons lists, stack, mutable/immutable (2 paras)]]

[[todo:Snippet of code - binding a variable, looking up a variable (1 figure)]]

** Evaluation algorithm
[[todo:Equals (W) (H) NF, Single-variable functions - trivial transcription (1 para)]]

[[todo:What are our neutrals (1 para?)]]

[[todo:Eval Quote, very briefly (2 paras)]]

[[todo:Extensions (Pi, Sigma, Eta) (2 paras)]]

We do not want to do η-reduction, as it might introduce non-termination or
undecidability (and in general is not compatible with subtyping
relations). η-expansion is sound for producing βη-long normal forms.

If we do not have a βη-long normal form and want to unify/compare (λx:A.M) with
N, you unify/compare \[N X\] and \[M[x≔X]\] for a fresh X. This is sufficient for
βη-equality (for the easy η-rule for Π/λ).

Normalization-by-evaluation:
- normalization = bring an expression with unknowns into a canonical form
- evaluation = compute the value of an expression relative to an environment
- machine code: normalizer ~ JIT compiler, evaluator ~ stack machine (env = stack)
- NbE = adapt an interpreter to simplify expressions with unknowns

NbE similar to machine code + equations from http://www.cse.chalmers.se/~abela/talkHabil2013.pdf

** Parser
Lexical and syntactic analysis is not the focus of this work, so simply I chose
the most prevalent parsing library in Java-based languages which seems to be
ANTLR [fn:2]. It comes with a large library of languages and protocols from
which to take inspiration [fn:3], so creating a parser for our language was not
hard, despite me only having prior experience with parser combinator libraries
and not parser generators.

ANTLR provides has two recommended ways of consuming the result of parsing using
classical design patterns: a listener and a visitor. I have used neither as they
were needlessly verbose or limiting [fn:4].

I have instead implemented a custom recursive-descent AST transformation that
converts ~ParseContexts~ created by ANTLR into our ~Presyntax~ data type that we can
see in Listing ref:presyntax. This is actually a slightly simplified version
compared to the original as I have omitted the portion that tracks which
position of the input file corresponds to each subtree, which is later used for
type error reporting.

#+label: presyntax
#+caption: The Presyntax data type
#+begin_src kotlin
  sealed class TopLevel
  data class RDecl(val n: String, val type: PreTerm) : TopLevel()
  data class RDefn(val n: String, val type: PreTerm?, val term: PreTerm) : TopLevel()
  data class RTerm(val cmd: Command, val term: PreTerm) : TopLevel()

  sealed class PreTerm

  data class RVar(val n: String) : PreTerm()
  data class RNat(val n: Int) : PreTerm()
  object RU : PreTerm()
  object RHole : PreTerm()

  data class RApp(
      val icit: Icit, val rator: PreTerm, val rand: PreTerm
  ) : PreTerm()
  data class RLam(
      val name: String, val icit: Icit, val body: PreTerm
  ) : PreTerm()
  data class RFun(
      val domain: PreTerm, val codomain: PreTerm
  ) : PreTerm()
  data class RPi(
      val name: String, val icit: Icit, val type: PreTerm, val body: PreTerm,
  ) : PreTerm()
  data class RLet(
      val name: String, val type: PreTerm, val defn: PreTerm, val body: PreTerm,
  ) : PreTerm()
  data class RForeign(
      val lang: String, val eval: String, val type: PreTerm
  ) : PreTerm()
#+end_src

The data structures are represented in a way that Kotlin
recommends [fn:5]--using /data classes/. These are classes whose primary purpose
is to hold data, so called Data Transfer Objects (DTOs), and that have special
language support in Kotlin. We have a ~TopLevel~ class with three children that
represent: definitions that assign a value to a name, optionally with a type;
declarations (sometimes called postulates) that only assign a type to a name;
and commands like ~%normalize~ that we will see in later sections.

Their ancestor is a ~sealed class~ which tells the compiler there will only ever
be the subclasses defined in this module. In particular, this means that in any
pattern match on the type of a ~TopLevel~ object we only ever need to handle three
cases.

The remaining classes generally map to elements from the language syntax as
specified in the previous section: a λ abstraction, function application, Π
abstraction, a ~let~ local binding. The ~RFun~ class is a specialization of the ~RPi~
class that binds an unnamed, non-dependent type argument.

We will not use these classes immediately--only in Chapter ref:elaboration will
we implement a way to convert this pre-syntax into correct-by-construction
~Syntax~ objects which can be evaluated and quoted.

** Type checking and elaboration
   :PROPERTIES:
   :CUSTOM_ID: elaboration
   :END:

[[todo:Approach - infer, check + eval/quote used, global contexts (2 paras)]]

[[todo:Metavariables and holes, sequential processing (1 para)]]

[[todo:How do we do unification (2 paras + 1 figure)]]
- cite:gundry13_pattern_tutorial - well-specified unification algorithm, pruning
- cite:abel11_sigma_unif - unifying sigma

** Primitive operations
A primop needs:
- a symbol (nullary constructor)
- a neutral head (when it gets stuck)
- evaluation rule in eval (as many VLams as necessary to apply it, so that the
  computation rule can be tested)

Not projections, though:
- Fst:Term, (~eval env Fst = VLam \$ \v -> vFst v~),
- projections should go to spines, not heads
- neutrals should make access to the Ex that blocks computation as easy as possible,
- see
  https://github.com/AndrasKovacs/setoidtt/blob/master/proto/Evaluation.hs#L293
  for an example
** User interface
The user interface we will use is a command-line one: a multi-purpose command
that can start an interactive command-line session, execute a file, or
pre-compile a source file into a Native Image.

From my research, JLine is the library of choice for interactive command-line
applications in Java, so that is what I used for the REPL (Read-Eval-Print
Loop). It is a rather easy-to-use library: for the most basic use case, we only
need to provide a prompt string, and a callback. We can also add
auto-completion, a parser to process REPL commands, custom keybindings, built-in
pager or multiplexer, or even a scripting engine using Groovy.

Look at cite:garcia18_simple_lambda for other interaction modes:
- golden tests (~:verbose~ as a command, e.g.)
- :verbose = write out after every reduction
- :ski = express in terms of combinators
- interpreter + batch processor that reads REPL commands as well
- Jupyter kernel
- :types on, switches between untyped and simply-typed
- @@(expr) = print out as a proof (type derivation) using typing rules

[[todo:State keeping - load, reload + querying the global state (1 para)]]

commands:
- ~:l~ create a NameTable
- ~:r~ recreate a NameTable
- ~:t~ inferVar, print unfolded
- ~:nt~ inferVar, print folded
- ~:n~ inferVar type, gQuote term, show
- ~:e~ print elaboration output including all metas

\missingfigure{Example CLI session}

** Results
[[todo:Quick look at everything that this toy can do (2-3 examples?)]]

Evaluation, simplification, elaboration with holes, unification using eqRefl

Error reporting

* Designing elaboration and evaluation benchmarks
  :PROPERTIES:
  :CUSTOM_ID: bench
  :END:
We want to evaluate a few programs of equivalent functionality, as evaluated by
elaboration, type-checking, and simplification in a number of dependently-typed
languages.

We also want to evaluate the performance of general β-normalization which only
requires a functional language--this is secondary, however.

We're mainly interested in asymptotic behaviors and not on constant
factors. Just-in-time compiled languages especially suffer from long warm-up
times, which means that common evaluation of "repeatedly running a command" will
not perform well.

** Subjects
Subjects for elaboration: Agda, Idris, Coq, GHC. Also cooltt, smalltt, redtt, Lean, ?

Subjects for normalization: as above, but also Cadenza (STLC), Clojure, GHC,
Scala, OCaml, ML, Eta, Frege, ?

Also subjects: Montuno, MontunoTruffle, and possibly other optimized versions.

** Workload
 - Nats - large type elaboration, call-by-need test
 - Nats - type-level calculation
 - Nats - value-level calculation
 - Nats - equality/forcing
 - Functions - nested function elaboration, implicits
 - Functions - embedded STLC?
 - pairs - large type elaboration, call-by-need test
 - pairs - nested accessors

Mention sources (this is from smalltt ^^)

Brief descriptions, what does each one evaluate/stress?

** Methodology
We need to use in-language support, if available. We want to avoid measuring
interpreter start-up, program parsing time, and other confounders.

To measure: memory usage (curve), compilation speed (in a type-heavy test),
evaluation speed (in a compute-heavy test)

hyperfine to benchmark - measures speed (what about ~prof~?)
memory profile from stderr output

Krun benchmark runner + its warmup_stats functionality for statistical analysis
of steady states, number of iterations it took to stabilize.

memory profile from GHC's RTS for agda/idris/smalltt (+RTS -p)
(what about coq? - https://github.com/coq/coq/blob/master/dev/doc/profiling.txt)
Graal's default memory profiler

Mention specific parameters (X iterations, machine specs, ?)

** Results: a starting point
[[todo:Results]]

1:1 evaluation/discussion of directly comparable subjects, confidence
intervals, likely causes of improvements/regressions, iterations to
steady-state.

* Adding JIT compilation to Montuno: MontunoTruffle
  :PROPERTIES:
  :CUSTOM_ID: truffle
  :END:
** Just-in-time compilation
[[todo:Mention the general JIT theory - classical, tracing, rewriting]]

[[todo:Mention partial evaluation, Futamora projections (2 paras + 3 items + example)]]

[[todo:Where is the partial evaluation in Graal (1 para)]]

Graal compiles "hot code" to machine code by partially evaluating it. Partial
evaluation is TODO (Futamora). In particular, in dynamic languages it helps to
eliminate dynamic dispatch / megamorphic call overhead.

** GraalVM and the Truffle Framework
*** GraalVM
[[todo:Re-read, simplify (what is it, why we want to use it, what specifically do we use?)]]

*GraalVM* is an Oracle research project that was originally created as a
replacement for the HotSpot virtual machine written in C++. [[inline:Cite Oracle]]
It has since expanded to include other features novel to the Java world.

#+LABEL: graal
#+ATTR_LaTeX: :placement [!htb]
#+CAPTION: GraalVM and Truffle (source: oracle.com)
[[./img/graalvm.jpg]]

The project consists of several components which can be seen in ref:graal, the
main components being the following ones:

*Graal* is an optimizing just-in-time compiler based on partial evaluation. Graal
uses the JVM Compiler Interface which allows the main JVM to offload compilation
to external Java code. It can also use C1 (the old JVM JIT) which implies tiered
compilation (...disabled with ~-XX:-TieredCompilation~). When using the GraalVM,
the only JVMCI-compatible compiler is Graal, so automatically
used. [[inline:Simplify language]]

*SubstrateVM* is an alternative virtual machine that uses aggressive ahead-of-time
compilation [[inline:Cite SubstrateVM]] of Java bytecode into a standalone
executables, a so-called /Native Image. The project aims to guarantee fast
start-up times, relatively small binary files, and low memory footprint--as
opposed to slow start-up times due to JIT compilation and large memory usage
common to JVM-based languages.

*Truffle* (and *Truffle DSL*) is a language implementation framework, a set of
libraries that expose the internals of the Graal compiler to interpreter-based
language implementations. It promises that its users only need to write an
interpreter with a few framework-specific annotations in order to automatically
gain:
- access to all optimizations available on the Java Virtual Machine
- debugger support
- multi-language (/polyglot/) support between any other Java-based or
  Truffle-based languages (currently JavaScript, Python, Ruby, R, C, C++, WebAssembly)
- the ability to gradually add optimizations like program graph rewriting,
  node specializations, or inline instruction caching



[[todo:Images from file:///home/inuits/Downloads/graalvm-190721074229.pdf]]

[[todo:images from https://www.slideshare.net/jexp/polyglot-applications-with-graalvm]]

GraalVM is also intended to allow creating /polyglot applications/ easily,
applications that have their parts written in different languages. It is
therefore easy to e.g. call R to create visualizations for the results of a
Python program, or to call any Truffle language from Java.

This seems like a good middle ground between spending large amounts of time on
an optimized compiler, and just specifying the semantics of a program in an
interpreter that, however, will likely not run quickly.

While GraalVM/Truffle is open-source and released under GPL v2, an
enterprise edition that claims large performance improvements is released
commercially.

#+COMMENT: https://chrisseaton.com/truffleruby/jokerconf17/ (done)

Graal-compiled code still runs on the original HotSpot VM (written in C++)

Graal is also a graph optimizer, which takes the data-flow and instruction-flow
of the original program and is able to reorder them to improve performance. It
does what the original HotSpot VM would do, optimize JVM bytecode by reordering,
pre-compiling, or entirely rewriting instructions.

This graph is also extensible cite:duboscq13_graalir

- Canonicization: constant folding, simplification
- Global value numbering: prevents same code from being executed multiple times
- Lock coarsening: simplifies ~synchronized~ calls
- Register allocation: data-flow equals the registers required, optimize
- Scheduling: instruction-flow implies instruction order

Graal by itself is just better HotSpot, but there are other technologies in the
mix. SubstrateVM is an ahead-of-time compiler for Java which takes Java bytecode
and compiles is into a single binary, including the Graal runtime, pre-compiling
application code to greatly reduce warm up times (that are otherwise shared by
all JIT compilers).

In the ideal world, what GraalVM can do with the code by itself would be enough,
if that is not sufficient then we can add specializations, caches, custom
typecasts, ... We can also hand-tune the code, adding our hand-generated
bytecode into the mix.

*** Truffle
- trace-based optimization frameworks (RPython) vs partial evaluation frameworks (Truffle)
- JIT compilation for (suitably instrumented) interpreters
- Truffle provides a set of class and method annotations which reduce
  boilerplate and implementation effort. They drive a compile-time code
  generator

[[todo:Introduce Truffle specifically, polyglot, graph (de)optimization (3 paras + code sample)]]

Tiered compilation: C1, C2, Graal replaces C2

Multiple uses of Graal - it can function as instead of the C2 (server) compiler
or as a PE/AoT for SubstrateVM

Instruments, Chrome and other debugging support

Warm-up benchmark: http://macias.info/entry/201912201300_graal_aot.md

[[todo:It really is fast enough - FastR, Python benchmarks (2 paras + graph)]]

** Features common to Truffle languages
*** General features
[[todo:What does a typical Truffle language look like? (1 para)]]

[[todo:"We can look at a piece of Ruby code as a graph" (1 para + graph)]]

Another visualization option: Seafoam

#+COMMENT: https://norswap.com/truffle-tutorial/ (done)

[[todo:Introduce the canonical example - Literal and addition, trivial execute
fn (1 para, 1 figure)]]

Truffle: framework for implementing high-performance interpreters, in
  particular for dynamic languages, and AST interpreters. Each node implements
  the semantics of the operation/language construct. Graph flow - execution
  flows downwards, results flow upwards.

Graph manipulations:
- replace() to a more specific variant
- adopt() a new child node
- GraalVM aggressively inlines stable method calls into efficient machine code

- cite:wurthinger17_partial_eval - Truffle boundaries
- cite:wimmer17_deoptimization - deoptimization, on-stack replacement

*** Type specialization

[[todo:Type system]]

Basic case is type specialization - when an addition node only encounters
integers, there is no need to generate machine code for floats, doubles, or
operator overloads - only verified by fast checks. When these fail, the node is
de-optimized, and eventually re-compiled again.

Inline caching for e.g. method lookups, virtual method calls are typically only
ever invoked on a single class, which can be cached, and the dispatch node can
be specialized, perhaps even inline the
operation. (uninitialized/monomorphic/polymorphic/megamorphic = working with
jumptables + guard conditions)

Specializations are general, though, and nodes can go be specialized on
arbitrary conditions, using custom assumptions and /compilation final/ values. In
general, node states form a directed acyclic graph - "a node can ever become more
general".

Using a graph visualizer, we can look at this process on a simple example
commonly used to demonstrate this part of the Truffle framework: the ~+~
operation.

#+label: add-lang
#+begin_src kotlin
  abstract class LangNode : Node() {
      abstract fun execute(frame: VirtualFrame): Any
  }
  class IntLiteralNode(private val value: Long) : LangNode() {
      override fun execute(frame: VirtualFrame): Any = value
  }
  abstract class AddNode(
      @Child val left: LangNode,
      @Child val right: LangNode,
  ) : LangNode() {
      @Specialization
      fun addInt(left: Int, right: Int): Int = left + right
      @Specialization
      fun addString(left: String, right: String): String = left + right
      @Fallback
      fun typeError(left: Any?, right: Any?): Unit = throw TruffleException("type error")
  }
#+end_src

The Truffle framework is said to be a domain-specific language, which in this
case means a library, a set of annotations, and a code generator. This code
generator finds classes that inherit from the ~Node~ class and generates, among
others, the logic behind switching specializations.

The program graph is formed from a tree of Truffle ~Nodes~ from which we derive
our language-specific base class, ~LangNode~ in this case. We define two classes
that inherit from this class, one representing integer literals, and one for the ~+~
operator.

The abstract method ~execute~ in ~LangNode~ is the evaluation of this node. It takes
a ~VirtualFrame~, which represents a stack frame, and its return value is also the
return value of the node. In addition, methods starting with ~execute~ are special
in Truffle. Truffle will pick the most appropriate one based on return type
(with ~Any~ being the most general) and parameters.

In ~IntLiteralNode~ we directly override the method ~execute~, as there is only one
possible implementation. In ~AddNode~, however, we keep the class abstract and
don't implement ~execute~, and instead rely on Truffle to generate the appropriate
specialization logic.

Truffle will decide between the specializations based on parameter types, and on
user-provided guards (we'll see further). Fallback specialization matches in
cases where no other one does. Names are irrelevant.

(Maybe show generated code?) Active an inactive specializations: can be multiple
active, execute method is based on state first, and only then on type checks -
smaller and possibly better optimized result. If no specialization matches, then
fall through to ~executeAndSpecialize~ which invalidates any currently compiled
using ~CompilerDirectives.transferToInterpreterAndInvalidate~ and sets state bits
for newly activated specializations.

(@Specialization(Replaces=[""]))


How to run? Need to wrap in a ~RootNode~, which represents executable things like
methods, functions and programs. Then create a ~CallTarget~ using
~Truffle.getRuntime().createCallTarget(root)~. Truffle uses CallTargets to record,
among others, how often a particular graph is called, and when to compile
it. Also it creates a VirtualFrame for this call target out of the provided
arguments.

IGV receives JIT compilation output - shows the Graal graphs produced during
optimization. Compilation is only triggered after a certain threshold of calls,
so we need to run a call target more than just once.

todo:Graal-graph

Another option: a CountNode (public int counter; execute = counter++). Green ==
state, grey is floating (not flow dependent), blue lines represent data flow
(data dependencies), red means control flow (order of operations)

*** Object Storage Model
cite:grimmer15_polyglot - Polyglot + OSM intro

- cite:mouton19_urilang - URI language, object-oriented

- OSM (Object Storage Model) - Frame (~typed HashMap)
- VirtualFrame - virtual/optimizable stack frame, "a function's/program's scope"
- MaterializedFrame - VirtualFrame in a specific form, not optimizable, can be
  stored in a ValueType

FrameDescriptor - shape of a frame
FrameSlot
FrameSlotKind

VirtualFrame - can be optimized, reordered

MaterializedFrame - an explicit Java Object on the heap, created from a
VirtualFrame by calling ~frame.materialize()~

Object Storage Model is a common way to organize data with layouts - Objects,
Shapes, Layouts.

TruffleRuby uses it for FFI to access Ruby objects directly as if they were C
~structs~. DynSem uses these to model frames/scopes instead of Frames, as it is a
meta-interpreter and uses Truffle Frames for its own data.

*** Dispatch
[[todo:dispatchNode, frames, argument passing, ExecutableNode, RootNode, CallTarget]]

- RootNode - can be made into a CallTarget. is at the root of a graph, "starting
  point of a function/program/builtin", "callable AST"

Direct/IndirectCallNode

VirtualFrames can be eliminated altogether, which results in highly efficient code

*** Caching
[[todo:@Cached()]]

*** Polyglot
[[todo:ValueTypes, InteropLibrary]]

cite:grimmer15_polyglot - Polyglot + OSM intro

*** Structure
[[todo:Common components - Launcher, LanguageRegistration, Nodes, Values, REPL (5 items)]]

Engine, Context, TruffleLanguage, Instrument

#+begin_src text
  N: unbounded
  P: N for exclusive, 1 for shared context policy
  L: number of installed languages
  I: number of installed instruments

  - 1 : Host VM Processs
   - N : Engine
     - N : Context
       - L : Language Context
     - P * L : TruffleLanguage
     - I : Instrument
       - 1 : TruffleInstrument
#+end_src

** Functional Truffle languages
*** Criteria
Evaluate languages on:
- overall project structure and runtime flow
- global/local names and environment handling
- calling convention
- lazy evaluation
- closure implementation
- graph manipulation, TruffleBoundaries, specializations

*** Truffled PureScript
#+COMMENT: https://github.com/slamdata/truffled-purescript/

Old project, but one of the only purely-functional Truffle languages.

Purescript is a derivative of Haskell, originally aimed at frontend
development. Specific to Purescript is eager evaluation order, so the Truffle
interpreter does not have to implement thunks/delayed evaluation.

Simple node system compared to other implementations:
- types are double and Closure (trivial wrapper around a RootCallTarget and a MaterializedFrame)
- VarExpr searches for a variable in all nested frames by string name
- Data objects are a HashMap
- ClosureNode materializes the entire current frame
- AppNode executes a closure, and calls the resulting function with a { frame, arg }
- CallRootNode copies its single argument to the frame
- IR codegen creates RootNodes for all top-level declarations, evaluates them,
  stores the result, saves them to a module Frame
- Abstraction == single-argument closure

*** Mumbler
An implementation of a Lisp

*** FastR
One of the larger Truffle languages

cite:stadler16_fastr

Replacement for GNU R, which was "made for statistics, not performance"

Faster without Fortran than with (no native FFI boundary, allows Graal to
optimize through it)

Interop with Python, in particular - scipy + R plots

Node replacement for specializing nodes, or when an assumption gets invalidated
and the node should be in a different state (AbsentFrameSlot,
ReplacementDispatchNode, CallSpecialNode, GetMissingValueNode, FunctionLookup.

#+begin_src kotlin
val ctx = Context.newBuilder("R").allowAllAccess(true).build();
ctx.eval("R", "sum").execute(arrayOf<Int>(1,2,3));
#+end_src

#+begin_src R
benchmark <- function(obj) {
    result <- 0L
    for (j in 1:100) {
       obj2 <- obj$objectFunction(obj)
       obj$intField <- as.integer(obj2$doubleField)
       for (i in 1:250) { result <- obj$intFunction(i, obj$intField) }
    }
    result
}
benchmark(.jnew("RJavaBench"))
#+end_src

Special features:
- Promises (call-by-need + eager promises)

*** Cadenza

- FrameBuilder - specialized MaterializedFrame
- Closure - rather convoluted-looking code

Generating function application looks like:
- TLam - creates Root, ClosureBody, captures to arr, arg/envPreamble
- Lam - creates Closure, BuilderFrame from all captures in frame
- Closure - is a ValueType, contains ClosureRootNode
- ClosureRootNode - creates a new VirtualFrame with subset of frame.arguments

*** Enso
A very late addition to this list, this is a project that originally rejected
Truffle (and dependent types in general, if I recall correctly) and used Haskell
instead. However, the project Luna was renamed to Enso, and rebuilt from scratch
using Truffle and Scala not long before my thesis deadline.

*** TruffleClojure
Implemented in a Master's thesis cite:feichtinger15_clojure

*** DynSem
- cite:vergu19_scopes - OSM for frames/scopes

* Language implementation: MontunoTruffle
  :PROPERTIES:
  :CUSTOM_ID: jit-interpreter
  :END:
** Introduction
Truffle is not primarily aimed at statically-typed languages or functional
languages. Its most easily accessible benefits lie in speculative optimization
of dynamically typed code and inline caches, where generic object-oriented code
can be specialized to a specific value type. Statically-typed languages have a
lot more information regarding the values that will flow through a function, and
e.g. GHC has a specific /specialization/ compiler pass.

However, there is a lot of overlap between the static optimizations done by
e.g. GHC and runtime optimizations done by Graal. An example would be
unfolding/inlining, where the compiler needs to make a single decision of
whether to replace a call to a function with its definition -- a decision that
depends on the size of the definition, whether they are in the same module, and
other heuristics (inline:https://www.microsoft.com/en-us/research/wp-content/uploads/2002/07/inline.pdf). A Truffle interpreter would be able to postpone the decision
until execution time, when the definition could be inlined if the call happened
enough times.

[[todo:mental model = graph through which values flow, values may contain graphs]]

Its execution model is a tree of nodes where each node has a single operation
~execute~ with multiple specializations. The elaboration/evaluation algorithm from
the previous chapter, however, has several interleaved algorithms (infer, check,
evaluate, quote) that we first need to graft on to the Truffle execution model.

There are also several features that we require that are not a natural fit for
it, but where we can find inspiration in other Truffle languages. In particular,
lazy evaluation (FastR promises), partial function application (Enso), ???

We also have several options with regard to the depth of embedding: The most
natural fit for Truffle is term evaluation, where a term could be represented as
a value-level Term, and a CallTarget that produces its value with regard to the
current environment. We can also embed the bidirectional elaboration algorithm
itself, as a mixture of infer/check nodes.

In dynamic interpreters that Truffle is aimed at, it is easy to think of the
interpreter structure as "creating a graph through which values flow".

[[todo:Move this elsewhere ↓]]

There are several concerns here:
- algorithmic improvement is asymptotic -- the better algorithm, the better we
  can optimize it
- Truffle's optimization is essentially only applicable to "hot code", code that
  runs many times, e.g. in a loop
- We need to freely switch between Term and Value representations using
  eval/quote

The representation is also quite different from the functional interpreter where
we've used functions and data classes, as in Truffle, all values and operations
need to be classes.

[[todo:Restructure, clarify]]
Specific changes:
- everything is a class, rewrite functions/operations as classes/nodes
- annotations everywhere
- function dispatch is totally different
- lazy values need to be different
- ???

** Parser
ParsingRequest/InlineParsingRequest

Unfortunately, Truffle requires that there is no access to the interpreter state
during parsing, which means that we need to perform elaboration inside of a
~ProgramRootNode~ itself, "during runtime" per se.

[[todo: need to perform elaboration inside a programRootNode (not while parsing)]]

[[todo:stmt;stmt;expr -> return a value]]

** Representing functions
As with the previous interpreter, we will start with function calls.

[[todo:dispatch, invoke, call Nodes, argument schema (copying), ?]]

[[todo:eta is TailCallException (2 para + example)]]

Passing arguments - the technical problem of copying arguments to a new stack
frame in the course of calling a function.

Despite almost entirely re-using the Enso implementation of function calls, with
the addition of implicit type parameters and without the feature of default
argument values,

I will nonetheless keep my previous analysis of calling conventions in
functional Truffle languages here, as it was an important part of designing an
Truffle interpreter and I spent not-insignificant amounts of time on it.

I have discovered Enso only a short while before finishing my thesis, and had to
incorporate the technologically-superior solution

Several parts of creating an AST for function calls:
- determining the position of arguments on the original stack - or evaluating
  and possibly forcing the arguments
- determining the argument's position on the stack frame of the function
- using this position in the process of inferring the new function call
- dispatch, invoke, call nodes???

** Representing environments and variables
[[todo:We need to use arrays, Collections are not recommended]]

[[todo:Frames and frame descriptors for local/global variable]]

[[todo:References, indices, uninitialized references]]

** Evaluation order
[[todo:We need to defer computations as late as possible - unused values that
will be eliminated (1 para)]]

[[todo:CBPV concepts, thunks with CallTargets (3 paras, example)]]

** Calling convention
[[todo:the need for the distinction - in languages with currying]]

[[todo:the eval/apply paper is a recipe for a stack-based implementation of
currying and helpful in our case when we need explicitly manage our stack via
Frames as opposed to the interpreter where we relied on the host language for
this functionality]]

known/unknown calls, partially/fully/over-saturated calls

cite:marlow04_fast_curry

push-enter - arguments are pushed onto the stack, the function then takes as
many as it requires

eval-apply - the caller sees the arity of the function and then decides whether
it is over-applied (evaluates the function and creates a continuation), appllied
exactly (EVAL), or under-applied (creates a PAP, a closure-like value)

[[todo:exactly describe the rules from eval-apply paper KNOWNCALL, EXACT, CALLK, PAP]]

** Value types
[[todo:Data classes with call targets]]

[[todo:...depends on what will work]]

Term and Val are ValueTypes that contain a callTarget - eval/quote(?)

We could use Objects/Shapes/Layouts for dependent sums or non-dependent named
coproducts.

** User interface
REPL needs to be implemented as a ~TruffleInstrument~, it needs to modify and
otherwise interact with the language context.

** Polyglot
Demonstrate calling Montuno from other languages

Demonstrate Montuno's eval construct

Demonstrate Montuno's FFI construct - requires projections/accessors

** Implementation
[[todo:How to connect this together?]]

evaluation phases - translate to Code, run typecheck, run eval vs glued, ???

[[todo:How to edit the REPL]]

[[todo:Language registration in mx/gu]]

** Results
[[todo:Same evaluation as in previous section]]

[[todo:how fast are we now? Are there any space/time leaks?]]

[[todo:show graphs: id, const, const id; optimized graphs]]

* Optimizations: Making MontunoTruffle fast
   :PROPERTIES:
   :CUSTOM_ID: optimizations
   :END:
** Possible performance problem sources
[[todo:Reiterate JGross again]]

[[todo:how to find out whether X is relevant to us or not? How to prove the effect of JIT?]]

[[todo:Show asymptotes - binders, terms, sizes]]
[[todo:Show the graphs - large values, many iterations (warmup), sharing]]
** Possible optimizations
[[todo:What does Enso do, optimization phases?]]
[[todo:What can we do?]]

Hash consing = sharing structurally equal values in the environment. See below from Kmett:
https://gist.github.com/ekmett/6c64c3815f9949d067d8f9779e2347ef

Inlining, let-floating

Avoid thunk chaining: box(box(box(() => x))

"Immutable, except to simplify" + assumptions
Maximize evaluation sharing - globals, cache, ?

- cite:blaguszewski10_llvm - potential optimizations, LLVM impl, closures
- cite:gross14_coq_experience - Coq experience, a few reasons, comparison
- cite:gross21_performance - a lot of reasons in Coq
- cite:eisenberg20_stitch - CSE

Ruby uses threads, can we? Automatic parallelism
- cite:reid98_resumable_holes - concurrency & parallelism in GHC evaluation
- cite:hughes82_supercombinators - CAFs? Lazy evaluation?

Think about the fast vs slow path!

[[todo:Show before and afters for each optimization]]

- cite:zheng17_deoptimization - reasons for deoptimization

OSM in DynSem:
- DynSem also had to consider concept mapping: a program graph starts with
  generic node operations that immediately specialize to language-specific
  operations during their first execution
- HashMaps are efficient, but bring downsides. The Graal compiler cannot see
  inside the HashMap methods, and so cannot analyze data flow in them and use it
  to optimize them.
- DynSem also had to deal with runtime specification of environment manipulation
  as this is also supplied by the language specification. Also split between
  local short-lived values inside frames, and long-lived heap variables.
- Relevant to us is their use of the Object Storage Model, which they use to
  model variable scoping which is the processed into fixed-shape stack frames
  (separate from the Truffle Frames, this is a meta-interpreter). OSM's use case
  is ideal for when all objects of a certain type have a fixed shape. This is
  ideal for us, as tuples and named records have, by definition, a fixed shape
  (unlike Ruby etc. we do not support dynamic object reshaping, obviously).
- They did it separately from the Virtual/MaterializedFrame functionality to
  avoid the overhead of MaterializedFrames that Graal cannot optimize away.
- Truffle/Graal discourage the use of custom collections, and instead push
  developers towards Frames (which support by-name lookups) and Objects (same).

To enhance compilation specialization/inlining:
- Visualizations of call graphs - whether or not node children are stable calls
- Most DynSem calls are not stable calls, they are dispatched on runtime based
  on arguments - something that Graal does not see as stable (CompilationFinal)
- Two types of rules: mono- and polymorphic. based on whether they are called
  with different types of values at runtime. Poly- are not inlined
- DynSem found two types: dynamic dispatch (meta-interpreter depended on runtime
  info), and structural dispatch (based on the program AST and not on
  values). This is similar to our EvalNode, QuoteNode and similar, which depend
  on the type of the value
- Overloaded rules--rules with the same input shape--are merged into a single
  FusedRule node and iterated over with @ExplodeLoop.
- For mono/polymorphic rules, they use an assumption that a rule is monomorphic,
  specialize the rule, and recompile if it becomes polymorphic.
- Inlining nodes - polymorphic rules reduced to a set of monomorphic rules - a
  rule from the registry is cloned in an uninitialized state in a monomorphic
  call site and "inlined" (in a CompilationFinal field)
- They use a central registry of CallTargets that contain rules that they can
  clone and adopt locally if necessary to specialize--we can do the same!
- Disadvantages: there is more to compile and inline by Graal, instead of a
  CallTarget, they use a child. Likely to take longer to stabilize, but faster
  in the end.
** Glued evaluation
An optimization technique that attempts to avoid even more computation.

Parallel operation on two types of values, glued and local. Glued are lazily
evaluated to a fully unfolded form; local are eagerly computed to a head-normal
form but not fully unfolded, to prevent size explosions. This results in better
performance in a large class of programs, although it is not an asymptotic
improvement, as we have a small eagerly evaluated term for quoting, and a large
lazily evaluated for conversion checking.

This is another case of specialization: we have two operations to perform on the
same class of values, but each operation has its own requirements; in this case,
on the size of the terms as in quoting we want a small folded value but require
the full term for conversion checking.

cite:kaposi19_gluing

https://eutypes.cs.ru.nl/eutypes_pmwiki/uploads/Meetings/Kovacs_slides.pdf

** Splitting/type specializations/dict passing
** Optimizing function dispatch
[[todo:lambda merging, eta expansion]]

** Caching, sharing
[[todo:sharing common values, multiple references to the same object]]

** Common FP optimizations
[[todo:floating, inlining by hand]]

** Specializations
*** Truffle recommended optimizations
The optimization workflow recommended by the Truffle developers is as follows:
1. Run with a profiler to sample the application and identify responsible
   compilation units. Use a sampling delay (--cpusampler.Delay=MILLISECONDS) to
   only profile after warmup. See the Profiling guide.
2. Understand what is being compiled and look for deoptimizations. Methods that
   are listed to run mostly in the interpreter likely have a problem with
   deoptimization.
3. Simplify the code as much as possible where it still shows the performance
   problem.
4. Enable performance warnings and list boundary calls.
5. Dump the Graal graph of the responsible compilation unit and look at the
   phase After TruffleTier.
   1. Look at the Graal graphs at the phases After TruffleTier and After
      PartialEscape and check if it is what you would expect. If there are nodes
      there that you don't want to be there, think about how to guard against
      including them. If there are more complex nodes there than you want, think
      about how to add specialisations that generate simpler code. If there are
      nodes you think should be there in a benchmark that are not, think about
      how to make values dynamic so they are not optimized away.
6. Search for Invoke nodes in the Graal IR. Invoke nodes that are not
   representing guest language calls should be specialized away. This may not be
   always possible, e.g., if the method does I/O.
7. Search for control flow splits (red lines) and investigate whether they
   result from control flow caused by the guest application or are just
   artifacts from the language implementation. The latter should be avoided if
   possible.
8. Search for indirections in linear code (Load and LoadIndexed) and try to
   minimize the code. The less code that is on the hot-path the better.

---
[[todo:More on splitting!]]

- ~--engine.TraceCompilation~ prints a line for each method compilation
- ~--engine.TraceCompilationDetail~ prints a line for compilation queuing, start, and finish
- ~--engine.TraceCompilationAST~ prints the entire compiled AST
- ~--engine.TraceInlining~ prints inlining decision details
- ~--engine.TraceSplitting~ prints splitting decisions
- ~--engine.TraceTransferToInterpreter~ prints a stack trace for each explicit invalidation
- ~--engine.TracePerformanceWarnings=(call|instanceof|store|all)~
  - ~call~ prints when PE cannot inline a call
  - ~instanceof~ prints when PE cannot resolve virtual ~instanceof~ to a specific type
  - ~store~ prints when PE store location argument is not compilation final
- ~--engine.CompilationStatistics~ prints total compilation statistics
- ~--engine.CompilationStatisticDetails~ prints compilation histograms for each node
- ~--engine.TraceMethodExpansion=truffleTier~ prints a tree of all expanded Java methods
- ~--engine.TraceNodeExpansion=truffleTier~ prints a tree of all expanded Nodes
- ~--engine.MethodExpansionStatistics=truffleTier~ prints total Graal nodes produced by a method
- ~--engine.NodeExpansionStatistics=truffleTier~ also includes Graal specializations
- ~--engine.InstrumentBoundaries~ prints info about boundaries encountered (should be minimized)
- ~--engine.InstrumentBranches~ prints info about branch profiles
- ~--engine.SpecializationStatistics~ prints details about specializations performed
- ~--vm.XX:+TraceDeoptimization~ prints all deoptimizations
- ~--vm.XX:+TraceDeoptimizationDetails~ prints all deoptimizations with details

It is also possible to control what is being compiled, add details to IGV graphs
dumped, and print the exact assembly produced: see
https://github.com/oracle/graal/blob/master/truffle/docs/Optimizing.md.

*** How to debug specializations
*Specialization histogram:* If compiled with
~-Atruffle.dsl.GenerateSpecializationStatistics=true~ and executed with
~--engine.SpecializationHistogram~, Truffle DSL will compile the nodes in a
special way and show a table of the specializations performed during the execution of a
program.

Example shown at
https://github.com/oracle/graal/blob/master/truffle/docs/SpecializationHistogram.md,
maybe include the table?

*Slow path only:* If compiled with ~-Atruffle.dsl.GenerateSlowPathOnly=true~,
Truffle will only execute the last, most generic specialization, and will
ignore all fast path specializations.
** (Profiling)
[[todo:then, what tools to use to find the problems]]

*** Ideal Graph VIsualizer
A graphical program that serves to visualize the process of Truffle graph
optimization. When configured correctly, the IGV will receive the results of all
partial evaluations.

*** CPU Sampler
#+COMMENT: https://github.com/oracle/graal/blob/master/truffle/docs/Profiling.md

Running the language launcher with the options ~--cpusampler
--cpusampler.Delay=MILLISECONDS~ will start the CPU sampler. This tool serves to
profile the guest language (as opposed to the regular JDK Async Profiler which
will profile the entire process.

~--cpusampler.Delay~ helps to not include warm-up time in the results.

Using additional options (~--cpusampler --cpusampler.SampleInternal
--cpusampler.Mode=roots --cpusampler.Output=json~) and postprocessing the
generated JSON with an additional script we
can create a so-called flamegraph with the results of the sampling.

#+COMMENT: https://github.com/eregon/FlameGraph/blob/graalvm/stackcollapse-graalvm.rb

* Evaluation (and next work): Is MontunoTruffle fast enough?
  :PROPERTIES:
  :CUSTOM_ID: evaluation
  :END:
** Evaluation
[[todo:Total evaluations, performance matrix]]

[[todo:Compare asymptotics now, with Agda, ...]]
** Next work
FFI, tooling

RPython, K Framework - exploration

SPMD on Truffle, Array languages

More type extensions OR totality (as a proof assistent)

Finite types, universes, no type in type, HoTT, CoC

[[todo:Is this useful at all? What's the benefit for the world? (in evaluation)]]

next work: LF, techniques, extensions, real language

* Conclusion
We tried X to do Y. It went well and we fulfilled the assignment.

As a side effect, I produced a reference book for functional/dependent
language implementation.

Original goal was X, it grew to encompass Y, Z as well.

* (bibliography, start of appendix)                           :ignoreheading:

#+BEGIN_EXPORT latex
\makeatletter
\def\@openbib@code{\addcontentsline{toc}{chapter}{Bibliography}}
\makeatother
\begin{flushleft}
#+END_EXPORT

bibliographystyle:bibstyle
bibliography:bibliography.bib

#+BEGIN_EXPORT latex
\end{flushleft}
\iftwoside\cleardoublepage\fi
\appendix
\appendixpage
\iftwoside\cleardoublepage\fi
\startcontents[chapters]
% \setlength{\parskip}{0pt}
% \printcontents[chapters]{l}{0}{\setcounter{tocdepth}{2}}
% \setlength{\parskip}{0.5\bigskipamount}
\iftwoside\cleardoublepage\fi
#+END_EXPORT

* Contents of the attached data storage
...
* Grammar
#+include: "../montuno/src/main/antlr/montuno/Montuno.g4" src antlr -n

* Footnotes

[fn:8] FastR is between 50 to 85x faster than GNU R, depending on the source. cite:fumero17_jit_gpu

[fn:7] Unfortunately, there are no officially published benchmarks, but a number
of articles claim that TruffleRuby is 10-30x faster than the official C
implementation. cite:shopify2020

[fn:6]Kotlin authors claim 40% reduction in the number of lines of code, (from
https://kotlinlang.org/docs/faq.html)
[fn:5]https://kotlinlang.org/docs/idioms.html
[fn:4] In particular, ANTLR-provided visitors require that all return values
share a common super-class. Listeners don't allow return values and would
require explicit parse tree manipulation.
[fn:3]https://github.com/antlr/grammars-v4/
[fn:2]https://www.antlr.org/
[fn:1]Even though Kotlin seems not to be recommended by Truffle authors, there
are several languages implemented in it, which suggests there are no severe
problems.  "[...] and Kotlin might use abstractions that don't properly
partially evaluate." (from https://github.com/oracle/graal/issues/1228)
