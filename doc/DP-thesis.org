#+BEGIN_COMMENT
Normostrana = 1800c ≈ 250 slov
org-word-count
org-wc-display

16.4. 2303w
17.4. 3023w
18.4. 3600w? (Σ36p)
19.4. 4000w? (Σ40p)
22.4. 5200w
24.4. 6700w
25.4. 7200w (Σ56p)
26.4. 7660w (Σ59p)
27.4. 8800w (Σ64p)
28.4. 10050w (Σ67p)
29.4. 11700w (Σ68p)
30.4. 12500w (Σ74p, 52p content)
3.5. 14000w (Σ81p, Σ58p content)
5.5. 14500w (Σ79p)
6.5. 15200w (Σ77p)
10.5. 18800w (Σ81p)
11.5. 20900w

minimum: 12,500
target: 17,500
maximum: 30,000
#+END_COMMENT
* (front matter)                                              :ignoreheading:
#+LANGUAGE: en
#+OPTIONS: texht:nil toc:nil author:nil ':t H:6 num:3
#+LATEX_CLASS: fitthesis
#+LATEX_CLASS_OPTIONS: [english,zadani,odsaz]
#+EXCLUDE_TAGS: noexport
# print = B&W links and logo
# cprint = B&W links, color logo
#+BIND: org-latex-title-command ""
#+BIND: org-latex-prefer-user-labels t
#+BIND: org-latex-default-figure-position "hbt"
#+BEGIN_EXPORT latex
\maketitle
\setlength{\parskip}{0pt}
{\hypersetup{hidelinks}\tableofcontents}
\iftotalfigures\listoffigures\fi
\iftotaltables\listoftables\fi
\iftotallistings\listoflistings\fi
\listoftodos
\iftwoside\cleardoublepage\fi
\setlength{\parskip}{0.5\bigskipamount}
#+END_EXPORT
* (Recommended thesis structure)                                   :noexport:
1. "Úvod do knížečky", jedna strana, bez podkapitol
2. "Co bylo třeba vystudovat?" - "Je tato informace potřebná k pochopení toho, co jsem udělal?"
3. "Nové myšlenky" - rozhodl jsem, vymyslel, rozvrhl, odvodil, vylepšil, navrhl, ...
4. "Implementace a vyhodnocení" - udělal jsem, posbíral data, výsledky
5. "Závěr" - žádné nové poznatky, možnosti pokračování, "naladit oponenta"

* (Casual outline)                                                 :noexport:
*Story:*
- Let's build a fast Agda!
- Why Agda, what's at its core?
- Oh, dependent types? What's that good for, how does it work?
- Why are all of these languages functional?
- Lambda calculus and everything - that's a lot of formalisms...
- How to build it? Fast λs, that's a start, then build it up.
- So, does it work? What can I use it for, does it fulfill the design goals?
- Why is it so slow? Asymptotics...
- So, Truffle should make it fast, what specifically? What's the inspiration?
- How do we start, what needs to be changed?
- Wow, that's a lot of concepts, slow down, explain!
- So, does this changed work function at all? What about polyglots? What UIs?
- What can we make improve, was this just the start?
- Did we fulfill our goals, is this useful at all?
- If this is somebody's starting point, what will they need to know?

*Original goal:*
- efficient βη-normalization runtime
- Show the asymptotics on Peano, Church numbers

*Side outcome:* compilation of all the relevant concepts.

- Citations: Werthinger et al. [45] have developed [...]

eqref:eq:1
#+begin_export latex
\begin{equation}\label{eq:1}
 ≡ Π_{x:A}B(x)
\end{equation}
#+end_export

* Introduction
#+COMMENT: Motivation: why this project? Why should anyone care?

Proof assistants like Coq, F*, Agda or Idris, or other languages with dependent
types like Cayenne or Epigram, allow programmers to write provably
correct-by-construction code in a manner similar to a dialog with the compiler
cite:norell08_agda_tutorial. They also face serious performance issues when
applied to problems or systems on a large-enough scale
cite:gross14_coq_experience
cite:gross21_performance. Their performance grows exponentially with the number
of lines of code in the worst case cite:nawaz19_survey_provers, which is a
significant barrier to their use. While many of the performance issues are
fundamentally algorithmic, a better runtime system would improve the
rest. However, custom runtime systems or more capable optimizing compilers are
time-consuming to build and maintain. This thesis seeks to answer the question
of whether just-in-time compilation can help to improve the performance of such
systems.

Moving from custom runtime systems to general language platforms like e.g., the
Java Virtual Machine (JVM) or RPython cite:bolz14_meta, has improved the
performance of several dynamic languages: project like TruffleRuby, FastR, or
PyPy. It has allowed these languages to re-use the optimization machinery
provided by these platforms, improve their performance, and simplify their
runtime systems.

#+COMMENT: Problem definition: What exactly are you solving? What is the core and what is a bonus? What parameters should a proper solution to the problem have?

As there are no standard benchmarks for dependently typed languages, we design a
small, dependently-typed core language to see if using specific just-in-time
(JIT) compilation techniques produces asymptotic runtime improvements in the
performance of β-normalization and βη-conversion checking, which are among the
main computational tasks in the elaboration process and is also the part that
can most likely benefit from JIT compilation. The explicit non-goals of this
thesis are language completeness and interoperability, as neither are required
to evaluate runtime performance.

#+COMMENT: Existing solutions: be fair in identifying their strengths and weaknesses. Cite important works from the field of your topic. Try to define well what is the state of the art.

State-of-the-art proof assistants like Coq, Agda, Idris, or others is what we
can compare our results with. There are also numerous actively developed
research projects in this area; Lean is a notable one that I found too late in
my thesis to incorporate its ideas. However, the primary evaluation will be
against the most well established proof assistants.

inline:Reformulate As for the languages that use Truffle, the language implementation framework
that allows interpreters to use the JIT optimization capabilities of GraalVM, an
alternative implementation of the Java Virtual Machine: there are numerous
general-purpose functional languages, the most prominent of which are
TruffleRuby and FastR. Both were reimplemented on the Truffle platform,
resulting in significant performance improvements[fn:7][fn:8]. We will
investigate the optimization techniques they used, and reuse those that are
applicable to our language.

There is also a number of functional languages on the Java Virtual Platform that
do not use the Truffle platform, like Clojure, Scala or Kotlin, as well as
purely functional languages like Eta or Frege. All of these languages compile
directly to JVM byte code: we may compare our performance against their
implementation, but we would not be able to use their optimization
techniques. To the best of my knowledge, neither meta-tracing nor partial
evaluation have been applied to the dependently-typed lambda calculus.

The closest project to this one is Cadenza cite:kmett_2019, which served as the
main inspiration for this thesis. Cadenza is an implementation of the
simply-typed lambda calculus on the Truffle framework. While it is unfinished
and did not show as promising performance compared to other simply-typed lambda
calculus implementations as its author hoped, this project applies similar ideas
to the dependently-typed lambda calculus, where the presence of type-level
computation should lead to larger gains.

#+COMMENT: Our solution: Make a quick outline of your approach, pitch your solution

inline:Rephrase In this thesis, I will use the Truffle framework to evaluate how
well are the optimizations provided by the just-in-time compiler GraalVM
suitable to the domain of dependently-typed languages. GraalVM helps to turn
slow interpreter code into efficient machine code by means of /partial evaluation/
cite:wurthinger13_graal. During partial evaluation, specifically the second
Futamura projection cite:latifi19_futamura, an interpreter is specialized
together with the source code of a program, yielding executable code. Parts of
the interpreter could be specialized, some optimized, and some could be left off
entirely. Depending on the quality of the specializer, this may result in
performance gains of several orders of magnitude.

Truffle makes this available to language creators, they only need to create an
interpreter for their language. It also allows such interpreters to take
advantage of GraalVM's /polyglot/ capabilities, and directly interoperate with
other JVM-based languages, their code and values
cite:sipek19_polyglot. Development tooling can also be derived for Truffle
languages quite easily cite:stolpe19_environment. Regardless of whether Truffle
can improve their performance, both of these features would benefit
dependently-typed or experimental languages.

#+COMMENT: Contributions: Sell your solution. Pinpoint your achievements. Be fair and objective.

While this project was originally intended just as a λΠ calculus compiler and an
efficient runtime, it has ended up much larger due to a badly specified
assignment. I also needed to study type theory and type checking and elaboration
algorithms that I have used in this thesis, and which form a large part of
chapters ref:lambda and ref:interpreter.

Starting from basic λ-calculus theory and building up to the systems of the
lambda cube, we specify the syntax and semantics of a small language that I
refer to as Montuno (Chapter ref:lambda). We go through the principles of
λ-calculus evaluation, type checking and elaboration, implement an interpreter
for Montuno in a functional style (Chapter ref:interpreter).  In the second part
of the thesis, we evaluate the capabilities offered by Truffle and the
peculiarities of Truffle languages, and implement an interpreter for Montuno
using the Truffle framework (Chapter ref:jit-interpreter), and apply various JIT
optimizations to it (Chapter ref:optimizations). After designing and using a set
of benchmarks to evaluate the language's performance, we close with a large list
of possible follow-up work (Chapter ref:evaluation).

* Language specification: λ⋆-calculus with extensions
  :PROPERTIES:
  :CUSTOM_ID: lambda
  :END:
** Introduction
Proof assistants like Agda or Idris are built around a fundamental principle
called the Curry-Howard correspondence that connects type theory and
mathematical logic, demonstrated in Figure ref:ch-logic. In simplified terms it
says that given a language with a self-consistent type system, writing a
well-typed program is equivalent to proving its correctness
cite:baez10_rosetta. It is often shown on the correspondence between natural
deduction and the simply-typed λ-calculus, as in Figure ref:ch-deduction. Proof
assistants often have a small core language around which they are built:
e.g. Coq is built around the Calculus of Inductive Constructions, which is a
higher-order typed λ-calculus.

#+label: ch-logic
#+CAPTION: Curry-Howard correspondence between mathematical logic and type theory
#+ATTR_LaTeX: :options [!htb]
#+begin_figure latex
\captionsetup{aboveskip=-3pt}
\begin{center}
\begin{tabular}{c|c}
Mathematical logic & Type theory \\\hline\\[-1em]
\shortstack{$⊤$ \\ true} &
\shortstack{$()$ \\ unit type} \\
\shortstack{$⊥$ \\ false} &
\shortstack{$∅$ \\ empty type} \\[3pt]
\shortstack{$p ∧ q$ \\ conjunction} &
\shortstack{$a × b$ \\ sum type} \\[3pt]
\shortstack{$p ∨ q$ \\ disjunction} &
\shortstack{$a + b$ \\ product type} \\[3pt]
\shortstack{$p ⇒ q$ \\ implication} &
\shortstack{$a → b$ \\ exponential (function) type} \\[5pt]
\shortstack{$∀x ∈ A, p$ \\ universal quantification} &
\shortstack{$Π_{x : A}B(x)$ \\ dependent product type} \\[5pt]
\shortstack{$∃x ∈ A, p$ \\ existential quantification} &
\shortstack{$Σ_{x : A}B(x)$ \\ dependent sum type} \\[5pt]
\end{tabular}
\end{center}
#+end_figure

#+label: ch-deduction
#+CAPTION: Curry-Howard correspondence between natural deduction and λ→-calculus
#+ATTR_LaTeX: :options [!htb]
#+begin_figure latex
\begin{center}
\begin{tabular}{c|c}
Natural deduction & λ→ calculus \\\hline\\[-1em]
\shortstack{
\AxiomC{}
\UnaryInfC{$Γ₁, α, Γ₂ ⊢ α$}
\DisplayProof \\ axiom} &
\shortstack{
\AxiomC{}
\UnaryInfC{$Γ₁, x : α, Γ₂ ⊢ x : α$}
\DisplayProof \\ variable} \\[7pt]

\shortstack{
\AxiomC{$Γ, α ⊢ β$}
\UnaryInfC{$Γ ⊢ α → β$}
\DisplayProof \\ implication introduction} &
\shortstack{
\AxiomC{$Γ, x : α ⊢ t : β$}
\UnaryInfC{$Γ ⊢ λx. t: α → β$}
\DisplayProof \\ abstraction} \\[7pt]

\shortstack{
\AxiomC{$Γ ⊢ α → β$}
\AxiomC{$Γ ⊢ α$}
\BinaryInfC{$Γ ⊢ β$}
\DisplayProof \\ modus ponens} &
\shortstack{
\AxiomC{$Γ ⊢ t : α → β$}
\AxiomC{$Γ ⊢ u : α$}
\BinaryInfC{$Γ ⊢ t u : β$}
\DisplayProof \\ application} \\[7pt]
\end{tabular}
\end{center}
#+end_figure

Compared to the type systems in languages like Java, dependent type systems can
encode much more information in types. We can see the usual example of a list
with a known length in Listing ref:idris-vect: the type ~Vect~ has two parameters,
one is the length of the list (a Peano number), the other is the type of its
elements. Using such a type we can define safe indexing operators like ~head~,
which is only applicable to non-empty lists, or ~index~, where the index must be
given as a finite number between zero and the length of the list (~Fin len~). List
concatenation uses arithmetic on the type level, and it is possible to
explicitly prove that concatenation preserves list length.

#+label: idris-vect
#+caption: Vectors with explicit length in the type, source: the Idris base library
#+begin_src idris
  data Vect : (len : Nat) -> (elem : Type) -> Type where
    Nil  : Vect Z elem
    (::) : (x : elem) -> (xs : Vect len elem) -> Vect (S len) elem

  -- Definitions elided
  head : Vect (S len) elem -> elem
  index : Fin len -> Vect len elem -> elem
  (++) : (xs : Vect m elem) -> (ys : Vect n elem) -> Vect (m + n) elem

  proofConcatLength
    : {m, n : Nat} -> {A : Type} -> (xs : Vect n A) -> (ys : Vect m A)
      -> length (xs ++ ys) = length xs + length ys
#+end_src

On the other hand, these languages are often restricted in some ways. General
Turing-complete languages allow non-terminating programs: non-termination leads
to a inconsistent type system, so proof assistants use various ways of keeping
the logic sound and consistent. Idris, for example, requires that functions are
total and finite. It uses a termination checker, checking that recursive
functions use only structural or primitive recursion, in order to ensure that
type-checking stays decidable.

This chapter aims to introduce the concepts required to specify the syntax and
semantics of a small dependently-typed language and use these to produce such a
specification, a necessary prerequisite so that we can create interpreters for
this language in later chapters. This chapter, however, does not attempt to be a
complete reference in the large field of type theory.

** Languages
*** λ-calculus
We will start from the untyped lambda calculus, as it is the language that all
following ones will build upon. Introduced in the 1930s by Alonzo Church as a
model of computation, it is a very simple language that consists of only three
constructions: abstraction, application, and variables, written as in Figure
ref:untyped.

#+label: untyped
#+CAPTION: λ-calculus written in Church and de Bruijn notation
#+ATTR_LaTeX: :options [htb]
#+begin_figure latex
\captionsetup[subfigure]{aboveskip=-1pt}
\begin{subfigure}[t]{.5\textwidth}\centering
  \[\begin{array}{ccll}
  e & ::= & v   & \text{variable} \\
    & |   & M~N & \text{application} \\
    & |   & λv.~M & \text{abstraction}
  \end{array}\]
  \caption{Standard (Church) notation}
\end{subfigure}
\begin{subfigure}[t]{.5\textwidth}\centering
  \[\begin{array}{ccll}
  e & ::= & v     \\
    & |   & (N)~M \\
    & |   & [v]~M
  \end{array}\]
  \caption{De Bruijn notation}
\end{subfigure}
#+end_figure

**** β-reduction
The λ-abstraction $λx.~t$ represents a program that, when applied to the
expression $x$, returns the term $t$. For example, the expression $(λx.x x) t$
produces the expression $t t$. This step, applying a λ-abstraction to a term, is
called /β-reduction/, and it is the basic /rewrite rule/ of λ-calculus. Another way
of saying that is that the x is assigned/replaced with the expression T, and it
is written as the substitution $M[x≔T]$

#+LATEX: \[ (λx. t) u ⟶_β t[x≔u] \]

**** α-conversion
We however need to ensure that the variables in the substituted terms do not
overlap and if they do, we need to rename them. This is called /α-conversion/ or α
renaming. In general, the variables that are not bound in λ-abstractions, /free
variables./ may need to be replaced before every β-reduction so that they do not
become /bound/ after substitution.

#+LATEX: \[ (λx. t) ⟶_α (λy. t[x≔y]) \]

**** η-conversion
Reducing a λ-abstraction that directly applies its argument to a term or
equivalently, rewriting a term in the form of $λx.f x$ to $f$ is called
/η-reduction/. The opposite rewrite rule, from $f$ to $λx.f x$ is
$\bar{η}\text{-expansion}$, and because the rewriting works in both ways, it is
also called the /η-conversion/.

#+LATEX: \[ λx.f x ⟶_η f\]  \[ f ⟶_{\bar{η}} λx.f x \]

**** δ-reduction
β-reduction together with α-renaming are sufficient to specify λ-calculus, but
there are three other rewriting rules that we will need later: /δ-reduction/ is
the replacement of a constant with its definition.

#+LATEX: \[ id t ⟶_δ (λx.x) t\]

**** ζ-reduction
For local variables, equivalent process is called the /ζ-reduction/.

#+LATEX: \[ let id = λx.x in id t ⟶_ζ (λx.x) t \]

**** ι-reduction
We will also use other types of objects than just functions. Applying a function
that extracts a value from an object is called the /ι-reduction/. In this example,
the object is a pair of values, and the function $π₁$ is a projection that
extracts the first value of the pair.

#+LATEX: \[ π₁ (a, b) ⟶_ι a \]

**** Normal form
By repeatedly βδιζ-reducing an expression--applying functions to their
arguments, replacing constants and local variables with their definitions,
evaluating objects, and α-renaming variables if necessary, we get a β-normal
form, or just /normal form/ for short. This normal form is unique up to
α-conversion, according to the Church-Rossier theorem.

#+begin_export latex
\[\begin{array}{rl}
     & \text{let} pair = λm.(m,m) \text{in} π₁ (pair (id 5)) \\
⟶_ζ & π₁ ((λm.(m,m)) (id 5)) \\
⟶_β & π₁ (id 5, id 5) \\
⟶_ι & id 5 \\
⟶_δ & (λx.x) 5 \\
⟶_β & 5 \\
\end{array}\]
#+end_export

**** Other normal forms
There are also other normal forms, they all have something to do with unapplied
functions. If we have an expression and repeatedly use only the β-reduction, we
end up with a function, or a variable applied to some free variables. These
other normal forms specify what happens in such a "stuck" case. In Figure
ref:normal-forms, $e$ is an arbitrary λ-term and $E$ is a term in the relevant
normal form cite:sestoft02_reduction. Closely related to the concept of a normal
form are /normalization strategies/ that specify the order in which
sub-expressions are reduced.

#+LATEX:{\renewcommand{\arraystretch}{1.3}%
#+LABEL: normal-forms
#+CAPTION: Normal forms in λ-calculus
#+begin_figure latex
\captionsetup{aboveskip=-1pt}
\begin{center}
\begin{tabular}{ccll}
& & \multicolumn{2}{c}{Reduce under abstraction} \\\cline{3-4}
& \multicolumn{1}{c|}{} & \textbf{Yes} & \multicolumn{1}{|c|}{\textbf{No}}
\\\cline{2-4}
\multicolumn{1}{c|}{\multirow{4}{*}{\rotatebox[origin=c]{90}{Reduce args}}} &
\multicolumn{1}{c|}{\multirow{2}{*}{\textbf{Yes}}}
& $E ≔ λx.E | x E₁...Eₙ$ & \multicolumn{1}{|l|}{$E ≔ λx.e | x E₁...Eₙ$}
\\
\multicolumn{1}{c|}{} & \multicolumn{1}{c|}{}
& Normal form  & \multicolumn{1}{|l|}{Weak normal form}
\\\cline{2-4}
\multicolumn{1}{c|}{} & \multicolumn{1}{c|}{\multirow{2}{*}{\textbf{No}}}
& $E ≔ λx.E | x e₁...eₙ$ & \multicolumn{1}{|l|}{$E ≔ λx.e | x e₁...eₙ$}
\\
\multicolumn{1}{c|}{} & \multicolumn{1}{c|}{}
& Head normal form & \multicolumn{1}{|l|}{Weak head normal form}
\\\cline{2-4}
\end{tabular}
\end{center}
#+end_figure
#+LATEX: }

**** Strong normalization
An important property of a model of computation is termination, the question of
whether there are expressions for which computation does not stop. In the
context of the λ-calculus it means whether there are terms, where repeatedly
applying rewriting rules does not produce a unique normal form in a finite
sequence steps. While for some expressions this may depend on the selected
rewriting strategy, the general property is as follows: If for all well-formed
terms $a$ there does not exist any infinite sequence of reductions $a ⟶_{β}
a' ⟶_{β} a''⟶_{β} ⋯$, then such a system is called /strongly normalizing/.

The untyped λ-calculus is not a strongly normalizing system, though, and there
are expressions that do not have a normal form. When such expressions are
reduced, they do not get smaller, but they /diverge/. The ω combinator:

#+LATEX: \[ω = λx.x~x\]

is one such example that produces an infinite term. Applying ω to itself
produces a divergent term whose reduction cannot terminate:

#+LATEX: \[ω~ω ⟶_δ (λx.x x)ω ⟶_β ω~ω\]

The fixed-point function, the Y combinator, is also notable:

#+LATEX: \[Y = λf.(λx.f(x x)) (λx.f(x x))\]

This is one possible way of encoding general recursion in λ-calculus, as it
reduces by applying $f$ to itself:

#+LATEX: \[Y f ⟶_{δβ} f(Y f) ⟶_{δβ} f(f(Y f))  ⟶_{δβ} ...\]

This, as we will see in the following chapter, is impossible to encode in the
typed λ-calculus without additional extensions.

As simple as λ-calculus may seem, it is a Turing-complete system that can encode
logic, arithmetic, or data structures. Some examples include /Church encoding/ of
booleans, pairs, or natural numbers (Figure ref:church).

#+LABEL: church
#+CAPTION: Church encoding of various concepts
#+ATTR_LaTeX: :options [!htb]
#+begin_figure latex
\begin{subfigure}[b]{.5\textwidth}\centering
  \[\begin{array}{ccl}
  0 & = & λf.λx.~x \\
  1 & = & λf.λx.~f~x
  \end{array}\]
  \caption{Natural numbers}
\end{subfigure}
\begin{subfigure}[b]{.5\textwidth}\centering
  \[\begin{array}{ccll}
  succ & = & λn.λf.λx.f~(n~f~x) \\
  plus & = & λm.λn.m~succ~n
  \end{array}\]
  \caption{Simple arithmetic}
\end{subfigure}
\begin{subfigure}[b]{.5\textwidth}\centering
  \[\begin{array}{ccll}
  true & = & λx.λy.x \\
  false & = & λx.λy.y \\
  not & = & λp.p~false~true \\
  and & = & λp.λq.p~q~p \\
  ifElse & = & λp.λa.λb.p~a~b
  \end{array}\]
  \caption{Logic}
\end{subfigure}
\begin{subfigure}[b]{.5\textwidth}\centering
  \[\begin{array}{ccll}
  cons & = & λf.λx.λy.f~x~y \\
  fst & = & λp.p~true \\
  snd & = & λp.p~false \\
  \end{array}\]
  \caption{Pairs}
\end{subfigure}
#+end_figure

*** λ→-calculus
It is often useful, though, to describe the kinds of objects we work
with. Already, in Figure ref:church we could see that reading such expressions
can get confusing: a boolean is a function of two parameters, whereas a pair is
a function of three arguments, of which the first one needs to be a boolean and
the other two contents of the pair.

The untyped λ-calculus defines a general model of computation based on functions
and function application. Now we will restrict this model using types that
describe the values that can be computed with.

The simply typed λ-calculus, also written λ→ as "→" is the connector used in
types, introduces the concept of types. We have a set of basic types that are
connected into terms using the arrow →, and type annotation or assignment $x :
A$. We now have two languages: the language of terms, and the language of
types. These languages are connected by a /type judgment/, or /type assignment/ $x :
T$ that asserts that the term $x$ has the type $T$
cite:guallart15_overview_types.

**** Church- and Curry-style
There are two ways of formalizing the simply-typed λ-calculus: λ→-Church, and
λ→-Curry. Church-style is also called system of typed terms, or the explicitly
typed λ-calculus as we have terms that include type information, and we say:

#+LATEX: \[λx : A.x : A → A,\]

or using parentheses to clarify the precedence

#+LATEX: \[λ(x : A).x : (A → A).\]

Curry-style is also called the system of typed assignment, or the implicitly
type λ-calculus as we assign types to untyped λ-terms that do not carry type
information by themselves, and we say $λx.x : A → A$. cite:barendregt92_typed.

There are systems that are not expressible in Curry-style, and vice versa.
Curry-style is interesting for programming, we want to omit type information;
and we will see how to manipulate programs specified in this way in Chapter
ref:interpreter. We will use Church-style in this chapter, but our language will
be Curry-style, so that we incorporate elaboration into the interpreter.

**** Well-typed terms
Before we only needed evaluation rules to fully specify the system, but
specifying a system with types also requires typing rules that describe what
types are allowed. We will also need to distinguish /well-formed terms/ from
/well-typed terms/: well-formed terms are syntactically valid, whereas well-typed
terms also obey the typing rules. Terms that are well-formed but not yet known
to be well typed are called /pre-terms/, or terms of /pre-syntax/.

There are some basis algorithms of type theory, in brief:
- given a pre-term and a type, /type checking/ verifies if the term can be assigned the type.
- given just a pre-term and no type, /type inference/ computes the type of an expression
- and finally /type elaboration/ is the process of converting a partially
  specified pre-term into a complete, well-typed term cite:ferreira14_bidi.

**** Types and context
The complete syntax of the λ→-calculus is in Figure ref:simple-syntax.
Reduction operations are the same as in the untyped lambda calculus, but we will
need to add the language of types to the previously specified language of
terms. This language consists of a set of /base types/ which can consist of
e.g. natural numbers or booleans, and /composite types/, which describe functions
between them. We also need a way to store the types of terms that are known, a
typing /context/, which consists of a list of /type judgments/ in the form $x:T$,
which associate variables to their types.

#+label: simple-syntax
#+CAPTION: λ→-calculus syntax
#+ATTR_LaTeX: :options [bht]
#+begin_figure latex
\[\begin{array}{ccll}
e & & & (terms) \\
  & ≔ & v     & \text{variable} \\
  & | & M~N   & \text{application} \\
  & | & λx.~t & \text{abstraction} \\
  & | & x:τ   & \text{annotation} \\[5pt]
τ & & & (types) \\
  & ≔ & β      & \text{base types} \\
  & | & τ → τ' & \text{composite type} \\[5pt]
Γ & & & (typing context) \\
  & ≔ & ∅     & \text{empty context} \\
  & | & Γ,x:τ & \text{type judgement} \\[5pt]
v & & & (values) \\
  & ≔ & λx. t & \text{closure} \\[5pt]
\end{array}\]
#+end_figure

**** Typing rules
The simply-typed λ-calculus can be completely specified by the typing rules in
Figure ref:simple-types cite:pierce02_types. These rules are read similarly to
logic proof trees: as an example, the rule *App* can be read as "if we can infer
$f$ with the type $A→B$ and $a$ with the type $A$ from the context Γ, then we
can also infer that function application $f a$ has the type $B$". Given these
rules and the formula

#+LATEX: \[λa:A.λb:B.a : A→B→A\]

we can also produce a derivation tree that looks similar to logic proofs and, as
mentioned before, its semantics corresponding to the logic formula "if $A$ and $B$,
then $A$" as per the Curry-Howard equivalence.

#+begin_export latex
\begin{prooftree}
\AxiomC{}
\UnaryInfC{$a:A, b:B ⊢ a:A$}
\UnaryInfC{$a:A ⊢ λb:B. a : B→A$}
\UnaryInfC{$⊢ λa:A. λb:B. a : A→B→A$}
\end{prooftree}
#+end_export

We briefly mentioned the problem of termination in the previous section; the
simply-typed λ-calculus is strongly normalizing, meaning that all well-typed
terms have a unique normal form. In other words, there is no way of writing a
well-typed divergent term; the Y combinator is impossible to type in λ→ and any
of the systems in the next chapter cite:bove08_atwork.

#+label: simple-types
#+CAPTION: λ→-calculus typing rules
#+ATTR_LaTeX: :options [!htb]
#+begin_figure latex
\begin{prooftree}
\AxiomC{$x : A ∈ Γ$}
\RightLabel{\textsc{(Var)}}
\UnaryInfC{$Γ ⊢ x : A$}
\end{prooftree}
\begin{prooftree}
\AxiomC{$Γ ⊢ f:A→B$}
\AxiomC{$Γ ⊢ a:A$}
\RightLabel{\textsc{(App)}}
\BinaryInfC{$Γ ⊢ fa : B$}
\end{prooftree}
\begin{prooftree}
\AxiomC{$Γ,x : A ⊢ b:B$}
\RightLabel{\textsc{(Abs)}}
\UnaryInfC{$Γ ⊢ λx : A. b : A→B$}
\end{prooftree}
#+end_figure

*** λ-cube
The λ→-calculus restricts the types of arguments to functions; types are static
and descriptive. When evaluating a well-typed term, the types can be erased
altogether without any effect on the computation. In other words, terms can only
depend on other terms.

Generalizations of the λ→-calculus can be organized into a cube called the
Barendregt cube, or the λ-cube cite:barendregt92_typed (Figure ref:cube). In λ→
only terms depend on terms, but there are also three other combinations
represented by the three dimensions of the cube: types depending on types
$(□,□)$, or also called type operators; terms depending on types $(□,⋆)$, called
/polymorphism/; and terms depending on types $(⋆,□)$, representing /dependent
types/.

#+label: cube
#+CAPTION: Barendregt cube (also λ-cube)
#+ATTR_LATEX: :options [!htb]
#+begin_figure latex
\centering
\begin{tikzpicture}
\matrix (m) [matrix of math nodes,
row sep=2.5em, column sep=2.5em,
text height=1.5ex,
text depth=0.25ex]{
   & λω             &     & λΠω            \\
λ2 &                & λΠ2 &                \\
   & λ\underline{ω} &     & λΠ\underline{ω}\\
λ→ &                & λΠ  \\
};
\path[-{Latex[length=2.5mm, width=1.5mm]}]
(m-1-2) edge (m-1-4)
(m-2-1) edge (m-2-3) edge node[fill=white,pos=0.4]{$(□,□)$} (m-1-2)
(m-3-2) edge (m-1-2) edge (m-3-4)
(m-4-1) edge node[fill=white]{$(□,⋆)$} (m-2-1)
(m-4-1) edge (m-3-2)
(m-4-1) edge node[fill=white]{$(⋆,□)$} (m-4-3)
(m-3-4) edge (m-1-4)
(m-2-3) edge (m-1-4)
(m-4-3) edge (m-3-4) edge (m-2-3);
\end{tikzpicture}
#+end_figure

**** Sorts
To formally describe the cube, we will need to introduce the notion of sorts. In
brief,

#+LATEX: \[t : T : ⋆ : □.\]

The meaning of the symbol $:$ is same as before, "x has type y". The type of a
term $t$ is a type $T$, the type of a type $T$ is a kind $*$, and the type of
kinds is the sort □. The symbols ⋆ and □ are called /sorts/. As with types, sorts
can be connected using arrows, e.g. $(⋆→⋆)→⋆$. To contrast the syntaxes of the
following languages, the syntax of λ→ is here:

#+begin_export latex
\[\begin{array}{ccccccc}
types & ≔ & T & | & A → B  &   &     \\
terms & ≔ & v & | & λx:A.t & | & a b \\
values & ≔ &  &   & λx:A.t &   & \\
\end{array}\]
#+end_export

**** λ\underline{ω}-calculus
Higher-order types or type operators generalizes the concepts of functions to
the type level, adding λ-abstractions and applications to the language of types.

#+begin_export latex
\[\begin{array}{ccccccccc}
types & ≔ & T & | & A → B  & | & A B & | & ΛA.B(a) \\
terms & ≔ & v & | & λx:A.t & | & a b \\
values & ≔ &  &   & λx:A.t \\
\end{array}\]
#+end_export

**** λ2-calculus
The dependency of terms on types adds polymorphic types to the language of
types: $∀X:k.A(X)$, and type abstractions (Λ-abstractions) and applications to
the language of terms. This system is also called System F, and it is equivalent
to propositional logic cite:barendregt92_typed.

#+begin_export latex
\[\begin{array}{ccccccccc}
types & ≔ & T & | & A → B  & | &     &   & ∀A.B \\
terms & ≔ & v & | & λx:A.t & | & a b & | & ΛA.t \\
values & ≔ &  &   & λx:A.t & | &     &   & ΛA.t \\
\end{array}\]
#+end_export

**** λΠ-calculus
Allowing types to depend on terms means that type of a function can depend on
its term-level arguments, hence dependent types, represented by the type
$Πa:A.B(a)$. This dependency is the reason for the name of dependently-typed
languages. This system is well-studied as the Logical Framework (LF)
cite:barendregt92_typed.

#+begin_export latex
\[\begin{array}{ccccccccc}
types & ≔ & T & | & A → B  & | &     &   & Πa:A.B \\
terms & ≔ & v & | & λx:A.b & | & a b & | & Πa:A.b \\
values & ≔ &  &   & λx:A.b & | &     &   & Πx:A.b \\
\end{array}\]
#+end_export

**** Pure type system
These systems can all be described by one set of typing rules instantiated with
a triple $(S, A, R)$. Given the set of sorts $S=\{⋆,□\}$ we can define relations
$A$ and $R$ where, for example, $A=\{(⋆,□)\}$ is translated to the axiom $⊢⋆:□$
by the rule *Start*, and $R=\{(⋆,□)\}$[fn:9] means that a kind can depend on a
type using the rule *Product*.

#+begin_export latex
\[\begin{array}{ccll}
S & ≔ & \{⋆,□\} & \text{set of sorts} \\
A & ⊆ & S×S   & \text{set of axioms} \\
R & ⊆ & S×S×S & \text{set of rules}
\end{array}\]
#+end_export

The typing rules in Figure ref:coc-rules apply to all the above-mentioned
type systems. The set $R$ exactly corresponds to the dimensions of the λ-cube,
so instantiating this type system with $R=\{(⋆,⋆)\}$ would produce the
λ→-calculus, whereas including all the dependencies $R=\{(⋆,⋆), (□,⋆),(⋆,□),
(□,□)\}$ produces the λΠω-calculus. If we also consider that the function arrow
$A→B$ is exactly equivalent to the type $Πa:A.B(a)$ if the variable $a$ is not
used in the expression $B(a)$, the similarity to Figure ref:simple-types should
be easy to see.

#+label:coc-rules
#+caption:Typing rules of a pure type system
#+ATTR_LaTeX: :options [!htb]
#+begin_figure latex
\centering
\begin{tabular}{cl}
\AxiomC{}
\RightLabel{$(s₁,s₂)∈A$}
\UnaryInfC{$⊢ s₁:s₂$}
\DisplayProof & \textsc{(Start)} \\[7pt]
\AxiomC{$Γ ⊢ A:s$}
\RightLabel{$s∈S$}
\UnaryInfC{$Γ,x:A ⊢ x:A$}
\DisplayProof & \textsc{(Var)} \\[13pt]
\AxiomC{$Γ ⊢ x : A$}
\AxiomC{$Γ ⊢ B : s$}
\RightLabel{$s∈S$}
\BinaryInfC{$Γ,y:B ⊢ x:A$}
\DisplayProof & \textsc{(Weaken)} \\[13pt]
\AxiomC{$Γ ⊢ f:Π_{x:A}B(x)$}
\AxiomC{$Γ ⊢ a:A$}
\BinaryInfC{$Γ ⊢ fa : B[x≔a]$}
\DisplayProof & \textsc{(App)} \\[13pt]
\AxiomC{$Γ,x : A ⊢ b:B$}
\AxiomC{$Γ ⊢ Π_{x:A}B(x) : s$}
\RightLabel{$s∈S$}
\BinaryInfC{$Γ ⊢ (λx : A. b) : Π_{x:A}B(x)$}
\DisplayProof & \textsc{(Abs)} \\[13pt]
\AxiomC{$Γ ⊢ A:s₁$}
\AxiomC{$Γ,x:A ⊢ B:s₂$}
\RightLabel{$(s₁,s₂,s₃)∈R$}
\BinaryInfC{$Γ ⊢ Π_{x:A}B(x) : s₃$}
\DisplayProof & \textsc{(Product)} \\[13pt]
\AxiomC{$Γ ⊢ a:A$}
\AxiomC{$Γ ⊢ A':s$}
\AxiomC{$A ⟶_β A'$}
\RightLabel{$s∈S$}
\TrinaryInfC{$Γ ⊢ a:A'$}
\DisplayProof & \textsc{(Conv)} \\[5pt]
\end{tabular}
#+end_figure

**** Universes
This can be generalized even more. Instantiating this system with an infinite
set of sorts $S=\{Type₀,Type₁,...\}$ instead of the set $\{⋆,□\}$ and setting
$A$ to $\{(Type₀, Type₁),\linebreak[1] (Type₁,Type₂), ...\}$ leads to an infinite hierarchy of
/type universes/, and is in fact an interesting topic in the field of type
theory. Proof assistants commonly use such a hierarchy cite:bove08_atwork.

**** Type in Type
Going the other way around, simplifying $S$ to $S=\{⋆\}$ and setting
$A$ to $\{(⋆,⋆)\}$, leads to an inconsistent logic system called λ⋆, also called a
system with a /Type in Type/ rule. This leads to paradoxes similar to the Russel's
paradox in set theory. [[inline:Show Girard's paradox?]]

In many pedagogic implementations of dependently-typed λ-calculi I saw, though,
this was simply acknowledged: separating universes introduces complexity but the
distinction is not as important for many purposes.

For the goal of this thesis--testing the characteristics of a runtime
system--the distinction is unimportant. In the rest of the text we will use the
inconsistent λ⋆-calculus, but with all the constructs mentioned in the preceding
type systems. We will now formally define these constructs, together with
several extensions to this system that will be useful in the context of
just-in-time compilation using Truffle, e.g., (co)product types, booleans, natural
numbers.

Proof assistants and other dependently-typed programming languages use systems
based on λΠω-calculus, which is called the Calculus of Constructions. They add
more extensions: induction and subtyping are common ones. We will discuss only a
subset of them in the following section, as many of these are irrelevant to the
goals of this thesis.

** Types
While it is possible to derive any types using only three constructs: Π-types
(dependent product), Σ-types (dependent sum), and $W\text{-types}$ (inductive
types), that we haven't seen so far; we will define specific /"wired-in"/ types
instead, as they are more straightforward to both use and implement.

We will specify the syntax and semantics of each type at the same time. For
syntax, we will define the terms and values, for semantics we will use four
parts: type formation, a way to construct new types; term introduction
(constructors), ways to construct terms of these types; term elimination
(destructors), ways to use them to construct other terms; and computation rules
that describe what happens when an introduced term is eliminated. The algorithms
to normalize and type-check these terms will be mentioned in the following
chapter. In this section we will solely focus on the syntax and semantics.

*** Π-types
As mentioned above, the type $Πa:A.B$, also called the /dependent product type/
or the /dependent function type/, is a generalization of the function type $A→B$.
Where the function type simply asserts that its corresponding function will
receive a value of a certain type as its argument, the Π-type makes the value
available in the rest of the type. Figure ref:type-pi introduces its semantics;
they are similar to the typing rules of λ→-calculus function application, except
for the substitution in the type of $B$ in rule *Elim-Pi*.

#+label: type-pi
#+CAPTION: Π-type semantics
#+ATTR_LaTeX: :options [!htb]
#+begin_figure latex
\centering
\begin{tabular}{cc}
\multicolumn{2}{c}{%
\AxiomC{$Γ ⊢ A:⋆$}
\AxiomC{$Γ, x:A ⊢ B:⋆$}
\RightLabel{\textbf{(Type-Pi)}}
\BinaryInfC{$Γ ⊢ Πx:A.B$}
\DisplayProof
} \\[15pt]
\AxiomC{$Γ,a:A ⊢ b:B$}
\RightLabel{\textbf{(Intro-Pi)}}
\UnaryInfC{$Γ ⊢ λx.b : Πx:A.B$}
\DisplayProof
&
\AxiomC{$Γ ⊢ f : Πx:A.B$}
\AxiomC{$Γ ⊢ a:A$}
\RightLabel{\textbf{(Elim-Pi)}}
\BinaryInfC{$Γ ⊢ f a : B[x≔a]$}
\DisplayProof
\\[15pt]
\multicolumn{2}{c}{%
\AxiomC{$Γ, a:A ⊢ b:B$}
\AxiomC{$Γ ⊢ a:A$}
\RightLabel{\textbf{(Eval-Pi)}}
\BinaryInfC{$Γ ⊢ (λx:A.b)a ⟶_β b[x≔a]$}
\DisplayProof
}
\end{tabular}
#+end_figure

While a very common example of a Π-type is the length-indexed vector
$Π(n:ℕ). Vec(ℝ, n)$, it is also possible to define a function with a /"dynamic"/
number of arguments like in the following listing. It is a powerful language
feature also for its programming uses, as it makes it possible to e.g. implement
a well-typed function ~printf~ that, e.g., produces the function $Nat → Nat →
String$ when called as ~printf "%d%d"~.

#+begin_export latex
\[\begin{array}{rcl}
succOrZero & : & Π(b:Bool). if b then (Nat→Nat) else Nat \\
succOrZero & = & Π(b:Bool). if b then (λx. x+1) else 0 \\[3pt]
succOrZero true 0 & ⟶_{βδ} & 1 \\
succOrZero false & ⟶_{βδ} & 0
\end{array}\]
#+end_export

**** Implicit arguments
The type-checker can infer many type arguments. Agda adds the concept of
implicit function arguments cite:bove08_atwork to ease the programmer's work and
mark inferrable type arguments in a function's type signature. Such arguments
can be specified when calling a function using a special syntax, but they are
not required cite:kovacs20_implicit. We will do the same, and as such we will
split the syntax of a Π-type back into three separate constructs, which can be
seen in Figure ref:syntax-pi.

#+label: syntax-pi
#+CAPTION: Π-type syntax
#+ATTR_LaTeX: :options [!htb]
#+begin_figure latex
\[\begin{array}{cclcccll}
term & ≔ & a → b & | & (a:A)→b & | & \{a:A\}→b & \text{(abstraction)} \\
     & | & f a   & | &         & | & f \{a\}   & \text{(application)} \\
value & ≔ & Πa:A.b
\end{array}\]
#+end_figure

The plain /function type/ $A→B$ is simple to type but does not bind the value
provided as the argument $A$. The /explicit Π-type/ $(a:A)→B$ binds the value $a$
and makes it available to use inside $B$, and the /implicit Π-type/ $\{a:A\}→B$
marks the argument as one that type elaboration should be able to infer from the
surrounding context. The following is an example of the implicit argument
syntax, a polymorphic function $id$.

#+begin_export latex
\[\begin{array}{rclcl}
id         & : & \{A:⋆\}→A→A   & ≔ &          Π(x:A).x \\
id \{Nat\} & : & Nat→Nat & ⟶_{βδ} & λ(x:Nat).x \\
id 1       & : & Nat     & ⟶_{βδ} & 1
\end{array}\]
#+end_export

*** Σ-types
The Σ-type is also called the /dependent pair type/, or alternatively the
dependent tuple, dependent sum, or even the dependent product type.  Like the
Π-type was a generalization of the function type, the Σ-type is a generalization
of a product type, or simply a /pair/. Semantically, the Σ-type is similar to the
tagged union in C-like languages: the type $Σ(a:A).B(a)$ corresponds to a value
$(a,b)$, only the type $B(a)$ can depend on the first member of the pair. This
is illustrated in Figure ref:type-sigma, where the dependency can be seen in
rule *Intro-Sigma*, in the substitution $B[x≔a]$.

#+label: type-sigma
#+CAPTION: Σ-type semantics
#+begin_figure latex
\centering
\begin{tabular}{cc}
\multicolumn{2}{c}{%
\AxiomC{$Γ ⊢ A : ⋆$}
\AxiomC{$Γ, x : A ⊢ B : ⋆$}
\RightLabel{\textbf{(Type-Sigma)}}
\BinaryInfC{$Γ ⊢ Σ_{x : A}B : ⋆$}
\DisplayProof
} \\[15pt]
\multicolumn{2}{c}{%
\AxiomC{$Γ ⊢ a : A$}
\AxiomC{$Γ, x : A ⊢ B : ⋆$}
\AxiomC{$Γ ⊢ b : B[x ≔ a]$}
\RightLabel{\textbf{(Intro-Sigma)}}
\TrinaryInfC{$Γ ⊢ (a, b) : Σ_{x : A}B$}
\DisplayProof
} \\[15pt]
\AxiomC{$Γ ⊢ p : Σ_{x : A}B$}
\RightLabel{\textbf{(Elim-Sigma1)}}
\UnaryInfC{$Γ ⊢ π₁ p : A$}
\DisplayProof &
\AxiomC{$Γ ⊢ p : Σ_{x : A}B$}
\RightLabel{\textbf{(Elim-Sigma2)}}
\UnaryInfC{$Γ ⊢ π₂ p : B[x ≔ fst p]$}
\DisplayProof \\[15pt]
\multicolumn{2}{c}{%
\AxiomC{$Γ ⊢ a : A$}
\AxiomC{$Γ, x : A ⊢ B : ⋆$}
\AxiomC{$Γ ⊢ b : B[x ≔ a]$}
\RightLabel{\textbf{(Eval-Sigma1)}}
\TrinaryInfC{$Γ ⊢ π₁ (a, b) ⟶_ι a : A$}
\DisplayProof
} \\[15pt]
\multicolumn{2}{c}{%
\AxiomC{$Γ ⊢ a : A$}
\AxiomC{$Γ, x : A ⊢ B : ⋆$}
\AxiomC{$Γ ⊢ b : B[x ≔ a]$}
\RightLabel{\textbf{(Eval-Sigma2)}}
\TrinaryInfC{$Γ ⊢ π₂ (a, b) ⟶_ι b : B$}
\DisplayProof
}
\end{tabular}
#+end_figure

Above, we had a function that could accept different arguments based on the
value of the first argument. Below we have a type that simply uses Σ in place of
Π in the type: based on the value of the first member, the second member can be
either a function or a value, and still be a well-typed term.

#+begin_export latex
\[\begin{array}{rcl}
FuncOrVal & : & Σ(b:Bool). if b then (Nat→Nat) else Nat \\
(true, λx. x+1) & : & FuncOrVal \\
(false, 0) & : & FuncOrVal
\end{array}\]

#+end_export

**** Pair
Similar to the function type, given the expression $Σ(a:A).B(a)$, if $a$ does
not occur in the expression $B(a)$, then it is the non-dependent pair type. The
pair type is useful to express an isomorphism also used in general programming
practice: a conversion between a function of two arguments, and a function of
one argument that returns a function of one argument:

#+begin_export latex
\[\begin{array}{rclcll}
          &   & A × B → C   & ⇔ & A → B → C \\
    curry & ≔ & λ(f:A×B→C). &   & λ(x:A).λ(y:B). & f (x,y) \\
  uncurry & ≔ & λ(f:A→B→C). &   & λ(x:A×B). & f (π₁ x) (π₂ y)
\end{array}\]
#+end_export

**** Tuple
The n-tuple is a generalization of the pair, a non-dependent set of an arbitrary
number of values, otherwise expressible as a set of nested pairs: commonly
written as $(a₁, ..., aₙ)$.

**** Record
A record type is similar to a tuple, only its members have unique labels. In
Figure ref:type-record we see the semantics of a general record type, using the
notation $\{l_i=t_i\} : \{l_i:T_i\}$ and a projection $record.member$.

#+label: type-record
#+CAPTION: Record semantics
#+ATTR_LaTeX: :options [!htb]
#+begin_figure latex
\centering
\begin{tabular}{cc}
\multicolumn{2}{c}{%
\AxiomC{$∀i∈\{1..n\} Γ ⊢ Tᵢ:⋆$}
\RightLabel{\textbf{(Type-Rec)}}
\UnaryInfC{$Γ ⊢ \{lᵢ:Tᵢ^{i∈\{1..n\}}\}:⋆$}
\DisplayProof } \\[15pt]
\multicolumn{2}{c}{%
\AxiomC{$∀i∈\{1..n\} Γ ⊢ tᵢ : Tᵢ$}
\RightLabel{\textbf{(Intro-Rec)}}
\UnaryInfC{$Γ ⊢ \{lᵢ=tᵢ^{i∈\{1..n\}}\} : \{lᵢ:Tᵢ^{i∈\{1..n\}}\}$}
\DisplayProof } \\[15pt]
\multicolumn{2}{c}{%
\AxiomC{$Γ ⊢ t : \{lᵢ:Tᵢ^{i∈\{1..n\}}\}$}
\RightLabel{\textbf{(Elim-Rec)}}
\UnaryInfC{$Γ ⊢ t.lᵢ : Tᵢ$}
\DisplayProof } \\[15pt]
\multicolumn{2}{c}{%
\AxiomC{$∀i∈\{1..n\} Γ ⊢ tᵢ : Tᵢ$}
\AxiomC{$Γ ⊢ t : \{lᵢ:Tᵢ^{i∈\{1..n\}}\}$}
\RightLabel{\textbf{(Eval-Rec)}}
\BinaryInfC{$Γ ⊢ \{lᵢ=tᵢ^{i∈\{1..n\}}\}.lᵢ ⟶_ι tᵢ : B$}
\DisplayProof
}
\end{tabular}
#+end_figure

In Figure ref:syntax-sigma we have a syntax that unifies all of these concepts:
a Σ-type, a pair, an n-tuple, a named record. A non-dependent n-tuple type is
written as $A×B×C$ with values $(a,b,c)$. Projections of non-dependent tuples
use numbers, e.g., $p.1$, $p.2$, ... A dependent sum type is written in the same
way as a named record: $(a:A)×B$ binds the value $a:A$ in the rest of the type
$B$, and on the value-level enables the projection $obj.a$.

#+label: syntax-sigma
#+CAPTION: Σ-type syntax
#+ATTR_LaTeX: :options [!htb]
#+begin_figure latex
\[\begin{array}{cclcll}
term & ≔ & T₁×⋯×Tₙ     & | & (l₁:T₁)×⋯×(lₙ:Tₙ)×T_{n+1} & \text{(types)} \\
     & | & t.i         & | & t.lₙ            & \text{(destructors)} \\
     & | & (t₁, ⋯, tₙ) &   &                 & \text{(constructor)} \\
value & ≔ & (t₁, ⋯, tₙ)
\end{array}\]
#+end_figure

**** Coproduct
The sum type or the coproduct $A+B$ can have values from both types $A$ and $B$,
often written as $a:A⊢ inl A:A+B$, where $inl$ means "on the left-hand side of
the sum $A+B$". This can be generalized to the concept of /variant types/, with an
arbitrary number of named members; shown below, using Haskell syntax:

#+LATEX: \[data Maybe a = Nothing | Just a\]

For the purposes of our language, a binary sum type is useful, but inductive
variant types would require more involved constraint checking, so we will ignore
those, only using simple sum types in the form of $A+B$. This type can be
derived using a dependent pair where the first member is a boolean.

#+LATEX: \[Char+Int ≃ Σ(x:Bool). if x Char Int\]

*** Value types
**** Finite sets
Pure type systems mentioned in the previous chapter often use types like *0*, *1*,
and *2* with a finite number of inhabitants, where the type *0* (with zero
inhabitants of the type) is the empty or void type. Type *1* with a single
inhabitant is the unit type, and the type *2* is the boolean type. Also, the
infinite set of natural numbers can be defined using induction over *2*.
For our purposes it is enough to define a fixed number of types, though.

**** Unit
The unit type *1*, or commonly written as the 0-tuple "$()$", is sometimes used as a
universal return value. As it has no evaluation rules, though, we can simply add
a new type $Unit$ and a new value and term $unit$, with the rule $unit : Unit$.

**** Booleans
The above-mentioned type *2* has two inhabitants and can be semantically mapped to
the boolean type. In Figure ref:type-bool we introduce the values (constructors)
$true$ and $false$, and a simple eliminator $if$ that returns one of two values
based on the truth value of its argument.

#+label: type-bool
#+CAPTION: \texttt{Bool} semantics
#+ATTR_LaTeX: :options [htb]
#+begin_figure latex
\centering
\begin{tabular}{cc}
\multicolumn{2}{c}{%
\AxiomC{}
\RightLabel{\textbf{(Type-Nat)}}
\UnaryInfC{$⊢ Bool : ⋆$}
\DisplayProof
} \\[15pt]
\AxiomC{}
\RightLabel{\textbf{(Intro-True)}}
\UnaryInfC{$⊢ true : Bool$}
\DisplayProof &
\AxiomC{}
\RightLabel{\textbf{(Intro-False)}}
\UnaryInfC{$⊢ false : False$}
\DisplayProof
\\[15pt]
\multicolumn{2}{c}{%
\AxiomC{$Γ ⊢ a₁:A$}
\AxiomC{$Γ ⊢ a₂:A$}
\RightLabel{\textbf{(Elim-Bool)}}
\BinaryInfC{$Γ,x:Bool ⊢ if x a₁ a₂ : A$}
\DisplayProof
} \\[15pt]
\AxiomC{$Γ ⊢ a₁:A$}
\AxiomC{$Γ ⊢ a₂:A$}
\RightLabel{\textbf{(Eval-True)}}
\BinaryInfC{$Γ ⊢ if true a₁ a₂ ⟶_ι a₁ : A$}
\DisplayProof &
\AxiomC{$Γ ⊢ a₁:A$}
\AxiomC{$Γ ⊢ a₂:A$}
\RightLabel{\textbf{(Eval-False)}}
\BinaryInfC{$if false a₁ a₂ ⟶_ι a₂ : A$}
\DisplayProof
\end{tabular}
#+end_figure

**** Natural numbers
The natural numbers form an infinite set, unlike the above value types.  On
their own, adding natural numbers to a type system does not produce
non-termination, as the recursion involved in their manipulation can be limited
to primitive recursion as e.g., used in Gödel's System T cite:bove08_atwork.
The constructions introduced in Figure ref:type-nat are simply the constructors
$zero$ and $succ$, and the destructor $natElim$ unwraps at most one layer of
$succ$. [[inline:Dependent eliminator too? ncatlab]]

#+label: type-nat
#+CAPTION: \texttt{Nat} semantics
#+ATTR_LaTeX: :options [htb]
#+begin_figure latex
\centering
\begin{tabular}{cc}
\multicolumn{2}{c}{%
\AxiomC{}
\RightLabel{\textbf{(Type-Nat)}}
\UnaryInfC{$⊢ Nat : ⋆$}
\DisplayProof
} \\[15pt]
\AxiomC{}
\RightLabel{\textbf{(Intro-Zero)}}
\UnaryInfC{$⊢ zero : Nat$}
\DisplayProof &
\AxiomC{$Γ ⊢ n : Nat$}
\RightLabel{\textbf{(Intro-Succ)}}
\UnaryInfC{$Γ ⊢ succ n : Nat$}
\DisplayProof
\\[15pt]
\multicolumn{2}{c}{%
\AxiomC{$Γ ⊢ a₁:A$}
\AxiomC{$Γ,n:Nat ⊢ a₂:A$}
\RightLabel{\textbf{(Elim-Nat)}}
\BinaryInfC{$Γ,x:Nat ⊢ natElim x a₁ (λx.a₂)$}
\DisplayProof
} \\[15pt]
\multicolumn{2}{c}{%
\AxiomC{$Γ ⊢ a₁:A$}
\AxiomC{$Γ,n:Nat ⊢ a₂:A$}
\RightLabel{\textbf{(Eval-Zero)}}
\BinaryInfC{$Γ ⊢ natElim zero a₁ (λx.a₂) ⟶_ι a₁ : A$}
\DisplayProof
} \\[15pt]
\multicolumn{2}{c}{%
\AxiomC{$Γ ⊢ a₁:A$}
\AxiomC{$Γ,n:Nat ⊢ a₂:A$}
\AxiomC{$Γ ⊢ n:Nat$}
\RightLabel{\textbf{(Eval-Succ)}}
\TrinaryInfC{$natElim (succ n) a₁ (λx.a₂) ⟶_ι a₂[x≔n] : A$}
\DisplayProof
}
\end{tabular}
#+end_figure

*** μ-types
There are multiple ways of encoding recursion in λ-calculi with types, based on
whether a recursive expression is delimited using types, or whether it is also
reflected in the type of a recursive expression. Recursion must be defined
carefully if the type system needs to be consistent, as non-restricted general
recursion leads to non-termination and inconsistency. /Iso-recursive types/ use
explicit folding and unfolding operations, that convert between the recursive
type $μa.T$ and $T[a≔μa.T]$, whereas in /equi-recursive types/ these operations
are implicit and inserted by the type-checker.

As both complicate the type-checker, we will use a simpler value-level recursive
combinator $fix$. While this does compromise the consistency of the type system,
it is sufficient for the purposes of runtime system characterization.

#+label: type-fix
#+CAPTION: \texttt{fix} semantics
#+ATTR_LaTeX: :options [htb]
#+begin_figure latex
\centering
\begin{tabular}{cc}
\multicolumn{2}{c}{%
\AxiomC{$Γ ⊢ f:A→A$}
\RightLabel{\textbf{(Type-Fix)}}
\UnaryInfC{$Γ ⊢ fix f : A$}
\DisplayProof
} \\[15pt]
\multicolumn{2}{c}{%
\AxiomC{$Γ,x:A ⊢ t:A$}
\RightLabel{\textbf{(Eval-Fix)}}
\UnaryInfC{$Γ ⊢ fix (λx.t) ⟶_β t[x≔(λx.t)] : A$}
\DisplayProof
}
\end{tabular}
#+end_figure

The semantics of the function $fix$ are described in Figure ref:type-fix. This
definition is sufficient to define e.g., the recursive computation of a
Fibonacci number or a local recursive binding as below.

#+begin_src text
fib = fix (λf. λn. if (isLess n 2) n (add (f (n - 1)) (f (n-2))))

evenOdd
  : (isEven : Nat → Bool) × (isOdd : Nat → Bool) × Top
  = fix (λf. ( if isZero x then true else f.isOdd (pred x)
            , if isZero x then false else f.isEven (pred x)
            , Top
            ))
#+end_src

** Remaining constructs
These constructs together form a complete core language capable of forming and
evaluating expressions. Already, this would be a usable programming
language. However, the /surface language/ is still missing: the syntax for
defining constants and variables, and interacting with the compiler.

**** Local definitions
The λ-calculus is, to use programming language terminology, a purely functional
programming language: without specific extensions, any language construct is an
expression. We will use the syntax of Agda, and keep local variable definition
as an expression as well, using a ~let-in~ construct, with the semantics given in
Figure ref:let-in.

#+label: let-in
#+CAPTION: \texttt{let-in} semantics
#+ATTR_LATEX: :options [htb]
#+begin_figure latex
\captionsetup{aboveskip=-3pt}
\begin{prooftree}
\AxiomC{$Γ ⊢ a : A$}
\AxiomC{$Γ,x:A ⊢ b : B$}
\RightLabel{\textbf{(Type-Let)}}
\BinaryInfC{$Γ ⊢ \text{let} x=a \text{in} b:B$}
\end{prooftree}
\begin{prooftree}
\AxiomC{$Γ ⊢ v:A$}
\AxiomC{$Γ,x:A ⊢ e:B$}
\RightLabel{\textbf{(Eval-Let)}}
\BinaryInfC{$\text{let} x=v \text{in} e ⟶_ζ e[x≔v]$}
\end{prooftree}
#+end_figure

**** Global definitions
Global definitions are not strictly necessary, as with local definitions and the
fixed-point combinator we could emulate them. However, global definitions will
be useful later in the process of elaborations, when global top-level
definitions will separate blocks that we can type-check separately. We will add
three top-level expressions: a declaration that only
assigns a name to a type, and a definition with and without type. Definitions
without types will have them inferred.

#+begin_export latex
\[\begin{array}{ccl}
top & ≔ & id : term \\
    & | & id : term = term \\
    & | & id = term \\
\end{array}\]
#+end_export

**** Holes
A construct that serves solely as information to the compiler and will not be
used at runtime is a /hole/. It can take the place of a term in an expression and
marks the missing term as one to be inferred ("filled in") during
elaboration[fn:10]. In fact, the syntax for a global definition without a type
will use a hole in place of its type. The semantics of a hole are omitted on
purpose as they would also require specifying the type inference algorithm.

#+begin_export latex
\[\begin{array}{ccl}
term & ≔ & \_
\end{array}\]
#+end_export

**** Interpreter directives
Another type of top-level expressions is a pragma, a direct command to the
compiler. We will use these when evaluating the time it takes to normalize or
elaborate an expression, or when enabling or disabling the use of "wired-in"
types, e.g. to compare the performance impact of using a Church encoding of
numbers versus a natural type that uses hardware integers. We will once again
use the syntax of Agda:

#+begin_export latex
\[\begin{array}{ccl}
top & ≔ & \{-\# BUILTIN id \#-\} \\
    & | & \{-\# ELABORATE term \#-\} \\
    & | & \{-\# NORMALIZE term \#-\} \\
\end{array}\]
#+end_export

**** Polyglot
Lastly, one language feature that will only be described and implemented in
Chapter ref:jit-interpreter: a /"polyglot"/ construct that offers a way to execute
code in a different language, which is a feature of the Truffle framework. The
selected syntax is a three-part expression that contains the name of language to
be used, the foreign code, and the type of the result of evaluating this foreign
code:

#+begin_export latex
\[\begin{array}{ccl}
term & ≔ & [| id | foreign | term |]
\end{array}\]
#+end_export

The syntax and semantics presented here altogether comprise a working
programming language. A complete listing of the semantics is included in
Appendix ref:spec. The syntax, written using the notation of the ANTLR parser
generator is in Listing ref:grammar. The syntax does not mention constants like
$true$ or $Nat$, as they will be implemented as global definitions bound in the
initial type-checking context and do not need to be recognized during parsing.

With this, the language specification is complete, and we can move on to the
next part, implementing a type-checker and an interpreter for this language.

#+label: grammar
#+caption: The complete grammar, written using simplified ANTLR
#+begin_src antlr
  FILE : STMT (STMTEND STMT)* ;
  STMT : "{-#" PRAGMA "#-}"
       | ID ":" EXPR
       | ID (":" EXPR)? "=" EXPR
       ;
  EXPR : "let" ID ":" EXPR "=" EXPR "in" EXPR
       | "λ" LAM_BINDER "." EXPR
       | PI_BINDER+ "→" EXPR
       | ATOM ARG*
       ;
  LAM_BINDER : ID | "_" | "{" (ID | "_") "}" ;
  PI_BINDER : ATOM ARG* | "(" ID+ ":" EXPR ")" | "{" ID+ ":" EXPR "}" ;
  ARG : ATOM | "{" ID ("=" TERM)? "}" ;
  ATOM : "[" ID "|" FOREIGN "|" TERM "]"
       | EXPR "×" EXPR
       | "(" EXPR ("," EXPR)+ ")"
       | "(" EXPR ")"
       | ID "." ID
       | ID
       | NAT
       | "*"
       | "_"
       ;
   STMTEND : ("\n" | ";")+ ;
   ID : [a-zA-Z] [a-zA-Z0-9] ;
   SKIP : [ \t] | "--" [^\r\n]* | "{-" [^#] .* "-}" ;
  // pragma discussed in text
#+end_src

* Language implementation: Montuno
  :PROPERTIES:
  :CUSTOM_ID: interpreter
  :END:

** Introduction
Now with a complete language specification, we can move onto the next step:
writing an interpreter. The algorithms involved can be translated from
specification to code quite naturally. at least in the style of interpreter we
will create at first. The second interpreter in Truffle will require a quite
different programming paradigm and deciding on many low-level implementation
details, e.g., how to implement actual function calls.

In this chapter we will introduce the algorithms at the core of an interpreter
and build a tree-based implementation for the language, elaborating on key
implementation decisions. This interpreter will be referred to using the working
name Montuno[fn:11].

This interpreter can be called an AST (abstract syntax tree) interpreter, as the
principal parts all consist of tree traversals, due to the fact that all the
main data structures involved are trees: pre-terms, terms, and values are
recursive data structures. The main algorithms to be discussed are: evaluation,
normalization, and elaboration, all of them can be translated to tree traversals
in a straightforward way.

**** Language
The choice of a programming language is mostly decided by the eventual target
platform Truffle, as we will be able to share parts of the implementation
between the two interpreters. The language of GraalVM and Truffle is Java,
although other languages that run on the Java Virtual Machine can be
used[fn:1]. My personal preference lies with more functional languages like
Scala or Kotlin, as the code often is cleaner and more concise[fn:6], so in the
end, after comparing the languages, I have selected Kotlin due to its
multi-paradigm nature: Truffle requires the use of annotated classes, but this
first interpreter can be written in a more natural functional style.

**** Libraries
Truffle authors recommend against using many external libraries in the internals
of the interpreter, as the techniques the libraries use may not work well with
Truffle inline:Source. Therefore, we will need to design our own supporting data
structures based on the fundamental data structures provided directly by
Kotlin. Only two external libraries would be too complicated to reimplement, and
both of these were chosen because they are among the most widely used in
their field:
- a parser generator, ANTLR, to process input into an abstract syntax tree,
- a terminal interface library, JLine, to implement the interactive interface.

For the build and test system, the recommended choices of Gradle and JUnit were
used.

#+LABEL:program-flow
#+CAPTION: Overview of interpreter components
#+ATTR_LATEX: :options [!htb]
#+begin_figure latex
\centering
\begin{tikzpicture}[line/.style={-latex}, block/.style={draw,thick,text width=2cm,minimum height=1cm,align=center}]
\node[block](repl){REPL};
\node[block,below=.5cm of repl](cli){CLI};
\node[block,below=.5cm of cli](file){File};
\node[block,right=of cli](driver){Driver};
\node[block,right=of driver](elab){Elaboration};
\node[block,right=of elab](eval){Evaluation};

\node[draw,inner xsep=3mm,inner ysep=2mm,fit=(repl)(cli)(file),label={90:Frontend}](front){};
\node[draw,inner xsep=3mm,inner ysep=2mm,fit=(elab)(eval),label={90:Backend}](back){};

\draw[line] (repl.east) to (driver);
\draw[line] (cli.east) to (driver);
\draw[line] (file.east) to (driver);
\draw[line] (driver) to[bend right=10] (elab);
\draw[line] (elab) to[bend right=10] (driver);
\draw[line] (eval) to[bend right=10] (elab);
\draw[line] (elab) to[bend right=10] (eval);
\end{tikzpicture}
#+end_figure

*** Program flow
A typical interpreter takes in the user's input, processes it, and outputs a
result. In this way, we can divide the interpreter into a frontend, a driver,
and a backend, to reuse compiler terminology. A frontend handles user
interaction, be it from a file or from an interactive environment, a backend
implements the language semantics, and a driver connects them, illustrated in
Figure ref:program-flow.

**** Frontend
The frontend is intended to be a simple way to execute the interpreter, offering
two modes: a batch processing mode that reads from a file, and an interactive
terminal environment that receives user input and prints out the result of the
command. Proof assistants like Agda offer deeper integration with editors like
tactics-based programming or others, similar to the refactoring tools offered in
development environments for object-oriented languages, but that is unnecessary
for the purposes of this thesis.

**** Backend
The components of the backend, here represented as /elaboration/ and /evaluation/,
implement the data transformation algorithms that are further illustrated in
Figure ref:data-flow. In brief, the /elaboration/ process turns user input in the
form of partially-typed, well-formed /pre-terms/ into fully-annotated well-typed
/terms/. /Evaluation/ converts between a /term/ and a /value/: a term can be compared to
program data, it can only be evaluated, whereas a value is the result of such
evaluation and can be e.g., compared for equality.

#+LABEL:data-flow
#+CAPTION:Data flow overview
#+ATTR_LATEX: :options[!htb]
#+begin_figure latex
\centering
\begin{tikzpicture}[line/.style={-latex}, block/.style={draw,thick,text width=2cm,minimum height=1cm,align=center}]
\node[block](s){String};
\node[block,right=1.5cm of s](p){Pre-term};
\node[block,right=1.5cm of p](t){Term};
\node[block,right=1.5cm of t](v){Value};

\node[draw,inner xsep=2.5mm,inner ysep=11mm,fit=(p)(t)(v),label={90:Elaboration}]{};
\node[draw,inner xsep=1.25mm,inner ysep=5mm,fit=(t)(v),label={90:Evaluation}]{};

\draw[line] (s) to node[midway,above]{Parse} (p);
\draw[line] (p) to node[midway,above]{Infer} node[midway,below]{Check} (t);
\draw[line] (t) to[bend left=10] node[midway,above]{Eval} (v);
\draw[line] (v) to[bend left=10] node[midway,below]{Quote} (t);
\draw[line] (t) to[bend left=20] node[midway,below]{Pretty-print} (s);
\draw[line] (v) to[loop right] node[midway,left]{Unify} (v);
\end{tikzpicture}
#+end_figure

**** Data flow
In Figure ref:data-flow, /Infer/ and /Check/ correspond to type checking and type
inference, two parts of the /bidirectional typing/ algorithm that we will
use. /Unification/ (/Unify/) forms a major part of the process, as that is how we
check whether two values are equal. /Eval/ corresponds to the previously described
βδζι-reduction implemented using the /normalization-by-evaluation/ style, whereas
/Quote/ builds a term back up from an evaluated value. To complete the
description, /Parse/ and /Pretty-print/ convert between the user-readable, string
representation of terms and the data structures of their internal
representation.  For the sake of clarity, the processes are illustrated using
their simplified function signatures in Listing ref:main-sigs.

#+label: main-sigs
#+caption: Simplified signatures of the principal functions
#+attr_latex: :position [!htb]
#+begin_src kotlin
fun parse(input: String): PreTerm;
fun pprint(term: Term): String;
fun infer(pre: PreTerm): Pair<Term, Val>;
fun check(pre: PreTerm, wanted: Val): Term;
fun eval(term: Term): Val;
fun quote(value: Val): Term;
fun unify(left: Val, right: Val): Unit;
#+end_src

In this chapter, we will first define the data types, especially focusing on
closure representation. Then, we will specify and implement two algorithms:
/normalization-by-evaluation/, and /bidirectional type elaboration/, and lastly, we
finish the interpreter by creating its driver and frontend.

** Data structures
We have specified the syntax of the language in the previous chapter, which we
first need to translate to concrete data structures before trying to implement
the semantics. Sometimes, the semantics impose additional constraints on the
design of the data structures, but in this case, the translation is quite
straight-forward.

**** Properties
Terms and values form recursive data structures. We will also need a separate
data structure for pre-terms as the result of parsing user input. All of these
structures represent only well-formed terms and in addition, terms and value
represent the well-typed subset of well-formed terms. Well-formedness should be
ensured by the parsing process, whereas type-checking will take care of the
second property.

**** Pre-terms
As pre-terms are mostly just an encoding of the parse tree without much further
processing, the complete data type is only included in Appendix
ref:montuno-data. The ~PreTerm~ class hierarchy mostly reflects the ~Term~ classes
with a few key differences, like the addition of compiler directives or variable
representation, so in the rest of this section, we will discuss terms and values
only.

**** Location
A key feature that we will also disregard in this chapter is term location that
maps the position of a term in the original source expression, mostly for the
purpose of error reporting. As location is tracked in a field that occurs in all
pre-terms, terms, and values, it will only be included in the final listing of
classes in Appendix ref:montuno-data.

#+label:syntax-recap
#+caption:Terms and values in Montuno (revisited)
#+attr_latex: :options [htb]
#+begin_figure latex
\[\begin{array}{rclclcl}
term & ≔ & v     & | & constant & & \\
     & | & a b   & | & a \{b\}  &   & \\
     & | & a→b   & | & (a:A)→b  & | & \{a:A\}→b \\
     & | & a × b & | & (l:A)×b  & | & a.l \\
     & | & \text{let} x=v \text{in} e &|& [| id | foreign | type |] && \\
     & | & \_ &&&& \\
value& ≔ & constant &&&& \\
     & | & λx:A.b & | & Πx:A.b && \\
     & | & (a₁,⋯,aₙ) &&&& \\
     & | & \_ &&&& \\
\end{array}\]
#+end_figure

The terms and values that were specified in Chapter ref:lambda are revisited in
Figure ref:syntax-recap, there are a two main classes of terms: those that
represent computation (functions and function application), and those that
represent data (pairs, records, constants).

**** Data classes
Most /data/ terms can be represented in a straight-forward way, as they map
directly to features of the host language, Kotlin in our case. Kotlin has a
standard way of representing primarily data-oriented structures using
\texttt{data class}es. These are classes whose primary purpose is to hold data,
so-called Data Transfer Objects (DTOs), and are the recommended approach in
Kotlin[fn:5]. In Listing ref:dto we have the base classes for terms and values,
and a few examples of structures that map directly from the syntax to a data
object.

#+label: dto
#+caption: Pair and \texttt{let-in} representations
#+begin_src kotlin
sealed class Term
sealed class Value

data class TLet(val id: String, val bind: Term, val body: Term) : Term()
data class TSigma(val id: String, val type: Term, val body: Term) : Term()
data class TPair(val left: Term, val right: Term) : Term()

data class VPair(val left: Value, val right: Value) : Value()
#+end_src

Terms that encode computation, whether delayed (λ-abstraction) or not
(application) will be more involved. Variables /can/ be represented in a
straight-forward way, but a string-based representations is not the most optimal
way. We will look at these three constructs in turn.

*** Functions
**** Closure
Languages, in which functions are first-class values, all use the concept of a
closure. A closure is, in brief, a function in combination with the environment
in it was created. The body of the function can refer to variables other than
its immediate arguments, which means the surrounding environment needs to be
stored as well. The simplest example is the $const$ function $λx.λy.x, which,
when partially applied to a single argument, e.g., $\text{let }five = const 5$,
needs to store the value $5$ until it is eventually applied to the remaining
second argument: $five 15 ⟶ 5$.

**** HOAS
As Kotlin supports closures on its own, it would be possible to encode λ-terms
directly as functions in the host language. This is possible, and it is one of
the ways of encoding functions in interpreters. This encoding is called the
higher-order abstract syntax (HOAS), which means that functions[fn:13] in the
language are equal to functions in the host language. Representing functions
using HOAS produces very readable code, and in some cases, e.g., on GHC produces
code an order of magnitude faster than using other representations
cite:kovacs_norm. An example of what it looks like is in Listing ref:hoas.

#+label:hoas
#+caption: Higher-order abstract syntax encoding of a closure
#+begin_src kotlin
data class Closure<T>(val fun: (T) -> T)

val constFive = Closure<Int> { (n) -> 5 }
#+end_src

**** Explicit closures
However, we will need to perform some operations on the AST that need explicit
access to environments and the arguments of a function. The alternative to
reusing functions of the host language is a /defunctionalized/ representation,
also called /explicit closure/ representation. We will need to use this
representation later, when creating the Truffle version: function calls will
need to be objects, nodes in the program graph, as we will see in Chapter
ref:jit-interpreter. In this encoding, demonstrated in Listing ref:nonhoas, we store the
term of the function body together with the state of the environment when the
closure was created.

#+label:nonhoas
#+caption:Defunctionalized function representation
#+begin_src kotlin
data class Closure<T>(val fun: Term, val environment: Map<Name,Term>)

val constFive = Closure<Int>(TLocal("x"), mapOf("x" to 5))
#+end_src

*** Variables
Representing variables can be as straight-forward as in Listing ref:nonhoas: a
variable can be a simple string containing the name of the variable. This is
also what our parser produces in the pre-term representation. Also, when
describing reduction rules and substitution, we have also referred to variables
by their names. That is not the best way of representing variables.

**** Named
Often, when specifying a λ-calculus, the process of substitution $t[x≔e]$ is
kept vague, as a concern of the meta-theory in which the λ-calculus is encoded.
When using variable names (strings), the terms themselves and the code that
manipulates them are easily understandable. Function application, however,
requires variable renaming (α-conversion), which involves traversing the entire
argument term and replacing each variable occurrence with a fresh name that does
not yet occur in the function body. However, this is a very slow process, and it
is not used in any real implementation of dependent types or λ-calculus.

**** Nameless
An alternative to string-based variable representation is a /nameless/
representation, which uses numbers in place of variable names
cite:kamareddine01_de_bruijn. These numbers are indices that point to the
current variable environment, offsets from the top or the top of the environment
stack. The numbers are assigned, informally, by /counting the lambdas/, as each
λ-abstraction corresponds to one entry in the environment. The environment can
be represented as a stack to which a variable is pushed with every function
application, and popped when leaving a function. The numbers then point to these
entries. These two approaches can be seen side-by-side in Figure ref:var-named.

#+label: var-named
#+CAPTION: Named and nameless variable representations
#+ATTR_LATEX: :options [htb]
#+begin_figure latex
\captionsetup{aboveskip=-1pt}
\begin{center}
\begin{tabular}{ccc}
& $fix$ & $succ$ \\
\textbf{Named} & $(λf.(λx.f (x x)) (λx.f (x x))) g$ & $λx.x (λy.x y)$ \\
\textbf{Indices}   & $(λ(λ1 (0 0) (λ1 (0 0)) g$ & $λ0 (λ1 0)$ \\
\textbf{Levels}    & $(λ(λ0 (1 1) (λ0 (1 1)) g$ & $λ0 (λ0 1)$ \\
\end{tabular}
\end{center}
#+end_figure

**** de Bruijn indices
The first way of addressing, de Bruijn indexing, is rather well-known. It is a
way of counting from the top of the stack, meaning that the argument of the
innermost lambda has the lowest number. It is a "relative" way of counting,
relative to the top of the stack, which is beneficial during e.g. δ-reduction in
which a reference to a function is replaced by its definition: using indices,
the variable references in the function body do not need to be adjusted after
such substitution.

**** de Bruijn levels
The second way is also called the "reversed de Bruijn indexing"
cite:lescanne95_levels, as it counts from the start of the stack. This means
that the argument of the innermost lambda has the highest number. In the entire
term, one variable is only ever addressed by one number, meaning that this is an
"absolute" way of addressing, as opposed to the "relative" indices.

**** Locally nameless
There is a third alternative that combines both named and nameless
representations, and it has been used in e.g., the Lean proof assistant
cite:ebner17_metaprogramming. De Bruijn indices are used for bound variables and
string-based names for free variables. This also avoids any need for bound
variable substitution, but free variables still need to be resolved later during
the evaluation of a term.

**** Our choice
We will use a representation that has been used in recent type theory
implementations cite:eisenberg20_stitch cite:gratzer19_modal_types: de Bruijn
indices in terms, and de Bruijn levels in values. Such a representation avoids
any need for substitution as terms are that substituted /into/ an existing value
do not need to have the "relative" indices adjusted based on the size of the
current environment, whereas the "absolute" addressing of levels in values means
that values can be directly compared. This combination of representations means
that we can doing avoid any substitution at all, as any adjustment of variables
is performed during the evaluation from term to value and back.

**** Implementation
Kotlin makes it possible to construct type-safe wrappers over basic data types
that are erased at runtime but that support custom operations. Representing
indices and levels as \texttt{inline class}es means that we can perform add and
subtract them using the natural syntax e.g. ~ix + 1~, which we will use when
manipulating the environment in the next section. The final representation of
variables in our interpreter is in Listing ref:indices.

#+label: indices
#+caption: Variable representation
#+begin_src kotlin
inline class Ix(val it: Int) {
    operator fun plus(i: Int) = Ix(it + i)
    operator fun minus(i: Int) = Ix(it - i)
    fun toLvl(depth: Lvl) = Lvl(depth.it - it - 1)
}

inline class Lvl(val it: Int) {
    operator fun plus(i: Int) = Lvl(it + i)
    operator fun minus(i: Int) = Lvl(it - i)
    fun toIx(depth: Lvl) = Ix(depth.it - it - 1)
}

data class VLocal(val it: Lvl) : Val()
data class TLocal(val it: Ix) : Val()
#+end_src

*** Class structure
Variables and λ-abstractions were the two non-trivial parts of the mapping
between our syntax and Kotlin values. With these two pieces, we can fill out the
remaining parts of the class hierarchy. The full class listing is in Appendix
ref:montuno-data, here only a direct comparison of the data structures is shown
on the $const$ function in Figure ref:syntax-comp, and the most important
differences between them are in Figure ref:syntax-table.

#+label:syntax-comp
#+caption: Direct comparison of \texttt{PreTerm}, \texttt{Term}, and \texttt{Value} objects
#+attr_latex: :options [htb]
#+begin_figure latex
\begin{subfigure}[t]{.25\textwidth}\centering
\begin{minted}{kotlin}
PLam("x", Expl,
  PLam("y", Expl,
    PVar("x")))
\end{minted}
\end{subfigure}
\begin{subfigure}[t]{.25\textwidth}\centering
\begin{minted}{kotlin}
TLam("x", Expl,
  TLam("y", Expl,
    TLocal(1)))
\end{minted}
\end{subfigure}
\begin{subfigure}[t]{.5\textwidth}\centering
\begin{minted}{kotlin}
VLam("x", Expl,
  VCl([valX], VLam("y", Expl,
    VCl([valX, valY], VLocal(0)))))
\end{minted}
\end{subfigure}
#+end_figure

#+label:syntax-table
#+CAPTION: Important distinctions between \texttt{PreTerm}, \texttt{Term}, and \texttt{Value} objects
#+begin_figure latex
\begin{tabular}{rlll}
 & Variables & Functions & Properties\\\hline
\texttt{PreTerm} & String names & \texttt{PreTerm} AST & well-formed\\
\texttt{Term} & de Bruijn index & \texttt{Term} AST & well-typed\\
\texttt{Value} & de Bruijn level & Closure (\texttt{Term} AST + \texttt{Values} in context) & normal form\\
\end{tabular}
#+end_figure

** Normalization
*** Approach
**** Normalization-by-evaluation
Normalization is a series of βδζι-reductions, as defined in Chapter
ref:lambda. While there are systems that implement normalization as an exact
series of reduction rules, it is an inefficient approach that is not common in
the internals of state-of-the-art proof assistants. An alternative way of
bringing terms to normal form is the so-called /normalization-by-evaluation/ (NbE)
cite:pagano12_nbe_dependent. The main principle of this technique is
interpretation from the syntactic domain of terms into a computational, semantic
domain of values and back. In brief, we look at terms as an executable program
that can be /evaluated/, the result of such evaluation is then a normal form of
the original term. NbE is total and provably confluent cite:altenkirch16_nbe for
any abstract machine or computational domain.

**** Neutral values
If we consider only closed terms that reduce to a single constant, we could
simply define an evaluation algorithm over the terms defined in the previous
chapter. However, normalization-by-evaluation is an algorithm to bring any term
into a full normal form, which means evaluating terms inside function bodies and
constructors. NbE introduces the concept of "stuck" values that cannot be
reduced further. In particular, free variables in a term cannot be reduced, and
any terms applied to a stuck variable cannot be further reduced and are "stuck" as
well. These stuck values are called /neutral values/, as they are inert with
regards to the evaluation algorithm.

**** Semantic domain
Proof assistants use abstract machines like Zinc or STG; any way to evaluate a
term into a final value is viable. This is also the reason to use Truffle, as we
can translate a term into an executable program graph, which Truffle will later
optimize as necessary. In this first interpreter, however, the computational
domain will be a simple tree-traversal algorithm.

The set of neutral values in Montuno is rather small (Figure ref:neutrals): an
unknown variable, function application with a neutral /head/ and arbitrary terms
in the /spine/, and a projection eliminator.

#+label: neutrals
#+caption: Neutral values
#+attr_latex: :options [htb]
#+begin_figure latex
\[\begin{array}{rclclcl}
neutral & ≔ & var & | & neutral a₁ ...aₙ & | & neutral.lₙ\\
\end{array}\]
#+end_figure

**** Specification
The NbE algorithm is fully formally specifiable using four operations: the
above-mentioned evaluation and quoting, reflection of a neutral value (/NeVal/)
into a value, and reification of a value into a normal value (/NfVal/) that
includes its type, schematically shown in Figure ref:nbe. In this thesis,
though, will only describe the relevant parts of the specification in words, and
say that NbE (as we will implement it) is a pair of functions $nf = quote(eval(term))$,

#+label: nbe
#+caption: Syntactic and semantic domains in NbE \cite{abel17_sized}
#+attr_latex: :options [htb]
#+begin_figure latex
\centering
\begin{tikzpicture}[line/.style={-latex}, block/.style={align=center}]
\node[block](t){Term};
\node[block,right=1.5cm of t](nft){NfTerm};
\node[block,right=1.5cm of nft](net){NeTerm};
\node[block,below=1.5cm of nft](nfv){NfValue};
\node[block,below=1.5cm of net](nev){NeValue};
\node[block,below=1.5cm of nfv](v){Value};
\node[block,below=1.5cm of t](vv){\hphantom{Term}};

\node[draw,inner xsep=3mm,inner ysep=2mm,fit=(t)(nft)(net),label={180:Syntactic domain}]{};
\node[draw,inner xsep=3mm,inner ysep=2mm,fit=(vv)(nfv)(nev)(v),label={180:Semantic domain}]{};

\draw[line] (t) to[bend right=10] node[midway,fill=white]{Eval} (v);
\draw[line] (v) to node[midway,fill=white]{Reify} (nfv);
\draw[line] (nev) to[bend left=10] node[midway,fill=white]{Reflect} (v);
\draw[line] (nfv) to node[midway,fill=white]{Quote} (nft);
\draw[line] (nev) to node[midway,fill=white]{Quote} (net);
\draw[line] (net) to node[midway,fill=white]{$⊆$} (nft);
\draw[line] (nft) to node[midway,fill=white]{$⊆$} (t);
\end{tikzpicture}
#+end_figure

*** Normalization strategies
Normalization-by-evaluation is, however, at its core inefficient for our
purposes cite:kleeblatt11_strongly_normalizing_stg. The primary reason to
normalize terms in the interpreter is for type-checking and inference and that,
in particular, needs normalized terms to check whether two terms are
equivalent. NbE is an algorithm to get a full normal form of a term, whereas to
compare values for equality, we only need the weak head-normal form. To
illustrate: to compare whether a λ-term and a pair are equal, we do not need to
compare two fully-evaluated values, but only to find out whether the term is a
pair of a λ-term, which is given by the outermost constructor, the /head/.

In Chapter ref:lambda we saw an overview of normal forms of λ-calculus. To
briefly recapitulate, a normal form is a fully evaluated term with all sub-terms
also fully evaluated. A weak head-normal form is a form where only the outermost
construction is fully evaluated, be it a λ-abstraction or application of a
variable to a spine of arguments.

**** Reduction strategy
Normal forms are associated with a reduction strategy, a set of small-step
reduction rules that specify the order in which sub-expressions are
reduced. Each strategy brings an expression to their corresponding normal form.
Common ones are /applicative order/ in which we first reduce sub-expressions
left-to-right, and then apply functions to them; and /normal order/ in which we
first apply the leftmost function, and only then reduce its arguments. In Figure
ref:reduction-order there are two reduction strategies that we will emulate.

#+label: reduction-order
#+CAPTION: Reduction strategies for λ-calculus \cite{sestoft02_reduction}
#+attr_latex: :options [htb]
#+begin_figure latex
\captionsetup[subfigure]{aboveskip=-1pt}
\begin{subfigure}[t]{.5\textwidth}\centering
\[x \xrightarrow{name} x\]
\begin{prooftree}
\AxiomC{\vphantom{$e \xrightarrow{norm} e'$}}
\UnaryInfC{$(λx.e) \xrightarrow{name} (λx.e)$}
\end{prooftree}
\begin{prooftree}
\AxiomC{$e₁ \xrightarrow{name} (λx.e)$}
\AxiomC{$e[x≔e₂] \xrightarrow{name} e'$}
\BinaryInfC{$(e₁ e₂) \xrightarrow{name} e'$}
\end{prooftree}
\begin{prooftree}
\AxiomC{$e₁ \xrightarrow{name} e'₁ ≢ λx.e$}
\UnaryInfC{$(e₁ e₂) \xrightarrow{name} (e'₁ e₂)$}
\end{prooftree}
\caption{Call-by-name to weak head normal form}
\end{subfigure}
\begin{subfigure}[t]{.5\textwidth}\centering
\[x \xrightarrow{norm} x\]
\begin{prooftree}
\AxiomC{$e \xrightarrow{norm} e'$}
\UnaryInfC{$(λx.e) \xrightarrow{norm} (λx.e')$}
\end{prooftree}
\begin{prooftree}
\AxiomC{$e₁ \xrightarrow{name} (λx.e)$}
\AxiomC{$e[x≔e₂] \xrightarrow{norm} e'$}
\BinaryInfC{$(e₁ e₂) \xrightarrow{norm} e'$}
\end{prooftree}
\begin{prooftree}
\AxiomC{$e₁ \xrightarrow{name} e'₁ ≢ λx.e$}
\AxiomC{$e'₁ \xrightarrow{norm} e''₁$}
\AxiomC{$e₂ \xrightarrow{norm} e'₂$}
\TrinaryInfC{$(e₁ e₂) \xrightarrow{norm} (e''₁ e₂)$}
\end{prooftree}
\caption{Normal order to normal form}
\end{subfigure}
#+end_figure

In general programming language theory, a concept closely related to reduction
strategies is an evaluation strategy. These also specify when an expression is
evaluated into a value, but in our case, they apply to our host language Kotlin.

**** Call-by-value
Call-by-value, otherwise called eager evaluation, corresponds to applicative
order reduction strategy cite:ariola97_cbn. Specifically, when executing a
statement, its sub-expressions are evaluated inside-out and immediately reduced
to a value.  This leads to predictable program performance (the program will
execute in the order that the programmer wrote it, evaluating all expressions in
order), but this may lead to unnecessary computations performed: given an
expression ~const 5 (ackermann 4 2)~, the value of ~ackermann 4 2~ will be computed
but immediately discarded, in effect wasting processor time.

**** Call-by-need
Call-by-need, also lazy evaluation, is the opposite paradigm. An expression will
be evaluated only when its result is first accessed, not when it is created or
defined. Using call-by-need, the previous example will terminate immediately as
the calculation ~ackermann 4 2~ will be deferred and then discarded. However, it
also has some drawbacks, as the performance characteristics of programs may be
less predictable or harder to debug.

Call-by-value is the prevailing paradigm, used in all commonly used languages
with the exception of Haskell. It is sometimes necessary to defer the evaluation
of an expression, however, and in such cases lazy evaluation is emulated using
closures or zero-argument functions: e.g., in Kotlin a variable can be
initialized using the syntax ~val x by lazy { ackermann(4, 2) }~, and the value
will only be evaluated if it is ever needed.

**** Call-by-push-value
There is also an alternative paradigm, called call-by-push-value, which subsumes
both call-by-need and call-by-value as they can be directly translated to
CBPV--in the context of λ-calculus specifically. It defines additional operators
/delay/ and /force/ to accomplish this, one to create a /thunk/ that contains a
deferred computation, one to evaluate the thunk. Also notable is that it
distinguishes between values and computations: values can be passed around, but
computations can only be executed, or deferred.

**** Emulation
We can emulate normalization strategies by implementing the full
normalization-by-evaluation algorithm, and varying the evaluation
strategy. Kotlin is by default a call-by-value language, though, and evaluation
strategy is an intrinsic property of a language so, in our case, this means that
we need to insert ~lazy~ annotations in the correct places, so that no values are
evaluated other than those that are actually used. In the case of the later
Truffle implementation, we will need to implement explicit /delay/ and /force/
operations of call-by-push-value, which is why we introduced all three paradigms
in one place.

*** Implementation
The basic outline of the implementation is based on Christiansen's
cite:christiansen19_nbe_haskell. In essence, it implements the obvious
evaluation algorithm: evaluating a function captures the current environment in
a closure, evaluating a variable looks up its value in the environment, and
function application inserts the argument into the environment and evaluates the
body of the function.

**** Environments
The brief algorithm description used a concept we have not yet translated into
Kotlin: the environment, or evaluation context. When presenting the λ→-calculus,
we have seen the typing context Γ, to which we add a value context.

#+latex: \[\begin{array}{rclll}Γ & ≔ & ∙ & | & Γ,x:t\end{array}\]

The environment, following the above definition, is a stack: defining a variable
pushes a pair of a name and a type to the top, which is then popped off when the
variable goes out of scope. An entry is pushed and popped whenever we enter and
leave a function context, and the entire environment needs to be captured in its
current state whenever we create a closure. When implementing closures in
Truffle, we will also need to take care about which variables are actually used
in a function. That way, we can capture only those that need to be captured and
not the entire environment.

**** Linked list
The natural translation of the environment definition is a linked list. It would
also be the most efficient implementation in a functional language like Haskell,
as appending to an immutable list is very cheap there. In Kotlin, however, we
need to take care about not allocating too many objects and will need to
consider mutable implementations as well.

**** Mutable/immutable
In Kotlin and other JVM-based languages, an ~ArrayDeque~ is a fast data structure,
a mutable implementation of the stack data structure. In general, array-backed
data structures are faster than recursive ones on the JVM, which we will use in
the Truffle implementation. In this first interpreter, however, we can use the
easier-to-use immutable linked list implementation. It is shown in Listing
ref:conslist, a linked list specialized for values; an equivalent structure is
also implemented for types.

#+label:conslist
#+caption:Environment data structure as an immutable linked list
#+attr_latex: :position [htb]
#+begin_src kotlin
data class VEnv(val value: Val, val next: VEnv?)

fun VEnv?.len(): Int = if (this == null) 0 else 1 + next.len()
operator fun VEnv?.plus(v: Val): VEnv = VEnv(v, this)
operator fun VEnv?.get(n: Ix): Val
   = if (n.it == 0) this!!.value else this!!.next[n - 1]
#+end_src

**** Environment operations
We need three operations from an environment data structure: insert (bind) a
value, look up a bound value by its level or index, and unbind a variable that
leaves the scope. In Listing ref:conslist, we see two of them: the operator
~plus~, used as ~env + value~, binds a value, and operator ~get~, used as ~env[ix]~,
looks a value up. Unbinding a value is implicit, because this is an immutable
linked list: the reference to the list used in the outer scope is not changed by
any operations in the inner scope. These operations are demonstrated in Listing
ref:eval, on the ~eval~ operations of a variable and a ~let-in~ binding.

There we also see the basic structure of the evaluation algorithm. Careful
placement of ~lazy~ has been omitted, as it splits the algorithm into two: parts
that need to be evaluated lazily and those that do not, but the basic structure
should be apparent. The snippet uses the Kotlin ~when-is~ construct, which checks
the class of the argument, in this case we check if ~this~ is a ~TLocal~, ~TLet~, etc.

#+label:eval
#+caption:Demonstration of the \texttt{eval} algorithm
#+attr_latex: :position [htb]
#+begin_src kotlin
fun eval(ctx: Context, term: Term, env: VEnv): Val = when (term) {
  is TLocal ->
    env[term.ix] ?: VNeutral(HLocal(Lvl(ctx.lvl - term.ix - 1), spineNil))
  is TLet -> eval(ctx, term.body, env + eval(ctx, term.defn, env))
  is TLam -> VLam(term.name, VCl(env, term.body))
  is TApp -> when (fn := eval(ctx, term.lhs, env)) {
    is VLam -> eval(ctx, fn.cl.term, fn.cl.env + eval(ctx, term.rhs, env))
    is VNeutral -> VNeutral(fn.head, fn.spine + term.right)
  }
  // ...
}
#+end_src

**** Eval
In Listing ref:eval, a variable is looked up in the environment, and considered
a neutral value if the index is bigger than the size of the current
environment. In ~TLet~ we see how an environment is extended with a local value. A
λ-abstraction is converted into a closure. Function application, if the
left-hand side is a ~VLam~, evaluates the body of this closure, and if the
left-hand side is a neutral expression, then the result is also neutral value
and its spine is extended with another argument. Other language constructs are
handled in a similar way,

**** Quote
In Listing ref:quote, we see the second part of the algorithm. In the domain of
values, we do not have plain variable terms, or ~let-in~ bindings, but
unevaluated functions and "stuck" neutral terms. A λ-abstraction, in order to be
in normal form, needs to have its body also in normal form, therefore we insert
a neutral variable into the environment in place of the argument, and eval/quote
the body. A neutral term, on the other hand, has at its head a neutral
variable. This variable is converted into a term-level variable, and the spine
reconstructed as a tree of nested ~TApp~ applications.

#+label:quote
#+caption:Demonstration of the \texttt{quote} algorithm
#+attr_latex: :position [htb]
#+begin_src kotlin
fun quote(ctx: Context, v: Val): Term = when (v) {
  is VNeutral -> {
    x = TLocal(Ix(ctx.depth - v.head - 1))
    for (vSpine in v.spine.reversed()) {
        x = TApp(x, quote(ctx, vSpine))
    }
    x
  }
  is VLam -> TLam(v.name,
      quote(ctx, eval(ctx, v.cl.body, v.cl.env + VNeutral(HLocal(ctx.lvl)))))
  // ...
}
#+end_src

These two operations work together, to fully quote a value, we need to also
lazily ~eval~ its sub-terms. The main innovation of the
normalization-by-evaluation approach is the introduction of neutral terms, which
have the role of a placeholder value in place of a value that has not yet been
supplied. As a result, the expression $quote(eval(term, emptyEnv))$ produces a
lazily evaluated normal form of a term in a weak head-normal form, with its
sub-terms being evaluated whenever accessed. Printing out such a term would
print out the fully normalized normal form.

**** Primitive operations
Built-in language constructs like $Nat$ or $false$ that have not been shown in
the snippet are mostly inserted into the initial context as values that can be
looked up by their name. In general, though, constructs with separate syntax,
e.g. Σ-types, consist of three parts:

- their type is bound in the initial context;
- the term constructor is added to the set of terms and values, and added in ~eval()~;
- the eliminator is added as a term and as a spine constructor, i.e., an
  operation to be applied whenever the neutral value is provided.

The full listing is provided in the supplementary source code, as it is too long
to be included in text.

** Elaboration
   :PROPERTIES:
   :CUSTOM_ID: elaboration
   :END:
*** Approach
The second part of the internals of the compiler is type elaboration.
Elaboration is the transformation of a partially-specified, well-formed program
submitted by a user into a fully-specified, well-typed internal representation
cite:ferreira14_bidi. In particular, we will use elaboration to infer types of
untyped Curry-style λ-terms, and to infer implicit function arguments that were
not provided by the user, demonstrated in Figure ref:elab-demo.

#+label:elab-demo
#+caption:Demonstration of type elaboration
#+attr_latex: :options [htb]
#+begin_figure latex
\captionsetup{aboveskip=-3pt}
\[\begin{array}{rl}
\text{function signature:} & id:\{A\}→A→A \\
\text{provided expression:} & id id 5 \\
\text{elaborated expression:} & (id \{Nat→Nat\} id) \{Nat\} 5 \\
\end{array}\]
#+end_figure

**** Bidirectional typing
Programmers familiar with statically-typed languages like Java are familiar with
type checking, in which all types are provided by the user, and therefore are
inputs to the type judgment $Γ ⊢e:t$. Omitting parts of the type specification
means that the type system not only needs to check the types for correctness,
but also infer (synthesize) types: the type $t$ in $Γ⊢e:t$ is produced as an
output. In some systems, it is possible to omit all type annotations and rely
only on the type constraints of built-in functions and literals. Bidirectional
systems that combine both input and output modes of type judgment are now a
standard approach cite:nawaz19_survey_provers, often used in combination with
constraint solving.

**** Judgments
The type system is composed of two additional type judgments we haven't seen
yet, that describe the two directions of computation in the type system:
- $Γ ⊢ e ⇒ t$ is "given the context Γ and term $e$, infer (synthesize) its type
  $t$", and
- $Γ ⊢ e ⇐ t$ is "given the context Γ, term $e$ and type $t$, check that $t$ is
  a valid type for $t$".

The entire typing system described in Chapter ref:lambda can be rewritten using
these type judgments. The main principle is that language syntax is divided into
two sets of constructs: those that constrain the type of a term and can be
checked against an inferred term, and those that do not constrain the type and
need to infer it entirely.

#+label:simple-bidi
#+caption:Bidirectional typing rules for the λ→-calculus
#+attr_latex: :options [hbt]
#+begin_figure latex
\centering
\begin{tabular}{cc}
\AxiomC{$a:t∈Γ$}
\RightLabel{\textbf{(Var)}}
\UnaryInfC{$Γ⊢a⇒t$}
\DisplayProof &
\AxiomC{$c \text{ is a constant of type } t$}
\RightLabel{\textbf{(Const)}}
\UnaryInfC{$Γ⊢c⇒t$}
\DisplayProof \\[15pt]
\AxiomC{$Γ,x:t⊢e ⇐ u$}
\RightLabel{\textbf{(Abs)}}
\UnaryInfC{$Γ⊢λx.e ⇐ t→u$}
\DisplayProof &
\AxiomC{$Γ⊢f⇒t→u$}
\AxiomC{$Γ⊢a⇒t$}
\RightLabel{\textbf{(App)}}
\BinaryInfC{$Γ⊢f a ⇒ u$}
\DisplayProof \\[15pt]
\AxiomC{$Γ⊢a⇒t$}
\AxiomC{$Γ⊢a=b$}
\RightLabel{\textbf{(ChangeDir)}}
\BinaryInfC{$Γ⊢a⇐b$}
\DisplayProof &
\AxiomC{$Γ⊢a⇐t$}
\RightLabel{\textbf{(Ann)}}
\UnaryInfC{$Γ⊢(a:t)⇒t$}
\DisplayProof
\end{tabular}
#+end_figure

**** Bidirectional λ→ typing
In Figure ref:simple-bidi, this principle is demonstrated on the simply-typed
λ-calculus with only variables, λ-abstractions and function application. The
first four rules correspond to rules that we have introduced in Chapter
ref:lambda, with the exception of the constant rule that we have not used
there. The two new rules are *(ChangeDir)* and *(Ann)*: *(ChangeDir)* says that if we
know that a term has an already inferred type, then we can satisfy any rule that
requires that the term checks against a type equivalent to this one. *(Ann)* says
that to synthesize the type of an annotated term $a:t$, the term first needs to
check against that type.

Rules *(Var)* and *(Const*) produce an assumption, if a term is already in the
context or a constant, then we can synthesize its type. In rule *(App)*, if we
have a function with an inferred type then we check the type of its argument,
and if it holds then we can synthesize the type of the application $f a$. To
check the type of a function in rule *(Abs)*, we first need to check whether the
body of a function checks against the type on the right-hand side of the arrow.

While slightly complicated to explain, this description produces a provably
sound and complete type-checking system cite:ferreira14_bidi that, as a side
effect, synthesizes any types that have not been supplied by the user.
Extending this system with other language constructs is not complex: the rules
used in Montuno for local and global definitions are in Figure ref:defn-bidi.

#+label:defn-bidi
#+caption:Bidirectional typing rules for \texttt{let-in} and top-level definitions
#+attr_latex: :options [htb]
#+begin_figure latex
\begin{prooftree}
\AxiomC{$Γ ⊢ t ⇐ ⋆$}
\AxiomC{$Γ ⊢ a ⇐ t$}
\AxiomC{$Γ,x:t ⊢ b ⇒ u$}
\RightLabel{\textbf{(Let-In)}}
\TrinaryInfC{$Γ⊢\text{let }x:t=a\text{ in }b ⇒ u$}
\end{prooftree}
\begin{prooftree}
\AxiomC{$Γ ⊢ t ⇐ ⋆$}
\AxiomC{$Γ ⊢ a ⇐ t$}
\RightLabel{\textbf{(Defn)}}
\BinaryInfC{$Γ⊢x:t=a ⇒ t$}
\end{prooftree}
#+end_figure

**** Meta-context
One concern was not mentioned in the previous description: when inferring a
type, we may not know all its component types: in rule *(Abs)*, the type of the
function we check may only be constrained by the way it is called. Implicit
function arguments $\{A B\}→A→B→A$ also only become specific when the function
is actually called. The solution to this problem is a /meta-context/ that contains
/meta-variables/.

These stand for yet undetermined terms cite:norell07_meta, either as
placeholders to be filled in by the user in interactive proof assistants
(written with a question mark, e.g. as $?α$), or terms that can be inferred from
other typing constraints using unification. These meta-variables can be either
inserted directly by the user in the form of a hole $"\_"$, or implicitly, when
inferring the type of a λ-abstraction or an implicit function argument
cite:kovacs20_implicit.

There are several ways of implementing this context depending on the scope of
meta-variables, or whether it should be ordered or the order of meta-variables
does not matter. A simple-to-implement but sufficiently useful for our purposes
is a globally-scoped meta-context divided into blocks placed between top-level
definitions.

#+label:metas
#+caption:Meta-context for the expression \texttt{id id 5}
#+begin_src text
id : {A} → A → A = λx.x
?α = Nat
?β = ?α → ?α
five = (id ?β id) ?α 5
#+end_src

The meta-context implemented in Montuno is demonstrated in Listing
ref:metas. When processing a file, we process top-level expressions
sequentially. The definition of the $id$ function is processed, and in the
course of processing $five$, we encounter two implicit arguments, which are
inserted on the top-level as the meta-variables $?α$ and $?β$.

*** Unification
Returning to the rule *(ChangeDir)* in Figure ref:defn-bidi, a critical piece of
the algorithm is the equality of two types that this rule uses. To check a term
against a type $Γ ⊢ a ⇐ t$, we first infer a type for the term $Γ ⊢ a ⇒ u$, and
then test its equivalence to the wanted type $t = u$.

**** Conversion checking
The usual notion of equivalence in λ-calculus is /α-equivalence of β-normal
forms/, that we discussed in Chapter ref:lambda, which corresponds to structural
equality of the two terms. /Conversion checking/ is the algorithm that determines
if two terms are convertible using a set of conversion rules.

**** Unification
As we also use meta-variables in the type elaboration process, these variables
need to be solved in some way. This process of conversion checking together with
solving meta-variables is called /unification/ cite:gundry13_pattern_tutorial, and
is a well-studied problem in the field of type theory.

**** Pattern unification
In general, solving meta-variables is undecidable cite:abel11_sigma_unif. Given
the constraint $?α 5 = 5$, we can produce two solutions: $?α = λx.x$ and $?α =
λx.5$. There are several possible approaches and heuristics: first-order
unification solves for base types and cannot produce functions as a result;
higher-order unification can produce functions but is undecidable; /pattern
unification/ is a middle ground and can produce functions as solutions, with some
restrictions.

**** Renaming
In this thesis, I have chosen to reuse the algorithm from cite:mazzoli16_unify
which, in brief, assumes that a meta-variable is a function whose arguments are
all local variables in scope at the moment of its creation. Then, when unifying
the meta-variable with another (non-variable) term, it builds up a list of
variables the term uses, and stores such a solution as a /renaming/ that maps the
arguments to a meta-variable to the variables with which it was unified.  As the
algorithm is rather involved but tangential to the goals of this thesis, I will
omit a detailed description and instead point an interested reader at the
original source cite:mazzoli16_unify.

*** Implementation
As with the implementation of normalization-by-evaluation, we will look at the
most illustrative parts of the implementation. This time, the comparison can be
made directly side-by-side, between the bidirectional typing algorithm and its
implementation.

What was not mentioned explicitly is that the type elaboration algorithm has
\texttt{PreTerm}s as its input, and produces \texttt{Term}s in the case of type
checking, and pairs of \texttt{Term}s and \texttt{Value}s (the corresponding
types) in the case of type inference. Unification, not demonstrated here, is
implemented as parallel structural recursion over two \texttt{Value} objects.

In Figure ref:impl-bidi-dir, we see the previously described rule that connects
the checking and synthesis parts of the algorithm and uses
unification. Unification solves meta-variables as a side-effect, here it is only
in the role of a guard as it does not produce a value. The code exactly follows
the typing rule, the pre-term is inferred, resulting in a pair of a well-typed
term and its type, the type is unified with the wanted type and, if successful,
the produced term is the return value.

#+label:impl-bidi-dir
#+caption:Side-by-side comparison of the \textbf{ChangeDir} rule
#+attr_latex: :options [htb]
#+begin_figure latex
\begin{prooftree}
\AxiomC{$Γ⊢a⇒t$}
\AxiomC{$Γ⊢a=b$}
\RightLabel{\textbf{(ChangeDir)}}
\BinaryInfC{$Γ⊢a⇐b$}
\end{prooftree}
\begin{minted}{kotlin}
fun LocalContext.check(pre: PreTerm, wanted: Value): Term = when (pre) {
  // ...
  else -> {
    val (t, actual) = infer(pre.term)
    unify(actual, wanted)
    t
  }
}
\end{minted}
#+end_figure

Figure ref:impl-bidi-let shows the exact correspondence between the rule and its
implementation, one read left-to-right, the other top-to-bottom. Checking of the
type and value are straight-forward, translation of $Γ,x:t ⊢ b ⇒ u$ binds a
local variable in the environment, so that the body of the ~let-in~ expression
can be inferred, and the result is a term containing the inferred body and type,
wrapped in a ~TLet~.

#+label:impl-bidi-let
#+caption:Side-by-side comparison of the \textbf{Let-in} rule
#+attr_latex: :options [htb]
#+begin_figure latex
\begin{prooftree}
\AxiomC{$Γ ⊢ t ⇐ ⋆$}
\AxiomC{$Γ ⊢ a ⇐ t$}
\AxiomC{$Γ,x:t ⊢ b ⇒ u$}
\RightLabel{\textbf{(Let-In)}}
\TrinaryInfC{$Γ⊢\text{let }x:t=a\text{ in }b ⇒ u$}
\end{prooftree}
\begin{minted}{kotlin}
fun LocalContext.infer(pre: PreTerm): Pair<Term, Value> = when (pre)
  is RLet -> {
    val t = check(pre.type, VStar)
    val a = check(pre.defn, t)
    val (b, u) = localDefine(pre.name, a, t).infer(pre.body)
    Pair(TLet(pre.name, t, a, b), u)
  }
  // ...
}
\end{minted}
#+end_figure

Lastly, the rule for a term-level λ-abstraction is demonstrated in Figure
ref:impl-bidi-abs. The type produced on the last line of the snippet is a ~VPi~
unlike the rule, as the rule was written for the λ→-calculus; it is semantically
equivalent, however.  This rule demonstrates the creation of a new meta-variable
as without a placeholder, we are not able to infer the type of the body of the
function. This meta-variable might or might not be solved in the course of
inferring the body: either way, both the term and the type only contain a
reference to a globally-scoped meta-variable and not the solution.

#+label:impl-bidi-abs
#+caption:Side-by-side comparison of the \textbf{Abs} rule
#+attr_latex: :options [htb]
#+begin_figure latex
\begin{prooftree}
\AxiomC{$Γ,x:t⊢e ⇐ u$}
\RightLabel{\textbf{(Abs)}}
\UnaryInfC{$Γ⊢λx.e ⇐ t→u$}
\end{prooftree}
\begin{minted}{kotlin}
fun LocalContext.infer(pre: PreTerm): Pair<Term, Value> = when (pre)
  is RLam -> {
    val a = newMeta()
    val (b, t) = localBind(pre.name, a).infer(pre.body)
    Pair(TLam(pre.name, b), VPi(pre.name, a, VCl(env, t.quote())))
  }
  // ...
}
\end{minted}
#+end_figure

** Driver
This concludes the complex part of the interpreter, what follows are rather
routine concerns. Next part of the implementation is the driver that wraps the
backend, and handles its interaction with the surrounding world. In particular,
the parser, pretty-printer, and state management.

**** Parser
Lexical and syntactic analysis is not the focus of this work, so simply I chose
the most prevalent parsing library in Java-based languages, which seems to be
ANTLR[fn:2]. It comes with a large library of languages and protocols from
which to take inspiration[fn:3], so creating the parser was a rather simple
matter.  ANTLR provides two recommended ways of consuming the result of parsing
using classical object-oriented design patterns: a listener and a visitor. I
used neither as they were needlessly verbose or limiting[fn:4].

Instead of these, a custom recursive-descent AST transformation was used that is
demonstrated in Listing ref:parser. This directly transforms the ~ParseContext~
objects created by ANTLR into our ~PreTerm~ data type.

#+label:parser
#+caption:Parser to \texttt{PreTerm} transformation as a depth-first traversal
#+attr_latex: :position [htb]
#+begin_src kotlin
fun TermContext.toAst(): PreTerm = when (this) {
  is LetContext -> RLet(id.toAst(), type.toAst(), defn.toAst(), body.toAst())
  is LamContext -> rands.foldRight(body.toAst()) { l, r -> RLam(l.toAst(), r) }
  is PiContext -> spine.foldRight(body.toAst()) { l, r -> l.toAst()(r) }
  is AppContext -> operands.fold(oprator.toAst()) { l, r -> r.toAst()(l) }
  else -> throw UnsupportedOperationException(javaClass.canonicalName)
}
#+end_src

The data type itself is shown in Listing ref:presyntax. As with terms and
values, it is a recursive data structure, presented here in a slightly
simplified manner compared to the actual implementation, as it omits the part
that tracks the position of a term in the original source.  The grammar that is
used as the source for the parser generator ANTLR was already presented once in
the conclusion of Chapter ref:lambda, so the full listing is only included in
Appendix ref:spec.

#+label: presyntax
#+caption: Data type \texttt{PreTerm}
#+attr_latex: :position [htb]
#+begin_src kotlin
  sealed class PreTerm
  typealias Pre = PreTerm
  typealias N = String

  sealed class TopLevel
  data class RDecl(val n: N, val type: Pre) : TopLevel()
  data class RDefn(val n: N, val type: Pre?, val term: Pre) : TopLevel()
  data class RTerm(val cmd: Command, val term: Pre) : TopLevel()

  object RU : Pre()
  object RHole : Pre()
  data class RVar(val n: N) : Pre()
  data class RNat(val n: Int) : Pre()
  data class RApp(val lhs: Pre, val rhs: Pre) : Pre()
  data class RLam(val n: N, val body: Pre) : Pre()
  data class RPi(val n: N, val type: Pre, val body: Pre) : Pre()
  data class RLet(val n: N, val type: Pre, val defn: Pre, val body: Pre) : Pre()
  data class RForeign(val lang: N, val eval: N, val type: Pre) : Pre()
#+end_src

**** Pretty-printer
A so-called pretty-printer is a transformation from an internal representation
of a data structure to a user-readable string representation. The implementation
of such a transformation is mostly straight-forward, complicated only by the
need to correctly handle operator precedence and therefore parentheses.

This part is implemented using the Kotlin library ~kotlin-pretty~, which is itself
inspired by the Haskell library ~prettyprinter~ which, among other things, handles
correct block indentation and ANSI text coloring: that functionality is also
used in error reporting in the terminal interface.

An excerpt from this part of the implementation is included in Listing
ref:pretty, which demonstrates the precedence enumeration ~Prec~, the optionally
parenthesizing operation ~par~, and other constructions of the ~kotlin-pretty~ library.

#+label:pretty
#+caption:Pretty-printer written using \texttt{kotlin-pretty}
#+attr_latex: :position [htb]
#+begin_src kotlin
enum class Prec { Atom, App, Pi, Let }
fun Term.pretty(ns: NameEnv?, p: Prec = Prec.Atom): Doc<Nothing> = when (this) {
  is TVar -> ns[ix].text()
  is TApp -> par(p, Prec.App,
    arg.pretty(ns, Prec.App) spaced body.pretty(ns, Prec.Atom))
  is TLet -> {
    val d = listOf(
      ":".text() spaced ty.pretty(ns, Prec.Let),
      "=".text() spaced bind.pretty(ns, Prec.Let),
    ).vCat().align()
    val r = listOf(
      "let $n".text() spaced d,
      "in".text() spaced body.pretty(ns + n, Prec.Let)
    ).vCat().align()
    par(p, Prec.Let, r)
  } // ...
}
#+end_src

**** State management
Last component of the driver code is global interpreter state, which consists
mainly of a table of global names, which is required for handling incremental
interpretation or suggestions (tab-completion) in the interactive environment.
It also tracks the position of the currently evaluated term in the original
source file for error reporting.

Overall, the driver receives user input in the form of a string, parses it,
expression by expression supplies it to the backend, receiving back a global
name, or an evaluated value, which it pretty-prints and returns back to the
user-facing frontend code.

** Frontend
We will consider only two forms of user interaction: batch processing of a file
via a command-line interface, and a terminal environment for interactive use.
Later, with the Truffle interpreter, we can also add an option to compile a
source file into an executable using Truffle's capability to produce /Native
Images/.

**** CLI
#+label:cli-example
#+caption:Example usage of the CLI interface
#+attr_latex: :position [htb]
#+begin_src text
> cat demo.mt
id : {A} -> A -> A = \x. x
{-# TYPE id #-}
{-# ELABORATE id 5 #-}
{-# NORMALIZE id 5 #-}

> montuno demo.mt
{A} -> A -> A
id {Nat} 5
5
> montuno --type id
{A} -> A -> A
#+end_src

We will reuse the entry point of Truffle languages, a ~Launcher~ class, so that
integration of the Truffle interpreter is easier later, and so that we are able
to create single executable that is able to use both interpreters.

~Launcher~ handles pre-processing command-line arguments for us, a feature for
which we would otherwise use an external library like ~JCommander~. In the Truffle
interpreter, we will also use the /execution context/ it prepares using various
JVM options but for now, we will only use ~Launcher~ for argument processing.

Two modes of execution are implemented, one mode that processes a single
expression provided on the command line and \texttt{--normalize}s it,
\texttt{--elaborate}s it, or find its ~--type~. The second mode is sequential
batch processing mode that reads source code either from a file or from standard
input, and processes all statements and commands in it sequentially.

As we need to interact with the user we encounter another problem, that of error
reporting. It has been mentioned in passing several times, and in this
implementation of the interpreter, it is handled only partially. To report an
error well, we need its cause and location. Did the user forget to close a
parenthesis, or is there a type error and what can they do to fix it? Syntactic
errors are reported well in this interpreter, but elaboration errors only
sometimes.

Error tracking pervades the entire interpreter, position records are stored in
all data structures, location of the current expression is tracked in all
evaluation and elaboration contexts, and requires careful placement of update
commands and throwing and catching of exceptions. As error handling is
implemented only passably and is not the focus of this thesis, it is only
mentioned briefly here.

In Listing ref:cli-example, a demonstration of the command-line interface is
provided: normalization of an expression, batch processing of a file, and
finally, starting up of the REPL.

**** REPL
Read-Eval-Print Loop is the standard way of implementing interactive terminal
interfaces to programming languages. The interpreter receives a string input,
processes it, and writes out the result. There are other concerns, e.g.,
implementing name completion, different REPL-specific commands or, in our case,
switching the backend of the REPL at runtime.

From my research, JLine is the library of choice for interactive command-line
applications in Java, so that is what I used. Its usage is simple, and
implementing a basic interface takes only 10s of lines. The commands reflect the
capabilities of the command-line interface: (re)loading a file, printing out an
expression in normalized or fully elaborated forms, and printing out the type of
an expression. These are demonstrated in a simple way in Listing
ref:repl-example.

#+label:repl-example
#+caption:REPL session example
#+attr_latex: :position [htb]
#+begin_src text
> montuno
Mt> :load demo.mt
Mt> <TAB><TAB>
Nat Bool zero succ true false if natElim id const
Mt> :normalize id 5
5
Mt> :elaborate id 5
id {Nat} 5
Mt> :type id
{A} -> A -> A
Mt> :quit
#+end_src

* Adding JIT compilation to Montuno: MontunoTruffle
  :PROPERTIES:
  :CUSTOM_ID: jit-interpreter
  :END:
** Introduction
In the first part of this thesis, we introduced the theory of dependent types,
specified a small, dependently typed language, and introduced some of the
specifics of creating an interpreter for this language, under the name
Montuno. The second part is concerned with the Truffle language implementation
framework: we will introduce the framework itself and the features it provides
to language designers, and use it to build a second interpreter.

To reiterate the goal of this thesis, the intent is to create a vehicle for
evaluating whether adding just-in-time compilation produces visible improvements
in the performance of dependently typed languages. Type elaboration is often a
bottleneck in their performance cite:gross21_performance, and because it
involves evaluation of terms, it should be possible to improve using JIT
compilation; as optimizing AST evaluation is a good candidate for JIT
compilation. We have designed a language that uses features and constructs that
are representative of state-of-the-art proof assistants and dependently typed
languages, so that such evaluation may be used as a guideline for further work.

This chapter is concerned with building a second interpreter based on
Truffle. First, however, we need to introduce the idea of just-in-time
compilation in general, and see how the Truffle implements the concept.

** Just-in-time compilation
Just-in-time compilation (JIT) is an optimization technique that is based on the
assumption that, when executing a program, its functions (and the functions in
the libraries it uses) are only called in a specific pattern, configuration, or
with a specific type of data. While a program is running, the JIT compiler
optimizes the parts of it that run often; using an electrical engineering
metaphor, such parts are sometimes called /"hot loops"/.

Often, when talking about specific optimizations, we will use the terms /slow
path/ and /fast path/. The fast path is the one for which the program is currently
optimized, whereas the slow paths are all the other ones, e.g., function calls
or branches that were not used during the specific program execution.

There are several approaches to JIT compilation: /meta-tracing/ and /partial
evaluation/ are the two common ones.

**** Meta-tracing
A JIT compiler based on meta-tracing records a /trace/ of the path taken during
program execution. Often used paths are then optimized: either rewritten, or
directly compiled to machine code. Tracing, however, adds some overhead to the
runtime of the program, so only some paths are traced. While the programmer can
provide hints to the compiler, meta-tracing may result in unpredictable peak
performance. This technique has been successfully used in projects like PyPy,
that is built using the RPython JIT compiler cite:bolz14_meta, or on GHC with
mixed results cite:schilling13_tracing_jit.

**** Partial evaluation
The second approach to JIT compilation is called /partial evaluation/, also called
the /Futamura projection/. The main principle is as follows: where evaluating
(running) an interpreter on a program produces some output, partially evaluating
(specializing) the interpreter with regards to a program produces an
executable. The specializer assumes that the program is constant and can e.g.,
eliminate parts of the interpreter that will not be used by the program. This is
the approach taken by Truffle cite:latifi19_futamura.

#+LABEL: partial-eval
#+ATTR_LaTeX: :placement [htb]
#+CAPTION: Partial evaluation with constant folding (source: oracle.com)
[[./img/partial-evaluation.png]]

The basic principle is demonstrated in Figure ref:partial-eval, on actual code
produced by Truffle. In its vocabulary, a ~CompilationFinal~ value is assumed to
be unchanging for a single instance of the program graph node (the field ~flag~ in
the figure), and so the JIT compiler can transform a conditional ~if~ statement
into an unconditional one, eliminating the second branch.

There are, in fact, three Futamura projections, referred to by their ordinals:
the /first Futamura projection/ specializes an interpreter with regards to a
program, producing an executable. The /second Futamura projection/ combines the
specializer itself with an interpreter, producing a compiler. The third
projection uses the specializer on itself, producing a compiler maker. As we
will see in later sections, Truffle and GraalVM implement both the first and
second projections cite:latifi19_futamura.

** Truffle and GraalVM
I have mentioned Truffle several times already in previous chapters. To
introduce it properly, we first need to take a look at the Java Virtual machine
(JVM). The JVM is a complex platform that consists of several components: a
number of compilers, a memory manager, a garbage collector, etc., and the entire
purpose of this machinery is to execute ~.class~ files that contain the bytecode
representation of Java, or other languages that run on the JVM platform. During
the execution of a program, code is first translated into generic executable
code using a fast C1 compiler. When a specific piece of code is executed enough
times, it is further compiled by a slower C2 compiler that performs more
expensive optimizations, but also produces more performant code.

The HotSpotVM is one such implementation of this virtual machine. The GraalVM
project, of which Truffle is a part, consists of several components and the main
one is the Graal compiler. It is an Oracle research project that replaces the C2
compiler inside HotSpotVM, to modernize an aging code base written in C++, and
replace it with a modern one built with Java cite:duboscq13_graalir.  The Graal
compiler is used in other ways, though, some of which are illustrated in Figure
ref:graal. We will now look at the main ones.

#+LABEL: graal
#+ATTR_LaTeX: :placement [htb]
#+CAPTION: GraalVM and Truffle (source: oracle.com)
[[./img/graalvm.jpg]]

**** Graal
Graal itself is at its core a graph optimizer applied to program graphs. It
processes Java bytecode into a graph of the entire program, spanning across
function calls, and reorders, simplifies and overall optimizes it.

It actually builds two graphs in one: a data-flow graph, and an instruction-flow
graph. Data-flow describes what data is required for which operation, which can
be reordered or optimized away, whereas the instruction-flow graph stores the
actual order of instructions as the will happen on the processor: see Figure
ref:graal-graph.

#+LABEL: graal-graph
#+ATTR_LaTeX: :placement [htb] :scale .7
#+CAPTION: Graal program graph, visualized using IGV (source: norswap.com)
[[./img/graal-graph.png]]

**** SubstrateVM
As Graal is a standalone Java library, it can also be used in contexts other
than the HotSpotVM. SubstrateVM is an alternative virtual machine that executes
Graal-optimized code. It does not perform just-in-time optimizations, though,
but uses Graal as an ahead-of-time compiler. The result is a small stand-alone
executable file that does not depend on a JVM being installed on a machine,
called a /Native Image/. By replacing JIT compilation with ahead-of-time, these
binaries start an order-of-magnitude faster than regular Java programs, and can
be freely copied between machines, similar to Go or Rust binaries
cite:wurthinger13_graal.

**** Truffle
The Graal program graph, Graal IR, is a directed graph structure in static
single assignment form. As it is implemented in Java itself, the graph structure
is extensible cite:duboscq13_graalir, and it is this capability that makes
Truffle possible. Truffle is, in essence, a graph manipulation library and a set
of utilities for creating these graphs. These graphs are the abstract syntax
tree of a language: each node has an ~execute~ method, calling it returns the
result of evaluating the expression it represents.

**** Interpreter/compiler
When creating a programming language, There is a trade-off between writing a
interpreter and a compiler. An interpreter is straight-forward to implement and
each function in the host language directly encodes the semantics of a language
construct, but the result can be rather slow: compared to the language in which
the interpreter is written, in can be slower often by a factor to 10x to 100x
cite:wurthinger13_graal. A compiler, on the other hand, does not execute a
program directly, but instead maps its semantics onto the semantics of a
different virtual machine, be it the JVM, LLVM, or x86 assembly.

Truffle attempts to side-step this trade-off by making it possible to create an
interpreter that can be compiled on-demand via JIT when interpreted or
ahead-of-time into a Native Image; the result should be an interpreter-based
language implementation with has the performance of a compiled language and
access to all JVM capabilities (e.g. memory management). Instead of running an
interpreter inside a host language like Java, the interpreter is embedded one
layer lower, into a program graph that runs directly on the JVM and is
manipulated by the Truffle runtime that runs next to it.

**** Polyglot
Truffle languages can all run next to one another on the JVM. As a side-effect,
communication between languages is possible without the need for usual FFI
(foreign function interface) complications. As all values are JVM objects,
access to object properties uses the same mechanisms across languages, as does
function invocation. In effect, any language from Figure ref:graal can access
libraries and values from any other such language.

**** TruffleDSL
Truffle is a runtime library that manages the program graph and a number of
other concerns like variable scoping, or the object storage model that allows
objects from different languages to share the same layout. TruffleDSL is a
user-facing library in the form of a domain-specific language (DSL) that aids in
simplifies construction specialized Truffle node classes, inline caches,
language type systems, and other specifics. This DSL is in the form of Java
/annotations/ that give additional information to classes, methods or fields, so
that a DSL processor can then use them to generate the actual implementation
details.

**** Instrumentation
The fact that all Truffle languages share the same basis, the program graph,
means that a shared suite of tooling could be built on top of it: a profiler
(VisualVM), a stepping debugger (Chrome Debugger), program graph inspector
(IGV), a language server (Graal LSP). We will use some of these tools in further
sections.

** Truffle in detail
This concludes the general introduction to Truffle and GraalVM. Now we will
look at the specifics of how a Truffle language differs from the type of
interpreter we created previously.

The general concept is very similar to the previously created AST interpreter:
there is again a tree data structure at the core, where each node corresponds to
one expression that can be evaluated. The main differences are in a number of
details that were previously implicit, though, like the simple action of
"calling a function" which in Truffle involves the interplay of, at a minimum,
five different classes.

#+LABEL: truffle-arch
#+ATTR_LaTeX: :placement [htb] :scale .5
#+CAPTION: Architecture of a Truffle language (source: oracle.com)
[[./img/truffle-typical.png]]

Figure ref:truffle-arch shows the components involved in the execution of a
Truffle language. Most of our work will be in the parts labeled "AST", "AST
interpreter", and "AST rewriting". All of these involve the contents of the
classes that form the abstract syntax tree, as individual graph nodes contain
their data, but also their interpretation and rewriting specifics.

Overall, the implementation of a Truffle language can be divided into a few
categories. Some of the classes to be sub-classed and methods to be implemented
are included in parentheses to give a brief idea of the terminology we will use,
although we will expand on each one momentarily. These blocks are:

- language execution (~Launcher~),
- language registration (~Language~, ~Context~, ~ParsingRequest~),
- program entry point (~RootNode~, ~CallTarget~),
- node execution (~VirtualFrame~, ~execute~, ~call~),
- node specialization (~Specialize~, ~Profile~, ~Assumption~),
- value types (~TypeSystem~, ~ValueType~),
- compiler directives (~transferToInterpreter~, ~TruffleBoundary~),
- function calls (~InvokeNode~, ~DispatchNode~, ~CallNode~),
- object model (~Layout~, ~Shape~, ~Object~),
- and others (instrumentation, ~TruffleLibrary~ interfaces, threads).

**** Launcher
The entry point to a Truffle language is a ~Launcher~ (Listing
ref:truffle-launcher). This component handles processing command-line arguments,
and uses them to build a language execution context. A language can be executed
from Java directly without a ~Launcher~, but it handles all GraalVM-specific
options and switches, many of which we will use later, and correctly builds a
the language execution environment, including all debugging and other tools that
the user may decide to use.

#+label:truffle-launcher
#+caption:A minimal language \texttt{Launcher}
#+attr_latex: :position [htb]
#+begin_src kotlin
class MontunoLauncher : AbstractLanguageLauncher() {
    companion object {
        @JvmStatic fun main(args: Array<String>) = Launcher().launch(args)
    }
    override fun getDefaultLanguages(): Array<String> = arrayOf("montuno");
    override fun launch(contextBuilder: Context.Builder) {
        contextBuilder.arguments(getLanguageId(), programArgs)
        Context context = contextBuilder.build()
        Source src = Source.newBuilder(getLanguageId(), file).build()
        Value returnVal = context.eval(src)
        return returnVal.execute().asInt()
    }
}
#+end_src

**** Language registration
A language's primary object is a ~Language~, whose primary purpose is to answer
\texttt{ParsingRequest}s with the corresponding program graphs, and to manage
execution \texttt{Context}s that contain global state of a single language
process. It also specifies general language properties like support for
multi-threading, or the MIME type and file extension, and decides which
functions and objects are exposed to other Truffle languages.

#+label:truffle-reg
#+caption:A minimal \texttt{Language} registration
#+attr_latex: :position [htb]
#+begin_src kotlin
  @TruffleLanguage.Registration(
      id = "montuno", defaultMimeType = "application/x-montuno"
  )
  class Language : TruffleLanguage<MontunoContext>() {
      override fun createContext(env: Env) = MontunoContext(this)
      override fun parse(request: ParsingRequest): CallTarget {
          CompilerAsserts.neverPartOfCompilation()
          val parseAST = parse(request.source)
          val nodes = parseAST.map { toNode(it, this) }.toTypedArray()
          return Truffle.getRuntime().createCallTarget(ProgramRootNode(nodes))
      }
  }
#+end_src

**** Program entry point
Listing ref:truffle-reg demonstrates both a language registration and the
creation of a ~CallTarget~. A call target represents the general concept of a
"callable object", be it a function or a program, and a single call to a call
target corresponds to a single stack ~VirtualFrame~, as we will see later. It
points to the ~RootNode~ at the entry point of a program graph, as shown in Figure
ref:truffle-interop.

A ~CallTarget~ is also the basic optimization unit of Truffle: the runtime tracks
how many times a ~CallTarget~ was entered (called), and triggers optimization
(partial evaluation) of the program graph as soon as a threshold is reached.

#+LABEL: truffle-interop
#+ATTR_LaTeX: :position [htb]
#+CAPTION: Combination of regular and partially-evaluated code (source: oracle.com)
[[./img/truffle-interop.png]]

**** Node execution
A ~RootNode~ is a special case of a Truffle ~Node~, the basic building block of the
program graph. Each node has a single way of obtaining the result of evaluating
the expression it represents, the ~execute~ method. We may see nodes with multiple
~execute~ methods later, but they are all ultimately translated by the Truffle DSL
processor into a single method: Truffle will pick the most appropriate one based
on the methods' return type, arguments types, or user-provided /guard/
expressions.

Listing ref:add-lang contains an example of with two nodes. They share a parent
class, ~LanguageNode~, whose only method is the most general version of ~execute~:
one that takes a ~VirtualFrame~ and returns anything. An ~IntLiteralNode~ has only
one way of providing a result, it returns the literal value it
contains. ~AddNode~, on the other hand, can add either integers or strings, so it
uses another Truffle DSL option, a ~@Specialization~ annotation, which then
generates the appropriate logic for choosing between the methods ~addInt~,
~addString~, and ~typeError~.

#+label: add-lang
#+caption: Addition with type specialization
#+attr_latex: :position [htb]
#+begin_src kotlin
  abstract class LanguageNode : Node() {
    abstract fun execute(frame: VirtualFrame): Any
  }
  class IntLiteralNode(private val value: Long) : LanguageNode() {
    override fun execute(frame: VirtualFrame): Any = value
  }
  abstract class AddNode(
    @Child val left: LanguageNode, @Child val right: LanguageNode,
  ) : LanguageNode() {
    @Specialization fun addInt(left: Int, right: Int) = left + right
    @Specialization fun addString(left: String, right: String) = left + right
    @Fallback fun typeError(left: Any?, right: Any?): Unit
      = throw TruffleException("type error")
  }
#+end_src

**** Specialization
Node specialization is one of the main optimization capabilities of Truffle. The
~AddNode~ in Listing ref:add-lang can handle strings and integers both, but if it
only ever receives integers, it does not need to check whether its arguments are
strings on the /fast path/ (the currently optimized path). Using node
specialization, the ~AddNode~ can be in one of four states: uninitialized,
integers-only, strings-only, and both generic. Whenever it encounters a
different combination of arguments, a specialization is /activated/. Overall, the
states of a node form a directed acyclic graph: a node can only ever become more
general, as the Truffle documentation emphasize.

#+LABEL: truffle-deopt
#+ATTR_LaTeX: :placement [htb]
#+CAPTION: Node optimization and deoptimization in Truffle (source: oracle.com)
[[./img/truffle-deopt.png]]

**** (De)optimization
Node specialization combined with the optimization of a ~CallTarget~ when called
enough times are sufficient to demonstrate the process of JIT compilation in
Truffle. Figure ref:truffle-deopt demonstrates this process on a node type with
several more state transitions. When a node reaches a stable state where no more
specializations take place, that part of the program graph may be partially
evaluated. This produces efficient machine code instead of slow
interpreter-based code, specialized for the nodes' current state.

However, this compilation is /speculative/, it assumes that nodes will not
encounter different values, and this is encoded in explicit /assumption/ objects.
When these assumptions are invalidated, the compiled machine code is discarded,
and the nodes revert back to their non-optimized form. This process is called
/deoptimization/ cite:wimmer17_deoptimization, and can be explicitly invoked using
the Truffle method ~transferToInterpreter~.

After a deoptimization, the states of nodes should again stabilize, so that they
may be partially evaluated into efficient machine code once more. Often, this
(de)optimization process repeats multiple times during the execution of a single
program: the period from the start of a program until a stable state is called
the /warm-up/ phase.

**** Value types
Nodes can be specialized based on various criteria, but the above-mentioned
specialization with regards to the type of arguments requires that these types
are all declared and aggregated into a ~TypeSystem~ object and annotation. These
are again processed by Truffle DSL into a class that can check the type of a
value (~isUnit~, ~asBoolean~), and perform implicit conversion between them
(~castLong~). Listing ref:truffle-value demonstrates a ~TypeSystem~ with a custom
type ~Unit~ and the corresponding required ~TypeCheck~, and with an implicit
type-cast in which an integer is implicitly convertible into a long integer.

#+label:truffle-value
#+caption: A \texttt{TypeSystem} with an implicit cast and a custom type
#+begin_src kotlin
@CompilerDirectives.ValueType
object Unit

@TypeSystem(Unit::class, Boolean::class, Int::class, Long::class)
open class Types {
  companion object {
    @ImplicitCast
    fun castLong(value: Int): Long = value.toLong()
    @TypeCheck(Unit::class)
    fun isUnit(value: Any): Boolean = value === Unit
  }
}
#+end_src
#+latex: \vspace*{-.5cm}

**** Function invocation
An important part of the implementation of any Truffle language consists of
handling function calls. A common approach in multiple Truffle is as follows:
Given an expression like ~fibonacci(5)~. This expression is evaluated in multiple
steps: an ~InvokeNode~ resolves the function that the expression refers to
(~fibonacci~) into a ~RootNode~ and a ~CallTarget~, and evaluates its arguments (~5~). A
~DispatchNode~ creates a ~CallNode~ for the specific ~CallTarget~ and stores it in a
cache, and finally a ~CallNode~ what actually performs the switch from one part of
the program graph to another, building a stack ~Frame~ with the function's
arguments, and entering the ~RootNode~.

**** Stack frames
#+label:truffle-frame
#+caption: Basic operations with a \texttt{Frame}
#+attr_latex: :position [!htb]
#+begin_src kotlin
class ReadLocalVarNode(val name: String) : Node {
  fun execute(frame: VirtualFrame): Any {
    val slot: FrameSlot = frame.getFrameDescriptor().findFrameSlot(name)
    return frame.getValue(slot ?: throw TruffleException("$name not found"));
} }
class WriteLocalVarNode(val name: String, val body: Node) : Node {
  fun execute(frame: VirtualFrame): Unit {
    val slot: FrameSlot = frame.getFrameDescriptor().addFrameSlot(name)
    frame.setObject(slot, body.execute(frame));
} }
#+end_src

\texttt{Frame}s were mentioned several times already: they are Truffle's
abstraction of a stack frame. In general, stack frames contain variables and
values in the local scope of a function, those that were passed as its arguments
and those declared in its body. In Truffle, this is encoded as a ~Frame~ object,
and it is passed as an argument to all ~execute~ functions. Frame layout is set by
a ~FrameDescriptor~ object, which contains \texttt{FrameSlot}s that refer to parts
of the frame. Listing ref:truffle-frame demonstrates two nodes that interact
with a ~Frame~: a reference to a local variable, and a local variable declaration.

There are two kinds of a ~Frame~, virtual and materialized frames. A ~VirtualFrame~
is, as its name suggests, virtual, and the values in it can be freely optimized
by Truffle, reorganized, or even passed directly in registers without being
allocated on the heap (using a technique called Partial Escape Analysis). A
~MaterializedFrame~ is not virtual, it is an object at the runtime of a program,
and it can be stored in program's values or nodes. A virtual frame is preferable
in almost all cases, but e.g., implementing closures requires a materialized
frame, as it needs to be stored in a ~Closure~ object. This is shown in Listing
ref:truffle-closure, where ~frame.materialize()~ captures a virtual frame and
stores it in a closure.

#+label:truffle-closure
#+caption: A closure value with a \texttt{MaterializedFrame}
#+attr_latex: :position [htb]
#+begin_src kotlin
@CompilerDirectives.ValueType
data class Closure(
  val callTarget: RootCallTarget,
  val frame: MaterializedFrame,
)
class ClosureNode(val root: FunctionRootNode) : Node {
  fun executeClosure(frame: VirtualFrame): Closure = Closure(
    Truffle.getRuntime().createCallTarget(root),
    frame.materialize()
  )
}
#+end_src

**** Caching
These were the main features required for writing a Truffle language, but there
are several more tools for their optimization, the first one being /inline
caching/. This is an old concept that originated in dynamic languages, where it
is impossible to statically determine the call target in a function invocation,
so it is looked up at runtime. Most function call sites only use a limited
number of call targets, so these can be cached. As the cache is a local one,
placed at the call site itself, it is called an /inline cache/. This concept is
used for a number of other purposes, e.g., caching the ~FrameSlot~ in an
assignment operator, or the ~Property~ slot in an object access operation.

In the case of function dispatch, a ~DispatchNode~ goes through the stages:
/uninitialized; /monomorphic/, specialized to a single call target; /polymorphic/,
stores a number of call targets small enough, that the cost of searching the
cache is smaller than the cost of function lookup; and /megamorphic/, when the
number of call targets exceeds the size of the cache, and every function call is
looked up again. Figure ref:truffle-cache demonstrates this on a DispatchNode,
adding a polymorphic cache with size 3, and also demonstrates the Truffle DSL
annotations ~Cached~ and ~guards~. The cache key is the provided ~CallTarget~, based
on which a ~DirectCallNode~ is created and cached as well. The megamorphic case
uses an ~IndirectCallNode~: in a ~DirectCallNode~, the call target can be inlined by
the JIT compiler, whereas in the indirect version it can not.

#+label: truffle-cache
#+caption: Polymorphic and megamorphic inline cache on a \texttt{DispatchNode}
#+begin_src kotlin
abstract class DispatchNode : Node {
  abstract fun executeDispatch(
    frame: VirtualFrame, callTarget: CallTarget, args: Array<Any>): Any

  @Specialization(limit = "3", guards = "callTarget == cachedCallTarget")
  fun doDirect(
    frame: VirtualFrame, callTarget: CallTarget, args: Array<Any>,
    @Cached("callTarget") cachedCallTarget: CallTarget,
    @Cached("create(cachedCallTarget)") callNode: DirectCallNode
  ) = callNode.call(args)

  @Specialization(replaces = "doDirect")
  fun doIndirect(
    frame: VirtualFrame, callTarget: CallTarget, args: Array<Any>,
    @Cached("create()") callNode: IndirectCallNode
  ) = callNode.call(callTarget, args)
}
#+end_src

**** Guards
Figure ref:truffle-cache also demonstrates another optimization feature, a
generalization of nodes specializing themselves based on types or arguments. A
~Specialization~ annotation can have arbitrary user-provided /guards/. These are
often used in tandem with a cache, or with complex type specializations. In
general, using a ~Specialization~ makes it possible to choose the most optimal
node implementation based on its situation or configuration.

**** Profiles
Another tool for optimization are /profiles/. These are objects that the developer
can use to track which branch did code execution take: in the implementation of
an ~if~ statement, or when handling an exception. The compiler will use the
information collected during optimization, e.g., when the condition in an ~if~
statement was true every time, and it is tracked in a ~ConditionProfile~, the
compiler will omit the ~else~ branch during compilation.

**** Assumptions
/Assumptions/ are the last tool that a developer can use to provide more
information to the compiler. Unlike profiles and specializations that are local
to a node, assumptions are global objects whose value can be changed from any
part of a program graph. An assumption is /valid/ when created, and it can be
/invalidated/, which triggers which triggers deoptimization of any code that
relies on it. A typical use of assumptions is shown in Figure ref:truffle-assume
cite:shopify2020, in which TruffleRuby relies on the fact that global variables
are only seldom changed and can be cached. A ~ReadGlobalVarNode~ reads the value
of the global variable only the first time, and relies on two assumptions
afterwards. These are invalidated whenever the value of the variable changes,
and the cached value is discarded.

#+label:truffle-assume
#+caption:Cached reading of a global variable using assumptions \cite{shopify2020}
#+begin_src kotlin
@Specialization(assumptions = [
  "storage.getUnchangedAssumption()",
  "storage.getValidAssumption()"
])
fun readConstant(
  @Cached("getStorage()") storage: GlobalVariableStorage,
  @Cached("storage.getValue()") value: Any
) = value
#+end_src

**** Inlining
During optimization, the Graal compiler replaces \texttt{DirectCallNode}s with
the contents of the call target they refer to, performing function /inlining/
cite:wurthinger17_partial_eval. Often, this is the optimization with the most
impact, as replacing a function call with the body of the callee means that many
other optimizations can be applied. For example, if a ~for~ loop contains only a
function call and the function is inlined, then the optimizer could further
analyze the data flow, and potentially either reduce the loop to a constant, or
to a vector instruction.

There are potential drawbacks, and Truffle documentation warns developers to
place ~TruffleBoundary~ annotations on functions that would be expanded to large
program graphs, like ~printf~, as Graal will not ever inline a function through a
~TruffleBoundary~.

**** Splitting
Related to inlining, a call target can also be /split/ into a number of
/monomorphic/ call targets. Previously, we saw an ~AddNode~ that could add either
integers or strings. If this was a global or built-in function that was called
from different places with different configurations of arguments, then this node
could be split into two: one that only handles integers and one for
strings. Only the monomorphic version would then be inlined at a call site,
leading to even better possibility of optimizations.

Both of these two techniques, inlining and splitting, are guided by Graal
heuristics, and they are generally one on the last optimization techniques to be
checked when there are no more gains to be gained from caching or
specializations.

**** Object model
#+label:frames
#+caption:Accessing an object property using a \texttt{Shape} and a \textt{Property} \cite{vergu19_scopes}
#+begin_src kotlin
@Specialization(guards = [
  "addr.key() == keyCached",
  "shapeCached.check(addr.frame())"
], limit = "20")
fun doSetCached(
  addr: FrameAddr, value: Any,
  @Cached("addr.key()") keyCached: Occurrence,
  @Cached("addr.frame().getShape()") shapeCached: Shape,
  @Cached("shapeCached.getProperty(keyCached)") slotProperty: Property
): Unit {
  slotProperty.set(addr.frame(), value, shapeCached)
}
#+end_src

Truffle has a standard way of structuring data with fixed layout, called the
Object Storage Model cite:grimmer15_polyglot. It is primarily intended for class
instances that have a user-defined data layout, but e.g., the meta-interpreter
project DynSem cite:vergu19_scopes uses it for variable scopes, and TruffleRuby
uses it to make C \texttt{struct}s accessible from Ruby as if they were
objects. Similar to \texttt{Frame}s, an empty ~DynamicObject~ is instantiated from
a ~Shape~ (corresponds to a ~FrameDescriptor~) that contains several instances of a
~Property~ (corresponds to a ~FrameSlot~). Figure ref:frames shows the main method
of a node that accesses an object property, also utilizing a polymorphic cache.

**** Interop
As previously mentioned, it is possible to evaluate /foreign/ code from other
languages using functions like /eval/, referred to as /polyglot/. However, Truffle
also makes it possible to use other languages' /values/: to define a foreign
function and use it in the original language, to import a library from a
different language and use it as if it was native. This is referred to as an
interoperability message protocol or /interop/, for short.

This is made possible by Truffle /libraries/, that play a role similar to
/interfaces/ in object-oriented languages cite:grimmer15_polyglot, and describe
capabilities of \texttt{ValueType}s. A library /message/ is an operation that a
value type can support, and it is implemented as a special node in the program
graph, as a nested class inside the value type. The \texttt{ValueType}s of a
foreign language then need to be mapped based on these libraries into a
language: a value that implements an ~ArrayLibrary~ can be accessed using array
syntax, see Listing ref:libraries. Libraries are also used for polymorphic
operations inside a language if there is a large amount of value types, to
remove duplicate code that would otherwise be spread over multiple
\texttt{Specialization}s.

#+label:libraries
#+caption:Array access using a \texttt{Library}\protect\footnotemark
#+begin_src kotlin
class ArrayReadNode : Node {
  @Specialization(guards = "arrays.isArray(array)", limit = "2")
  fun doDefault(
    array: Object, index: Int, @CachedLibrary("array") arrays: ArrayLibrary
  ): Int = arrays.read(array, index)
}
#+end_src
#+latex: \footnotetext{Source: \url{https://www.graalvm.org/graalvm-as-a-platform/language-implementation-framework/TruffleLibraries/}}

** Mapping concepts to Truffle
*** How Truffle can help?
While the framework is a general language implementation framework, many
concepts and features are based on speculative optimization, which is best
applicable in dynamically-typed languages, and first need to be mapped onto the
features required by our type elaboration and normalization procedures.

Truffle is not primarily aimed at statically-typed languages or functional
languages. Its most easily accessible benefits lie in speculative optimization
of dynamically typed code and inline caches, where generic object-oriented code
can be specialized to a specific value type. Statically-typed languages have a
lot more information regarding the values that will flow through a function, and
e.g. GHC has a specific /specialization/ compiler pass.

However, there is a lot of overlap between the static optimizations done by
e.g. GHC and runtime optimizations done by Graal. An example would be
unfolding/inlining, where the compiler needs to make a single decision of
whether to replace a call to a function with its definition -- a decision that
depends on the size of the definition, whether they are in the same module, and
other heuristics cite:jones02_inliner. A Truffle interpreter would be able to
postpone the decision until execution time, when the definition could be inlined
if the call happened enough times.

Its execution model is a tree of nodes where each node has a single operation
~execute~ with multiple specializations. The elaboration/evaluation algorithm from
the previous chapter, however, has several interleaved algorithms (infer, check,
evaluate, quote) that we first need to graft on to the Truffle execution model.

In dynamic interpreters that Truffle is aimed at, it is easy to think of the
interpreter structure as "creating a graph through which values flow".

*** 

*** Functional languages on Truffle
--- one paragraph each, specialties

For inspiration, I have looked at a number of other functional languages created
using Truffle: a number of theses (TruffleClojure cite:feichtinger15_clojure,
TrufflePascal cite:flimmel17_truffle_pascal, Mozart-Oz
cite:istasse17_oz_truffle), two Oracle projects (FastR cite:stadler16_fastr,
TruffleRuby cite:shopify2020), and a few other projects that will be mentioned
throughout the text.

- https://github.com/enso-org/enso/
- https://github.com/cesquivias/mumbler
- DynSem, cite:vergu19_scopes - OSM for frames/scopes

While I was finalizing this thesis, the Enso project was publicly released. It
is a dependently-typed language that implements many of the same principles that
I do in this thesis. I have attempted to incorporate some of its solutions that
were better than the solutions that were originally presented here, for the sake
of comparison. These will be mentioned whenever relevant.

Evaluate languages on:
- overall project structure and runtime flow
- global/local names and environment handling
- calling convention
- lazy evaluation
- closure implementation
- graph manipulation, TruffleBoundaries, specializations

**** Truffled PureScript
https://github.com/slamdata/truffled-purescript/

Purescript is a derivative of Haskell, originally aimed at frontend
development. Specific to Purescript is eager evaluation order, so the Truffle
interpreter does not have to implement thunks/delayed evaluation.

Simple node system compared to other implementations:
- types are double and Closure (trivial wrapper around a RootCallTarget and a MaterializedFrame)
- VarExpr searches for a variable in all nested frames by string name
- Data objects are a HashMap
- ClosureNode materializes the entire current frame
- AppNode executes a closure, and calls the resulting function with a { frame, arg }
- CallRootNode copies its single argument to the frame
- IR codegen creates RootNodes for all top-level declarations, evaluates them,
  stores the result, saves them to a module Frame
- Abstraction == single-argument closure

**** FastR
One of the larger Truffle languages, a replacement for GNU R, which was "made
for statistics, not performance". Faster without Fortran than with (no native
FFI boundary, allows Graal to optimize through it)

cite:stadler16_fastr

Interop with Python, in particular - scipy + R plots

Node replacement for specializing nodes, or when an assumption gets invalidated
and the node should be in a different state (AbsentFrameSlot,
ReplacementDispatchNode, CallSpecialNode, GetMissingValueNode, FunctionLookup.

#+begin_src kotlin
val ctx = Context.newBuilder("R").allowAllAccess(true).build();
ctx.eval("R", "sum").execute(arrayOf<Int>(1,2,3));
#+end_src

#+begin_src R
benchmark <- function(obj) {
    result <- 0L
    for (j in 1:100) {
       obj2 <- obj$objectFunction(obj)
       obj$intField <- as.integer(obj2$doubleField)
       for (i in 1:250) { result <- obj$intFunction(i, obj$intField) }
    }
    result
}
benchmark(.jnew("RJavaBench"))
#+end_src

Special features:
- Promises (call-by-need + eager promises)

**** Cadenza
cite:kmett_2019

- FrameBuilder - specialized MaterializedFrame
- Closure - rather convoluted-looking code

Generating function application looks like:
- TLam - creates Root, ClosureBody, captures to arr, arg/envPreamble
- Lam - creates Closure, BuilderFrame from all captures in frame
- Closure - is a ValueType, contains ClosureRootNode
- ClosureRootNode - creates a new VirtualFrame with subset of frame.arguments

**** TruffleClojure
Implemented in a Master's thesis cite:feichtinger15_clojure

- I might want to implement envs as tries? Not arrays nor linked lists? Need to try
- each method impl is a root node, kept in bundles of callTargets by a ClojureFn
- closures - by a reference to the outer materializedFrame
- macros expanded during parse time, arguments not evaluated
- macroexpand function that expands macros
- separate section with a heading+listing+description of each special form (do
  we need this?)


*** Approach
We also have several options with regard to the depth of embedding: The most
natural fit for Truffle is term evaluation, where a term could be represented as
a value-level Term, and a CallTarget that produces its value with regard to the
current environment. We can also embed the bidirectional elaboration algorithm
itself, as a mixture of infer/check nodes.

The representation is also quite different from the functional interpreter where
we have used functions and data classes, as in Truffle, all values and operations
need to be classes.

There are several concerns here:
- algorithmic improvement is asymptotic -- the better algorithm, the better we
  can optimize it
- Truffle's optimization is essentially only applicable to "hot code", code that
  runs many times, e.g. in a loop
- We need to freely switch between Term and Value representations using
  eval/quote

- program features - what do we do, and how to map it to Truffle:
  - infer/check - nodes
  - eval/quote - term = graph, value = value
  - function dispatch
  - instantiating based on type arguments

- project structure - package list with brief description?

Specific changes:
- everything is a class, rewrite functions/operations as classes/nodes
- annotations everywhere
- function dispatch is totally different
- lazy values need to be different
- ???

- required restructuring: compiler structure, hard parts, mention other
  languages throughout and not specifically an info dump
- providing more information - specialization, constants, invalidation

evaluation phases - translate to Code, run typecheck, run eval vs glued, ???

show program graphs: id, const, const id; optimized graphs

- Type system - Fun, Pap, Closure?, U
- Arrays - how much copying?

** Specific changes in implementation
*** Data structures
We need to use arrays, Collections are not recommended

Arguments copied to the local frame in function preamble, to have unified access
to then and not need to duplicate logic

Frames and frame descriptors for local/global variable

References, indices, uninitialized references

dispatch, invoke, call Nodes, argument schema (copying), ?

eta is TailCallException (2 para + example)

Passing arguments - the technical problem of copying arguments to a new stack
frame in the course of calling a function.

Despite almost entirely re-using the Enso implementation of function calls, with
the addition of implicit type parameters and without the feature of default
argument values,

I will nonetheless keep my previous analysis of calling conventions in
functional Truffle languages here, as it was an important part of designing an
Truffle interpreter and I spent not-insignificant amounts of time on it.

I have discovered Enso only a short while before finishing my thesis, and had to
incorporate the technologically-superior solution

Several parts of creating an AST for function calls:
- determining the position of arguments on the original stack - or evaluating
  and possibly forcing the arguments
- determining the argument's position on the stack frame of the function
- using this position in the process of inferring the new function call
- dispatch, invoke, call nodes???

**** Value types
Data classes with call targets

...depends on what will work

Term and Val are ValueTypes that contain a callTarget - eval/quote(?)

We could use Objects/Shapes/Layouts for dependent sums or non-dependent named
coproducts.

*** Normalization
**** Evaluation order
We need to defer computations as late as possible - unused values that
will be eliminated (1 para)

CBPV concepts, thunks with CallTargets (3 paras, example)

**** Calling convention
the need for the distinction - in languages with currying

the eval/apply paper is a recipe for a stack-based implementation of
currying and helpful in our case when we need explicitly manage our stack via
Frames as opposed to the interpreter where we relied on the host language for
this functionality

known/unknown calls, partially/fully/over-saturated calls

cite:marlow04_fast_curry

push-enter - arguments are pushed onto the stack, the function then takes as
many as it requires

eval-apply - the caller sees the arity of the function and then decides whether
it is over-applied (evaluates the function and creates a continuation), appllied
exactly (EVAL), or under-applied (creates a PAP, a closure-like value)

- function application in languages with currying can be implemented using two
  evaluation models, push/enter and eval/apply
- compiled implementations should use eval/apply
- push/enter - arguments are pushed onto the stack, fun is entered, fun checks
  the number of arguments
- eval/apply - caller evaluates the function and applies it to the correct
  number of arguments
- need to distinguish known and unknown function calls,
- formalism uses heap objects FUN(●≥0), PAP(●(f)≥●≥1), CON(constructor), THUNK,
  BLACKHOLE
- + unboxed values not wrapped in any of these
- Rules: thunk→blackhole, blackhole→val, exact, over, under, thunkCall, papCall, retCall
- Truffle in theory supports both, but eval/apply plays better to the
  optimization where a calltarget should be as specialized as possible
- unboxing requires instanceof checks, we want to specialize/split
- push/enter means we need to copy arguments into an array

*** Elaboration
DerefNode - reads a variable, either blocks or returns an object. A meta-variable is
replaced with a Term whenever it is evaluated/unblocked

*** Polyglot
Demonstrate calling Montuno from other languages

Demonstrate Montuno's eval construct

Demonstrate Montuno's FFI construct - requires projections/accessors

*** Driver
ParsingRequest/InlineParsingRequest

Unfortunately, Truffle requires that there is no access to the interpreter state
during parsing, which means that we need to perform elaboration inside of a
~ProgramRootNode~ itself, "during runtime" per se.

need to perform elaboration inside a programRootNode (not while parsing)

stmt;stmt;expr -> return a value

*** Frontend
REPL needs to be implemented as a ~TruffleInstrument~, it needs to modify and
otherwise interact with the language context.

Language registration in mx/gu

**** Instrumentation
Truffle is not only aimed at language developers but also at developers of
language tools. We will specifically need this I would not mention this otherwise, but we will need to
implement an instrument to add a REPL to the language. These tools are not
specific to a single language, but most of them work for any language
cite:seaton14_debugging. The ~Instrument~ API is based on events, an instrument
declares which nodes it wants to receive events from based on source MIME or on
tags (an annotation for debugging tools, can be custom), a /probe/ is inserted on
these places (a program location that emits events) using a /WrapperNode/ that
replaces a node and contains both this probe and the original, and this probe
emits /events/ that mark when execution has entered of left a node.

** Bleh                                                            :noexport:
- cite:altenkirch08_pisigma, cite:altenkirch10_pisigma - "I had discovered the ΠΣ paper when finishing my thesis: too late, unfortunately"
- cite:eisenberg20_stitch is an interesting tutorial of a dependent interpreter of dependent languages
- cite:juan20_unif_thesis - well described contexts + language specification - can I take as inspiration?
- well investigated in cite:lindley05_nbe_sml where there is a comprehensive of
  NbE techniques as applied to ML
- also in cite:lindley05_nbe_sml there is a treatment of η reduction/expansion - READ

* Evaluation
  :PROPERTIES:
  :CUSTOM_ID: evaluation
  :END:

We want to evaluate a few programs of equivalent functionality, as evaluated by elaboration, type-checking, and simplification in a number of dependently typed languages.

We also want to evaluate the performance of general β-normalization which only requires a functional language--this is secondary, however.

We are mainly interested in asymptotic behaviors and not on constant factors. Just-in-time compiled languages especially suffer from long warm-up times, which means that common evaluation of "repeatedly running a command" will not perform well.

** Subjects
(One page)

Subjects for elaboration: Agda, Idris, Coq, GHC. Also cooltt, smalltt, redtt, Lean, ?

Subjects for normalization: as above, but also Cadenza (STLC), Clojure, GHC, Scala, OCaml, ML, Eta, Frege, ?

Also subjects: Montuno, MontunoTruffle, and possibly other optimized versions.

** Workload
(One page)

 - Nats - large type elaboration, call-by-need test
 - Nats - type-level calculation
 - Nats - value-level calculation
 - Nats - equality/forcing
 - Functions - nested function elaboration, implicits
 - Functions - embedded STLC?
 - pairs - large type elaboration, call-by-need test
 - pairs - nested accessors

- typical programs - see typical Agda
- computation-heavy tests (numerics,)
- memory-heavy tests (id id id...)
- nofib suite? (typecheck, circsim, infer, anna)
- hanoi, sort an array using trees
- use FFI in benchmarks - externalize a FFT?

Mention sources (this is from smalltt ^^)

Brief descriptions, what does each one evaluate/stress?

** Methodology
(One page)

We need to use in-language support, if available. We want to avoid measuring interpreter start-up, program parsing time, and other confounders.

To measure: memory usage (curve), compilation speed (in a type-heavy test), evaluation speed (in a compute-heavy test)

hyperfine to benchmark - measures speed (what about ~prof~?)
memory profile from stderr output

Krun benchmark runner + its warmup_stats functionality for statistical analysis of steady states, number of iterations it took to stabilize.

memory profile from GHC's RTS for agda/idris/smalltt (+RTS -p) (what about coq? - https://github.com/coq/coq/blob/master/dev/doc/profiling.txt) Graal's default memory profiler

Mention specific parameters (X iterations, machine specs, ?)

*** Renaissance
cite:prokopec19_renaissance

- a set of benchmarks and measurement tools
- measures: synchronized, object.wait, object.notify, atomic ops, park
  operations, average cpu usage, cache misses, objects allocated, arrays
  allocated, method calls, dynamic method calls
- needs us to package it in a special way, but useful to compare between truffle
  optimization versions
- https://github.com/Gwandalff/krun-dsl-benchmarks is an alternative that has
  examples with Truffle, measures only s/op

** Preliminary results
(1 page, interp, Truffle, Agda, Idris, Coq)

* Optimizations: Making MontunoTruffle fast
   :PROPERTIES:
   :CUSTOM_ID: optimizations
   :END:
** Possible performance problem sources
\blind{12}

Reiterate JGross

how to find out whether X is relevant to us or not? How to prove the effect of JIT?

Show asymptotes - binders, terms, sizes

Show the graphs - large values, many iterations (warmup), sharing

** Possible optimizations
Show before and afters for each optimization

What does Enso do, optimization phases?

What can we do?

Hash consing = sharing structurally equal values in the environment. See below from Kmett:
https://gist.github.com/ekmett/6c64c3815f9949d067d8f9779e2347ef

Inlining, let-floating

Avoid thunk chaining: box(box(box(() => x))

Frame slot clearing - simplifies Graal's role, as Graal tracks dataflow, and
this shortens an object's lifetime

Static optimization  - changing the structure of the interpreter so that it
would be faster even without JIT

Dynamic optimization - using more Truffle-specific features, so that Graal can
more efficiently optimize the code: CompilerDirectives, BranchProfiles,
TruffleBoundaries, inline caches, ControlFlowExceptions

"Immutable, except to simplify" + assumptions
Maximize evaluation sharing - globals, cache, ?

- cite:blaguszewski10_llvm - potential optimizations, LLVM impl, closures
- cite:gross14_coq_experience - Coq experience, a few reasons, comparison
- cite:gross21_performance - a lot of reasons in Coq
- cite:eisenberg20_stitch - CSE

Ruby uses threads, can we? Automatic parallelism
- cite:reid98_resumable_holes - concurrency & parallelism in GHC evaluation
- cite:hughes82_supercombinators - CAFs? Lazy evaluation?

Think about the fast vs slow path!

- cite:zheng17_deoptimization - reasons for deoptimization

OSM in DynSem:
- DynSem also had to consider concept mapping: a program graph starts with generic node operations that immediately specialize to language-specific operations during their first execution
- HashMaps are efficient, but bring downsides. The Graal compiler cannot see inside the HashMap methods, and so cannot analyze data flow in them and use it to optimize them.
- DynSem also had to deal with runtime specification of environment manipulation as this is also supplied by the language specification. Also split between local short-lived values inside frames, and long-lived heap variables.
- Relevant to us is their use of the Object Storage Model, which they use to model variable scoping which is the processed into fixed-shape stack frames (separate from the Truffle Frames, this is a meta-interpreter). OSM's use case is ideal for when all objects of a certain type have a fixed shape. This is ideal for us, as tuples and named records have, by definition, a fixed shape (unlike Ruby etc. we do not support dynamic object reshaping, obviously).
- They did it separately from the Virtual/MaterializedFrame functionality to avoid the overhead of MaterializedFrames that Graal cannot optimize away.
- Truffle/Graal discourage the use of custom collections, and instead push developers towards Frames (which support by-name lookups) and Objects (same).

To enhance compilation specialization/inlining:
- Visualizations of call graphs - whether or not node children are stable calls
- Most DynSem calls are not stable calls, they are dispatched on runtime based on arguments - something that Graal does not see as stable (CompilationFinal)
- Two types of rules: mono- and polymorphic. based on whether they are called with different types of values at runtime. Poly- are not inlined
- DynSem found two types: dynamic dispatch (meta-interpreter depended on runtime info), and structural dispatch (based on the program AST and not on values). This is similar to our EvalNode, QuoteNode and similar, which depend on the type of the value
- Overloaded rules--rules with the same input shape--are merged into a single FusedRule node and iterated over with @ExplodeLoop.
- For mono/polymorphic rules, they use an assumption that a rule is monomorphic, specialize the rule, and recompile if it becomes polymorphic.
- Inlining nodes - polymorphic rules reduced to a set of monomorphic rules - a rule from the registry is cloned in an uninitialized state in a monomorphic call site and "inlined" (in a CompilationFinal field)
- They use a central registry of CallTargets that contain rules that they can clone and adopt locally if necessary to specialize--we can do the same!
- Disadvantages: there is more to compile and inline by Graal, instead of a CallTarget, they use a child. Likely to take longer to stabilize, but faster in the end.

** Glued evaluation
\blind{12}

An optimization technique that attempts to avoid even more computation.

Parallel operation on two types of values, glued and local. Glued are lazily evaluated to a fully unfolded form; local are eagerly computed to a head-normal form but not fully unfolded, to prevent size explosions. This results in better performance in a large class of programs, although it is not an asymptotic improvement, as we have a small eagerly evaluated term for quoting, and a large lazily evaluated for conversion checking.

This is another case of specialization: we have two operations to perform on the same class of values, but each operation has its own requirements; in this case, on the size of the terms as in quoting we want a small folded value but require the full term for conversion checking.

cite:kaposi19_gluing

https://eutypes.cs.ru.nl/eutypes_pmwiki/uploads/Meetings/Kovacs_slides.pdf

** Splitting
\blind{12}

type specializations/dict passing

** Function dispatch
\blind{12}

lambda merging

eta expansion

** Caching and sharing
\blind{12}

Sharing computation and common values

Multiple references to the same object

let-floating

inlinable functions

** Specializations
\blind{6}

**** Truffle recommended optimizations
The optimization workflow recommended by the Truffle developers is as follows:
1. Run with a profiler to sample the application and identify responsible compilation units. Use a sampling delay (--cpusampler.Delay=MILLISECONDS) to only profile after warmup. See the Profiling guide.
2. Understand what is being compiled and look for deoptimizations. Methods that are listed to run mostly in the interpreter likely have a problem with deoptimization.
3. Simplify the code as much as possible where it still shows the performance problem.
4. Enable performance warnings and list boundary calls.
5. Dump the Graal graph of the responsible compilation unit and look at the phase After TruffleTier.
   1. Look at the Graal graphs at the phases After TruffleTier and After PartialEscape and check if it is what you would expect. If there are nodes there that you do not want to be there, think about how to guard against including them. If there are more complex nodes there than you want, think about how to add specialisations that generate simpler code. If there are nodes you think should be there in a benchmark that are not, think about how to make values dynamic so they are not optimized away.
6. Search for Invoke nodes in the Graal IR. Invoke nodes that are not representing guest language calls should be specialized away. This may not be always possible, e.g., if the method does I/O.
7. Search for control flow splits (red lines) and investigate whether they result from control flow caused by the guest application or are just artifacts from the language implementation. The latter should be avoided if possible.
8. Search for indirections in linear code (Load and LoadIndexed) and try to minimize the code. The less code that is on the hot-path the better.

---
Add more info on splitting!!

- ~--engine.TraceCompilation~ prints a line for each method compilation
- ~--engine.TraceCompilationDetail~ prints a line for compilation queuing, start, and finish
- ~--engine.TraceCompilationAST~ prints the entire compiled AST
- ~--engine.TraceInlining~ prints inlining decision details
- ~--engine.TraceSplitting~ prints splitting decisions
- ~--engine.TraceTransferToInterpreter~ prints a stack trace for each explicit invalidation
- ~--engine.TracePerformanceWarnings=(call|instanceof|store|all)~
  - ~call~ prints when PE cannot inline a call
  - ~instanceof~ prints when PE cannot resolve virtual ~instanceof~ to a specific type
  - ~store~ prints when PE store location argument is not compilation final
- ~--engine.CompilationStatistics~ prints total compilation statistics
- ~--engine.CompilationStatisticDetails~ prints compilation histograms for each node
- ~--engine.TraceMethodExpansion=truffleTier~ prints a tree of all expanded Java methods
- ~--engine.TraceNodeExpansion=truffleTier~ prints a tree of all expanded Nodes
- ~--engine.MethodExpansionStatistics=truffleTier~ prints total Graal nodes produced by a method
- ~--engine.NodeExpansionStatistics=truffleTier~ also includes Graal specializations
- ~--engine.InstrumentBoundaries~ prints info about boundaries encountered (should be minimized)
- ~--engine.InstrumentBranches~ prints info about branch profiles
- ~--engine.SpecializationStatistics~ prints details about specializations performed
- ~--vm.XX:+TraceDeoptimization~ prints all deoptimizations
- ~--vm.XX:+TraceDeoptimizationDetails~ prints all deoptimizations with details

It is also possible to control what is being compiled, add details to IGV graphs dumped, and print the exact assembly produced: see https://github.com/oracle/graal/blob/master/truffle/docs/Optimizing.md.

**** How to debug specializations
*Specialization histogram:* If compiled with ~-Atruffle.dsl.GenerateSpecializationStatistics=true~ and executed with ~--engine.SpecializationHistogram~, Truffle DSL will compile the nodes in a special way and show a table of the specializations performed during the execution of a program.

Example shown at
https://github.com/oracle/graal/blob/master/truffle/docs/SpecializationHistogram.md,
maybe include the table?

*Slow path only:* If compiled with ~-Atruffle.dsl.GenerateSlowPathOnly=true~, Truffle will only execute the last, most generic specialization, and will ignore all fast path specializations.

** Profiling
Now then, what tools to use to find the problems

*** Ideal Graph Visualizer
A graphical program that serves to visualize the process of Truffle graph optimization. When configured correctly, the IGV will receive the results of all partial evaluations.

*** CPU Sampler
#+COMMENT: https://github.com/oracle/graal/blob/master/truffle/docs/Profiling.md

Running the language launcher with the options ~--cpusampler --cpusampler.Delay=MILLISECONDS~ will start the CPU sampler. This tool serves to profile the guest language (as opposed to the regular JDK Async Profiler which will profile the entire process.

~--cpusampler.Delay~ helps to not include warm-up time in the results.

Using additional options (~--cpusampler --cpusampler.SampleInternal --cpusampler.Mode=roots --cpusampler.Output=json~) and postprocessing the generated JSON with an additional script we can create a so-called flamegraph with the results of the sampling.

#+COMMENT: https://github.com/eregon/FlameGraph/blob/graalvm/stackcollapse-graalvm.rb

* Discussion
** Results
(A few pages)

One-to-one evaluation and discussion of directly comparable subjects, confidence
intervals, likely causes of improvements/regressions, iterations to
steady-state.

** Discussion
Size of codebase

Effort required

Effect produced

Is this road viable?

** Next work
(A few pages, subsections/mini-headers)

FFI, tooling

RPython, K Framework - exploration

SPMD on Truffle, Array languages

More type extensions OR totality (as a proof assistent)

Finite types, universes, no type in type, HoTT, CoC

Is this useful at all? What's the benefit for the world? (in evaluation)

next work: LF, techniques, extensions, real language

* Conclusion
We tried X to do Y. It went well and we fulfilled the assignment.

As a side effect, I produced a reference book for functional/dependent language implementation.

Original goal was X, it grew to encompass Y, Z as well.
* (bibliography, start of appendix)                           :ignoreheading:

#+BEGIN_EXPORT latex
\makeatletter
\def\@openbib@code{\addcontentsline{toc}{chapter}{Bibliography}}
\makeatother
\begin{flushleft}
#+END_EXPORT

bibliographystyle:bibstyle
bibliography:bibliography.bib

#+BEGIN_EXPORT latex
\end{flushleft}
\iftwoside\cleardoublepage\fi
\appendix
\appendixpage
\iftwoside\cleardoublepage\fi
\startcontents[chapters]
% \setlength{\parskip}{0pt}
% \printcontents[chapters]{l}{0}{\setcounter{tocdepth}{2}}
% \setlength{\parskip}{0.5\bigskipamount}
\iftwoside\cleardoublepage\fi
#+END_EXPORT

* Contents of the attached data storage
* Language specification
   :PROPERTIES:
   :CUSTOM_ID: spec
   :END:
** Syntax
#+include: "../montuno/src/main/antlr/montuno/Montuno.g4" src antlr -n

** Semantics

** Built-in constructs
- Unit : Type
- unit : Unit
- Nat : Type
- zero : Nat
- succ : Nat → Nat
- natElim : {A} → Nat → A → (Nat → A) → A
- Bool : Type
- true : Bool
- false : Bool
- if : {A} → Bool → A → A → A
- fix : {A} → (A→A) → A
- the : (A) → A → A
- eval : {A} → String → A
- typeOf : {A} → A → Type

* Montuno
  :PROPERTIES:
  :CUSTOM_ID: montuno-data
  :END:
** Pre-terms
#+include: "../montuno/src/main/montuno/syntax/presyntax.kt" src kotlin -n
* Footnotes

[fn:14] 

[fn:13] In descriptions of the higher-order abstract syntax, the term /binders/ is
commonly used instead of function or λ-abstractions, as these constructs /bind/ a
value to a name. 

[fn:12] https://github.com/AndrasKovacs/normalization-bench 

[fn:11] Montuno, as opposed to the project Cadenza, to which this project is a
follow-up. Both are music terms, /cadenza/ being a "long virtuosic solo section",
whereas /montuno/ is a "faster, semi-improvised instrumental part".

[fn:10] Proof assistants also use the concept of a metavariable, often with the syntax $?α$.

[fn:9] The elements of $R$ are written as $(s₁,s₂)$, which is equivalent to $(s₁,s₂,s₂)$.

[fn:8] FastR is between 50 to 85x faster than GNU R, depending on the source. cite:fumero17_jit_gpu

[fn:7] Unfortunately, there are no officially published benchmarks, but a number of articles claim that TruffleRuby is 10-30x faster than the official C implementation. cite:shopify2020

[fn:6]Kotlin authors claim 40% reduction in the number of lines of code, (from https://kotlinlang.org/docs/faq.html)
[fn:5]https://kotlinlang.org/docs/idioms.html
[fn:4] In particular, ANTLR-provided visitors require that all return values share a common super-class. Listeners don't allow return values and would require explicit parse tree manipulation.
[fn:3]https://github.com/antlr/grammars-v4/
[fn:2]https://www.antlr.org/
[fn:1]Even though Kotlin seems not to be recommended by Truffle authors, there are several languages implemented in it, which suggests there are no severe problems.  "[...] and Kotlin might use abstractions that don't properly partially evaluate." (from https://github.com/oracle/graal/issues/1228)
