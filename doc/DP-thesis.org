#+BEGIN_COMMENT
Normostrana = 1800c ≈ 250 slov
org-word-count
org-wc-display

16.4. 2303w
17.4. 3023w
18.4. 3600w? (Σ36p)
19.4. 4000w? (Σ40p)
22.4. 5200w
24.4. 6700w
25.4. 7200w (Σ56p)
26.4. 7660w (Σ59p)
27.4. 8800w (Σ64p)
28.4. 10050w (Σ67p)
29.4. 11700w (Σ68p)
30.4. 12500w (Σ74p, 52p content)
3.5. 14000w (Σ81p, Σ58p content)

minimum: 12,500
target: 17,500
maximum: 30,000
#+END_COMMENT
* (front matter)                                              :ignoreheading:
#+LANGUAGE: en
#+OPTIONS: texht:nil toc:nil author:nil ':t H:6 num:3
#+LATEX_CLASS: fitthesis
#+LATEX_CLASS_OPTIONS: [english,zadani,odsaz]
#+EXCLUDE_TAGS: noexport
# print = B&W links and logo
# cprint = B&W links, color logo
#+BIND: org-latex-title-command ""
#+BIND: org-latex-prefer-user-labels t
#+BIND: org-latex-default-figure-position "hbt"
#+BEGIN_EXPORT latex
\maketitle
\setlength{\parskip}{0pt}
{\hypersetup{hidelinks}\tableofcontents}
\iftotalfigures\listoffigures\fi
\iftotaltables\listoftables\fi
\iftotallistings\listoflistings\fi
\listoftodos
\iftwoside\cleardoublepage\fi
\setlength{\parskip}{0.5\bigskipamount}
#+END_EXPORT
* (Recommended thesis structure)                                   :noexport:
1. "Úvod do knížečky", jedna strana, bez podkapitol
2. "Co bylo třeba vystudovat?" - "Je tato informace potřebná k pochopení toho, co jsem udělal?"
3. "Nové myšlenky" - rozhodl jsem, vymyslel, rozvrhl, odvodil, vylepšil, navrhl, ...
4. "Implementace a vyhodnocení" - udělal jsem, posbíral data, výsledky
5. "Závěr" - žádné nové poznatky, možnosti pokračování, "naladit oponenta"

* (Casual outline)                                                 :noexport:
*Story:*
- Let's build a fast Agda!
- Why Agda, what's at its core?
- Oh, dependent types? What's that good for, how does it work?
- Why are all of these languages functional?
- Lambda calculus and everything - that's a lot of formalisms...
- How to build it? Fast λs, that's a start, then build it up.
- So, does it work? What can I use it for, does it fulfill the design goals?
- Why is it so slow? Asymptotics...
- So, Truffle should make it fast, what specifically? What's the inspiration?
- How do we start, what needs to be changed?
- Wow, that's a lot of concepts, slow down, explain!
- So, does this changed work function at all? What about polyglots? What UIs?
- What can we make improve, was this just the start?
- Did we fulfill our goals, is this useful at all?
- If this is somebody's starting point, what will they need to know?

*Original goal:*
- efficient βη-normalization runtime
- Show the asymptotics on Peano, Church numbers

*Side outcome:* compilation of all the relevant concepts.

- Citations: Werthinger et al. [45] have developed [...]

eqref:eq:1
#+begin_export latex
\begin{equation}\label{eq:1}
 ≡ Π_{x:A}B(x)
\end{equation}
#+end_export

* Introduction
#+COMMENT: Motivation: why this project? Why should anyone care?

Proof assistants like Coq, F*, Agda or Idris, or other languages with dependent
types like Cayenne or Epigram, allow programmers to write provably
correct-by-construction code in a manner similar to a dialog with the compiler
cite:norell08_agda_tutorial. They also face serious performance issues when
applied to problems or systems on a large-enough scale
cite:gross14_coq_experience
cite:gross21_performance. Their performance grows exponentially with the number
of lines of code in the worst case cite:nawaz19_survey_provers, which is a
significant barrier to their use. While many of the performance issues are
fundamentally algorithmic, a better runtime system would improve the
rest. However, custom runtime systems or more capable optimizing compilers are
time-consuming to build and maintain. This thesis seeks to answer the question
of whether just-in-time compilation can help to improve the performance of such
systems.

Moving from custom runtime systems to general language platforms like e.g., the
Java Virtual Machine (JVM) or RPython cite:bolz14_meta, has improved the
performance of several dynamic languages: project like TruffleRuby, FastR, or
PyPy. It has allowed these languages to re-use the optimization machinery
provided by these platforms, improve their performance, and simplify their
runtime systems.

#+COMMENT: Problem definition: What exactly are you solving? What is the core and what is a bonus? What parameters should a proper solution to the problem have?

As there are no standard benchmarks for dependently typed languages, we design a
small, dependently-typed core language to see if using specific just-in-time
(JIT) compilation techniques produces asymptotic runtime improvements in the
performance of β-normalization and βη-conversion checking, which are among the
main computational tasks in the elaboration process and is also the part that
can most likely benefit from JIT compilation. The explicit non-goals of this
thesis are language completeness and interoperability, as neither are required
to evaluate runtime performance.

#+COMMENT: Existing solutions: be fair in identifying their strengths and weaknesses. Cite important works from the field of your topic. Try to define well what is the state of the art.

State of the art proof assistants like Coq, Agda, Idris, or others is what we
can compare our results with. There is also a large number of research projects
being actively developed in this area; Lean is a notable one that I found too
late in my thesis to incorporate its ideas. However, the primary evaluation will
be against the most well established proof assistants.

inline:Reformulate As for the languages that use Truffle, the language implementation framework
that allows interpreters to use the JIT optimization capabilities of GraalVM, an
alternative implementation of the Java Virtual Machine: there are numerous
general-purpose functional languages, the most prominent of which are
TruffleRuby and FastR. Both were reimplemented on the Truffle platform,
resulting in significant performance improvements[fn:7][fn:8]. We will
investigate the optimization techniques they used, and reuse those that are
applicable to our language.

There is also a number of functional languages on the Java Virtual Platform that
do not use the Truffle platform, like Clojure, Scala or Kotlin, as well as
purely functional languages like Eta or Frege. All of these languages compile
directly to JVM byte code: we may compare our performance against their
implementation, but we would not be able to use their optimization
techniques. To the best of my knowledge, neither meta-tracing nor partial
evaluation have been applied to the dependently-typed lambda calculus.

The closest project to this one is Cadenza cite:kmett_2019, which served as the
main inspiration for this thesis. Cadenza is an implementation of the
simply-typed lambda calculus on the Truffle framework. While it is unfinished
and did not show as promising performance compared to other simply-typed lambda
calculus implementations as its author hoped, this project applies similar ideas
to the dependently-typed lambda calculus, where the presence of type-level
computation should lead to larger gains.

- cite:kleeblatt11_strongly_normalizing_stg - STG compiler JITed?
- cite:schilling13_tracing_jit - trace-based interpreter for GHC, a different approach

#+COMMENT: Our solution: Make a quick outline of your approach, pitch your solution

inline:Rephrase In this thesis, I will use the Truffle framework to evaluate how
well are the optimizations provided by the just-in-time compiler GraalVM
suitable to the domain of dependently-typed languages. GraalVM helps to turn
slow interpreter code into efficient machine code by means of /partial evaluation/
cite:wurthinger13_graal. During partial evaluation, specifically the second
Futamura projection cite:latifi19_futamura, an interpreter is specialized
together with the source code of a program, yielding executable code. Parts of
the interpreter could be specialized, some optimized, and some could be left off
entirely. Depending on the quality of the specializer, this may result in
performance gains of several orders of magnitude.

Truffle makes this available to language creators, they only need to create an
interpreter for their language. It also allows such interpreters to take
advantage of GraalVM's /polyglot/ capabilities, and directly interoperate with
other JVM-based languages, their code and values
cite:sipek19_polyglot. Development tooling can also be derived for Truffle
languages rather easily cite:stolpe19_environment. Regardless of whether Truffle
can improve their performance, both of these features would benefit
dependently-typed or experimental languages.

#+COMMENT: Contributions: Sell your solution. Pinpoint your achievements. Be fair and objective.

While this project was originally intended just as a λΠ calculus compiler and an
efficient runtime, it has ended up much larger due to a badly specified
assignment. I also needed to study type theory and type checking and elaboration
algorithms that I have used in this thesis, and which form a large part of
chapters ref:lambda and ref:interpreter.

Starting from basic λ calculus theory and building up to the systems of the
lambda cube, we specify the syntax and semantics of a small language that I
refer to as Montuno (Chapter~ref:lambda). We go through the principles of λ
calculus evaluation, type checking and elaboration, implement an interpreter for
Montuno in a functional style (Chapter ref:interpreter).  In the second part of
the thesis, we evaluate the capabilities offered by Truffle and the
peculiarities of Truffle languages (Chapter ref:truffle), implement an
interpreter for Montuno on the Truffle framework (Chapter ref:jit-interpreter),
and apply various JIT optimizations to it (Chapter ref:optimizations). After
designing and using a set of benchmarks to evaluate the language's performance,
we close with a large list of possible follow-up work (Chapter ref:evaluation).

* Language specification: λΠ calculus with extensions
  :PROPERTIES:
  :CUSTOM_ID: lambda
  :END:
** Introduction
Proof assistants like Agda or Idris are built around a fundamental principle
called the Curry-Howard correspondence that connects type theory and
mathematical logic, demonstrated in Figure ref:ch-logic. In simplified terms it
says that given a language with a self-consistent type system, writing a
well-typed program is equivalent to proving its correctness
cite:baez10_rosetta. It is often shown on the correspondence between natural
deduction and the simply-typed λ calculus, as in Figure ref:ch-deduction. Proof
assistants often have a small core language around which they are build:
e.g. Coq is build around the Calculus of Inductive Constructions, which is a
higher-order typed λ calculus.

-- Dependent languages are not only proof checkers but also general programming
languages (Cayenne, Epigram), and Eisenberg's Stitch cite:eisenberg20_stitch is
an interesting tutorial of a dependent interpreter of dependent languages

#+label: idris-vect
#+caption: Vectors with explicit length in the type, source: the Idris base library
#+begin_src idris
  data Vect : (len : Nat) -> (elem : Type) -> Type where
    Nil  : Vect Z elem
    (::) : (x : elem) -> (xs : Vect len elem) -> Vect (S len) elem

  -- Definitions elided

  head : Vect (S len) elem -> elem
  index : Fin len -> Vect len elem -> elem

  (++) : (xs : Vect m elem) -> (ys : Vect n elem) -> Vect (m + n) elem
  proofConcatLength
    : {m, n : Nat} -> {A : Type} -> (xs : Vect n A) -> (ys : Vect m A)
      -> length (xs ++ ys) = length xs + length ys
#+end_src

#+label: ch-logic
#+CAPTION: Curry-Howard correspondence between mathematical logic and type theory
#+ATTR_LaTeX: :options [!htb]
#+begin_figure latex
\begin{center}
\begin{tabular}{c|c}
Mathematical logic & Type theory \\\hline\\[-1em]
\shortstack{$⊤$ \\ true} &
\shortstack{$()$ \\ unit type} \\[7pt]
\shortstack{$⊥$ \\ false} &
\shortstack{$∅$ \\ empty type} \\[7pt]
\shortstack{$p ∧ q$ \\ conjunction} &
\shortstack{$a × b$ \\ sum type} \\[7pt]
\shortstack{$p ∨ q$ \\ disjunction} &
\shortstack{$a + b$ \\ product type} \\[7pt]
\shortstack{$p ⇒ q$ \\ implication} &
\shortstack{$a → b$, $a^b$ \\ exponential (function) type} \\[7pt]
\shortstack{$∀x ∈ A, p$ \\ universal quantification} &
\shortstack{$Π_{x : A}B(x)$ \\ dependent product type} \\[7pt]
\shortstack{$∃x ∈ A, p$ \\ existential quantification} &
\shortstack{$Σ_{x : A}B(x)$ \\ dependent sum type} \\[7pt]
\end{tabular}
\end{center}
#+end_figure

#+label: ch-deduction
#+CAPTION: Curry-Howard correspondence between natural deduction and λ→-calculus
#+ATTR_LaTeX: :options [!htb]
#+begin_figure latex
\begin{center}
\begin{tabular}{c|c}
Natural deduction & λ→ calculus \\\hline\\[-1em]
\shortstack{
\AxiomC{}
\UnaryInfC{$Γ₁, α, Γ₂ ⊢ α$}
\DisplayProof \\ axiom} &
\shortstack{
\AxiomC{}
\UnaryInfC{$Γ₁, x : α, Γ₂ ⊢ x : α$}
\DisplayProof \\ variable} \\[7pt]

\shortstack{
\AxiomC{$Γ, α ⊢ β$}
\UnaryInfC{$Γ ⊢ α → β$}
\DisplayProof \\ implication introduction} &
\shortstack{
\AxiomC{$Γ, x : α ⊢ t : β$}
\UnaryInfC{$Γ ⊢ λx. t: α → β$}
\DisplayProof \\ abstraction} \\[7pt]

\shortstack{
\AxiomC{$Γ ⊢ α → β$}
\AxiomC{$Γ ⊢ α$}
\BinaryInfC{$Γ ⊢ β$}
\DisplayProof \\ modus ponens} &
\shortstack{
\AxiomC{$Γ ⊢ t : α → β$}
\AxiomC{$Γ ⊢ u : α$}
\BinaryInfC{$Γ ⊢ t u : β$}
\DisplayProof \\ application} \\[7pt]
\end{tabular}
\end{center}
#+end_figure

Compared to the type systems in languages like Java, systems like this enable
encoding much more information in the types. We can see the usual example of a
list with a known length in Listing ref:idris-vect: the type ~Vect~ has two
parameters, one is the length of the list as a Peano number, the other is the type of its
elements. Using this we can define safe indexing operators like ~head~, which
is only applicable to non-empty lists, or ~index~, where the index must be given as a
finite number between zero and the length of the list. List concatenation uses
arithmetic on the type level, and it is possible to explicitly prove that
concatenation preserves list length.

On the other hand, these languages are often restricted in some ways. General
Turing-complete languages allow non-terminating programs which leads to a
inconsistent type system, and so proof assistants use various ways of keeping
the logic sound and consistent. Idris, for example, is requires that functions
are total. It uses a termination checker, checking that recursive functions use
only structural or primitive recursion, in order to ensure that type-checking
stays decidable.

Our goal in this chapter is to specify a minimal dependently-typed core
language, so that we can create an interpreter for is, and write some
computationally interesting programs in it. To do that, we first need to cover
the concepts that we will need in later parts of the thesis.

The goal of this chapter is to introduce the concepts relevant to the language
specification, and not to create a complete reference to the large field of type
theory.

** Languages
*** λ-calculus
We will start from the untyped lambda calculus, as it is the language that all
following ones will build upon. Introduced in the 1930s by Alonzo Church as a
model of computation. It is a very simple language that consists of only three
constructions: abstraction, application, and variables, written as in Figure
ref:untyped.

#+label: untyped
#+CAPTION: λ-calculus written in Church and de Bruijn notation
#+ATTR_LaTeX: :options [h]
#+begin_figure latex
\captionsetup[subfigure]{aboveskip=-1pt}
\begin{subfigure}[t]{.5\textwidth}\centering
  \[\begin{array}{ccll}
  e & ::= & v   & \text{variable} \\
    & |   & M~N & \text{application} \\
    & |   & λv.~M & \text{abstraction}
  \end{array}\]
  \caption{Standard (Church) notation}
\end{subfigure}
\begin{subfigure}[t]{.5\textwidth}\centering
  \[\begin{array}{ccll}
  e & ::= & v     \\
    & |   & (N)~M \\
    & |   & [v]~M
  \end{array}\]
  \caption{De Bruijn notation}
\end{subfigure}
#+end_figure

**** β-reduction
The λ-abstraction $λx.~t$ represents a program that, when applied to the
expression $x$, returns the term $t$. For example, the expression $(λx.x x) t$
produces the expression $t t$. This step, applying a λ-abstraction to an term,
is called /β-reduction/, and it is the basic /rewrite rule/ of λ-calculus. Another
way of saying that is that the x is assigned/replaced with the expression T, and
it is written as the substitution $M[x≔T]$

#+LATEX: \[ (λx. t) u ⟶_β t[x≔u] \]

**** α-conversion
We however need to ensure that the variables do not overlap. If they do, we need
to rename them. (Free variables, bound variables) This is called /α-conversion/ or
α renaming. (The expression $t[s/x]$ denotes the result of taking a term $t$ and
replacing every free occurence of the variable $x$ by the term $s$.)

#+LATEX: \[ (λx. t) ⟶_α (λy. t[x≔y]) \]

**** η-conversion
Reduction of a term in the form of $λx.f x$ to $f$ --that is, a function that receives
an argument and directly applies $T$ to it--is called /η-reduction/. The opposite
rewriting, from $f$ to $λx.f x$ is $\bar{η}\text{-expansion}$, and because the
rewriting works in both ways, it is also called the /η-conversion/.

#+LATEX: \[ λx.f x ⟶_η f\]  \[ f ⟶_{\bar{η}} λx.f x \]

**** δ-reduction
We'll skip a bit, using terms that we will only define later, but in order to
have all the basic computation rules in one place... There are three other types
of rewriting rules that we will use. /δ-reduction/ is the replacement of a
constant with its definition.

#+LATEX: \[ id t ⟶_δ (λx.x) t\]

**** ζ-reduction
For local variables, the same process is called the /ζ-reduction/:

#+LATEX: \[ let id = λx.x in id t ⟶_ζ (λx.x) t \]

**** ι-reduction
We will also use different type of objects than just functions. Applying a
function that extracts a value from an object is called the /ι-reduction/. In this
example, the object is a pair of values, and the function $π₁$ is a projection
that extracts the first value of the pair.

#+LATEX: \[ π₁ (a, b) ⟶_ι a \]

**** Normal form
By repeatedly βδιζ-reducing an expression--applying functions to their
arguments, replacing constants and local variables with their definitions,
evaluating objects, and α-renaming variables if necessary, we get a β-normal
form, or just /normal form/ for short. This normal form is unique (up to
α-conversion), according to the Church-Rossier theorem.

#+LATEX: \missingfigure{Show derivation steps, when to β, δ, ζ, ι?}

**** Other normal forms
There are also other normal forms, they all have something to do with unapplied
functions. If we have an expression and repeatedly use only the β-reduction, we
end up with a function, or a variable applied to some free variables. These
other normal forms specify what happens in such a "stuck" case. In Figure
ref:normal-forms $e$ is an arbitrary λ-term, $E$ is a term in the relevant
normal form. cite:sestoft02_reduction There are /normalization strategies/ that
specify the order in which sub-expressions are reduced,

#+LABEL: normal-forms
#+CAPTION: Normal forms in λ-calculus
#+begin_figure latex
\captionsetup{aboveskip=-1pt}
\begin{center}
\begin{tabular}{cll}
            & \multicolumn{2}{c}{Reduce under abstraction} \\\cline{2-3}
Reduce args & \multicolumn{1}{c}{\textbf{Yes}} & \multicolumn{1}{c}{\textbf{No}} \\\hline
\multicolumn{1}{c|}{\multirow{2}{*}{\textbf{Yes}}}
                       & $E ≔ λx.E | x E₁...Eₙ$ & $E ≔ λx.e | x E₁...Eₙ$ \\
\multicolumn{1}{c|}{}  & Normal form            & Weak normal form \\[2pt]
\multicolumn{1}{c|}{\multirow{2}{*}{\textbf{No}}}
                       & $E ≔ λx.E | x e₁...eₙ$ & $E ≔ λx.e | x e₁...eₙ$ \\
\multicolumn{1}{c|}{}  & Head normal form      & Weak head normal form \\
\end{tabular}
\end{center}
#+end_figure

An important property of a model of computation (like the λ-calculus) is
termination, the question of whether there are expressions for which computation
does not stop, i.e. rewriting a term does not produce a single unique normal
form. For some expressions, this may depend on the selected rewriting strategy.

**** Strong normalization
Given a well-typed term $Γ ⊢ a:A$, if there does not exist an infinite sequence of reductions
$a ⟶_{β} a' ⟶_{β} a''⟶_{β} ⋯$, then we call such a system /strongly normalizing/.

The untyped λ-calculus is not a strongly normalizing system, though, and here
are expressions that do not have a normal form; when reduced, they do not get
smaller, they /diverge/. The ω combinator

#+LATEX: \[ω = λx.x~x\]

is one such example that produces an infinite term. Applying it to itself
produces a divergent term whose reduction cannot terminate:

#+LATEX: \[ω~ω ⟶_δ (λx.x x)ω ⟶_β ω~ω\]

Also notable is the fixed-point function, or the Y combinator.

#+LATEX: \[Y = λf.(λx.f(x x)) (λx.f(x x))\]

This is one possible way of encoding general recursion in λ calculus, as it
reduces by applying $f$ to itself:

#+LATEX: \[Y f ⟶_{δβ} f(Y f) ⟶_{δβ} f(f(Y f))  ⟶_{δβ} ...\]

This, as we will see in the following chapter, is impossible to encode in the
typed λ calculus without additional extensions.

As simple as λ-calculus may seem, it is a Turing-complete system, and we can
encode logic, arithmetic, or data structures in it: some examples include /Church
encodings/ of booleans, pairs, or natural numbers (Figure ref:church).

#+LABEL: church
#+CAPTION: Church encoding of various concepts
#+ATTR_LaTeX: :options [!htb]
#+begin_figure latex
\begin{subfigure}[t]{.5\textwidth}\centering
  \[\begin{array}{ccl}
  0 & = & λf.λx.~x \\
  1 & = & λf.λx.~f~x
  \end{array}\]
  \caption{Natural numbers}
\end{subfigure}
\begin{subfigure}[t]{.5\textwidth}\centering
  \[\begin{array}{ccll}
  succ & = & λn.λf.λx.f~(n~f~x) \\
  plus & = & λm.λn.m~succ~n
  \end{array}\]
  \caption{Simple arithmetic}
\end{subfigure}
\begin{subfigure}[t]{.5\textwidth}\centering
  \[\begin{array}{ccll}
  true & = & λx.λy.x \\
  false & = & λx.λy.y \\
  not & = & λp.p~false~true \\
  and & = & λp.λq.p~q~p \\
  ifElse & = & λp.λa.λb.p~a~b
  \end{array}\]
  \caption{Logic}
\end{subfigure}
\begin{subfigure}[t]{.5\textwidth}\centering
  \[\begin{array}{ccll}
  cons & = & λf.λx.λy.f~x~y \\
  fst & = & λp.p~true \\
  snd & = & λp.p~false \\
  \end{array}\]
  \caption{Pairs}
\end{subfigure}
#+end_figure

*** λ→-calculus
It is often useful, though, to describe the kinds of objects we work
with. Already, in the previous examples we could see that reading such code can
get confusing: a boolean is a function of two parameters, a pair is a function
of three arguments, but the first one should be a boolean and the other two
contents of the pair.

The untyped λ-calculus defines a general model of computation based on functions
and function application. Now we will restrict this model using types that
describe the values that can be computed with.

The simply typed λ calculus, also written λ→ as "→" is the connector used in
types, introduces the concept of types. We have a set of basic types that are
connected into terms using →, and type annotation or assignment $x : A$. We now
have two languages: the language of terms, and the language of types. These
languages are connected by a /type judgment/, or /type assignment/ $x : T$ that
asserts that the term $x$ has the type $T$ cite:guallart15_overview_types.

**** Church- and Curry-style
There are two ways of specifying the simply-typed λ calculus: λ→-Church, and
λ→-Curry. Church-style is also called system of typed terms, or the explicitly
typed λ calculus as we have terms that include type information, and we say

#+LATEX: \[λx : A.x : A → A,\]

or using parentheses to clarify the precedence

#+LATEX: \[λ(x : A).x : (A → A).\]

Curry-style is also called the system of typed assignment, or the implicitly
type λ calculus as we assign types to untyped λ-terms that do not carry type
information by themselves, and we say $λx.x : A → A$. cite:barendregt92_typed.

There are systems that are not expressible in Curry-style, and vice versa.
Curry-style is interesting for programming, we want to omit type information;
and we will see how to manipulate programs specified in this way in following
chapters re:type elaboration. We will use Church-style in this chapter, but our
language will be Curry-style, so that we incorporate elaboration into the
interpreter.

**** Well-typed terms
Before we only needed evaluation rules to fully specify the system, but now we
will also need typing rules. We will also need to distinguish /well-formed terms/
that are syntactically valid from /well-typed terms/ that are well-formed and also
obey typing rules of the system. We will call well-formed terms that we do not
yet know if they are well-typed /preterms/, or terms of /presyntax/.

There are some basis algorithms of type theory, in brief:
- given a pre-term and a type, /type checking/ verifies whether the term can be
  assigned the type.
- given just a pre-term and no type, /type inference/ computes the type of an expression
- and finally /type elaboration/ is the process of converting a partially
  specified pre-term into a complete, well-typed term cite:ferreira14_bidi.

#+label: simple-syntax
#+CAPTION: λ→-calculus syntax
#+begin_figure latex
\[\begin{array}{ccll}
e & & & (terms) \\
  & ≔ & v     & \text{variable} \\
  & | & M~N   & \text{application} \\
  & | & λx.~t & \text{abstraction} \\
  & | & x:τ   & \text{annotation} \\[5pt]
τ & & & (types) \\
  & ≔ & β      & \text{base types} \\
  & | & τ → τ' & \text{composite type} \\[5pt]
Γ & & & (typing context) \\
  & ≔ & ∅     & \text{empty context} \\
  & | & Γ,x:τ & \text{type judgement} \\[5pt]
v & & & (values) \\
  & ≔ & λx. t & \text{closure} \\[5pt]
\end{array}\]
#+end_figure

**** Types and context
The complete syntax of the λ→-calculus is in Figure ref:simple-syntax.
Reduction operations are the same as in the untyped lambda calculus, but we will
need to add the language of types to the previously specified language of
terms. This language consists of a set of /base types/ which can consist of
e.g. natural numbers or booleans, and /composite types/, which describe functions
between them. We also need a way to store the types of terms that are known, a
typing /context/, which consists of a list of /type judgments/ in the form $x:T$,
which associate variables to their types.

#+label: simple-types
#+CAPTION: λ→-calculus typing rules
#+begin_figure latex
\begin{prooftree}
\AxiomC{$x : A ∈ Γ$}
\RightLabel{\textsc{(Var)}}
\UnaryInfC{$Γ ⊢ x : A$}
\end{prooftree}
\begin{prooftree}
\AxiomC{$Γ ⊢ f:A→B$}
\AxiomC{$Γ ⊢ a:A$}
\RightLabel{\textsc{(App)}}
\BinaryInfC{$Γ ⊢ fa : B$}
\end{prooftree}
\begin{prooftree}
\AxiomC{$Γ,x : A ⊢ b:B$}
\RightLabel{\textsc{(Abs)}}
\UnaryInfC{$Γ ⊢ λx : A. b : A→B$}
\end{prooftree}
#+end_figure

**** Typing rules
The simply-typed λ-calculus can be completely specified by the typing rules in
Figure ref:simple-types cite:pierce02_types. These rules are read similar to
logic proof trees: as an example, the rule *App* reads as "if we can infer $f$
with the type $A→B$ and $a$ with the type $A$ from the context Γ, then we can
also infer that function application $f a$ has the type $B$". Given these rules
and the formula

#+LATEX: \[λa:A.λb:B.a : A→B→A\]

we can also produce a derivation tree that looks similar to logic proofs and, as
mentioned before, its semantics may be exactly the same as
inline:WhatFormulaIsThis [...] as per the Curry-Howard equivalence.

#+begin_export latex
\begin{prooftree}
\AxiomC{}
\UnaryInfC{$a:A, b:B ⊢ a:A$}
\UnaryInfC{$a:A ⊢ λb:B. a : B→A$}
\UnaryInfC{$⊢ λa:A. λb:B. a : A→B→A$}
\end{prooftree}
#+end_export

We briefly mentioned the problem of termination in the previous section; the
simply-typed λ-calculus is strongly normalizing, meaning that all well-typed
terms have a unique normal form. In other words, there is no way of writing a
well-typed divergent term; the Y combinator is impossible to type in λ→ and any
of the systems in the following chapter cite:bove08_atwork.

*** λ-cube
The λ→-calculus restricts the (types of functions that can be used); types are
static and descriptive. When evaluating a well-typed term, the types can be
erased altogether without any effect on the computation. In other words, terms
can only depend on other terms. This can be generalized in three ways.

Generalizations of the λ→ calculus can be organized into a cube called the λ
cube cite:barendregt92_typed (Figure ref:cube). The three dimensions of the cube
each represent a new type of dependency. In λ→ only terms depend on terms, but
there are also three other combinations: types depending on types ($(□,□)$ in
the figure, higher-order types, also called type operators), terms depending on
types ($(□,⋆)$ in the figure, also /polymorphism), and terms depending on types
($(⋆,□)$ in the figure, also /dependent types/).

#+label: cube
#+CAPTION: Barendregt cube (also λ-cube)
#+begin_figure latex
\begin{tikzpicture}
\matrix (m) [matrix of math nodes,
row sep=3em, column sep=3em,
text height=1.5ex,
text depth=0.25ex]{
   & λω             &     & λΠω            \\
λ2 &                & λΠ2 &                \\
   & λ\underline{ω} &     & λΠ\underline{ω}\\
λ→ &                & λΠ  \\
};
\path[-{Latex[length=2.5mm, width=1.5mm]}]
(m-1-2) edge (m-1-4)
(m-2-1) edge (m-2-3) edge node[fill=white,pos=0.4]{$(□,□)$} (m-1-2)
(m-3-2) edge (m-1-2) edge (m-3-4)
(m-4-1) edge node[fill=white]{$(□,⋆)$} (m-2-1)
(m-4-1) edge (m-3-2)
(m-4-1) edge node[fill=white]{$(⋆,□)$} (m-4-3)
(m-3-4) edge (m-1-4)
(m-2-3) edge (m-1-4)
(m-4-3) edge (m-3-4) edge (m-2-3);
\end{tikzpicture}
#+end_figure

To formally describe the cube, we will need to introduce the notion of sorts. In
brief,

#+LATEX: \[t : T : ⋆ : □,\]

where $t$ is a term, $T$ a type, $⋆$ is a kind, the sort of types, and $□$ is
the sort of kinds, The type of terms is a type, the type of types is a kind, the
type of kinds is a sort. As with types, sorts can be connected using arrows,
e.g. $(⋆→⋆)→⋆$.

To contrast the syntaxes of the following languages, the syntax of λ→ is here:

#+begin_export latex
\[\begin{array}{ccl}
types & ≔ & T | A → B' \\
terms & ≔ & v | λx:A.t | a b \\
values & ≔ & λx:A.t \\
\end{array}\]
#+end_export

**** λ\underline{ω}-calculus
Higher-order type operators, the dependency $(□, □)$, simply generalize the
concepts of functions to the type level, adding λ-abstractions and applications
to the language of types.

#+begin_export latex
\[\begin{array}{ccl}
types & ≔ & T | A → B | ΛA.B(a) | A B \\
terms & ≔ & v | λx:A.t | a b \\
values & ≔ & λx:A.t \\
\end{array}\]
#+end_export

**** λ2-calculus
Polymorphism adds polymorphic types to the language of types: $∀X:k.A(X)$, and
type abstractions (Λ-abstractions) and applications to the language of
terms. This system is also called System F, and it is equivalent to
propositional logic.

#+begin_export latex
\[\begin{array}{ccl}
types & ≔ & T | A → B | ∀A.B \\
terms & ≔ & v | λx:A.t | a b | ΛA.t \\
values & ≔ & λx:A.t | ΛA.t\\
\end{array}\]
#+end_export

**** λΠ-calculus
Dependence allows the function type to depend on its arguments, adding the
Π-type: $Πa:A.B(a)$. This system is well-studied as the Logical Framework (LF).

#+begin_export latex
\[\begin{array}{ccl}
types & ≔ & T | A → B | Πa:A.B(a) \\
terms & ≔ & v | λx:A.b | a b | Πa:A.b \\
values & ≔ & λx:A.b | Πx:A.b\\
\end{array}\]
#+end_export

**** Pure type system
These systems can all be described by one set of typing rules instantiated with
a triple $(S, A, R)$. Given the set of sorts $S=\{⋆,□\}$ we can define relations
$A$ and $R$ where, for example, $A=\{(⋆,□)\}$ is translated to the axiom $⊢⋆:□$
by the rule *Start*, and $R=\{(⋆,□)\}$[fn:9] means that a kind can depend on a
type using the rule *Product*.

#+begin_export latex
\[\begin{array}{ccll}
S & ≔ & \{⋆,□\} & set of sorts \\
A & ⊆ & S×S   & set of axioms \\
R & ⊆ & S×S×S & set of rules
\end{array}\]
#+end_export

The typing rules in Figure ref:coc-rules apply to all of the above-mentioned
type systems, if we consider that the function arrow $A→$ is exactly equivalent
to the Π-type $Πa:A.B(a)$ if the variable $a$ is not used in the expression
$B(a)$. The set $R$ exactly corresponds to the dimensions of the λ-cube, so
instantiating this type system with $R=\{(⋆,⋆)\}$ would produce the
λ→-calculus, whereas including all of the dependencies $R=\{(⋆,⋆), (□,⋆),(⋆,□),
(□,□)\}$ produces the λΠω-calculus.

#+label:coc-rules
#+caption:Typing rules of a pure type system
#+ATTR_LaTeX: :options [!htb]
#+begin_figure latex
\centering
\begin{tabular}{cl}
\AxiomC{}
\RightLabel{$(s₁,s₂)∈A$}
\UnaryInfC{$⊢ s₁:s₂$}
\DisplayProof & \textsc{(Start)} \\[7pt]
\AxiomC{$Γ ⊢ A:s$}
\RightLabel{$s∈S$}
\UnaryInfC{$Γ,x:A ⊢ x:A$}
\DisplayProof & \textsc{(Var)} \\[13pt]
\AxiomC{$Γ ⊢ x : A$}
\AxiomC{$Γ ⊢ B : s$}
\RightLabel{$s∈S$}
\BinaryInfC{$Γ,y:B ⊢ x:A$}
\DisplayProof & \textsc{(Weaken)} \\[13pt]
\AxiomC{$Γ ⊢ f:Π_{x:A}B(x)$}
\AxiomC{$Γ ⊢ a:A$}
\BinaryInfC{$Γ ⊢ fa : B[x≔a]$}
\DisplayProof & \textsc{(App)} \\[13pt]
\AxiomC{$Γ,x : A ⊢ b:B$}
\AxiomC{$Γ ⊢ Π_{x:A}B(x) : s$}
\RightLabel{$s∈S$}
\BinaryInfC{$Γ ⊢ (λx : A. b) : Π_{x:A}B(x)$}
\DisplayProof & \textsc{(Abs)} \\[13pt]
\AxiomC{$Γ ⊢ A:s₁$}
\AxiomC{$Γ,x:A ⊢ B:s₂$}
\RightLabel{$(s₁,s₂,s₃)∈R$}
\BinaryInfC{$Γ ⊢ Π_{x:A}B(x) : s₃$}
\DisplayProof & \textsc{(Product)} \\[13pt]
\AxiomC{$Γ ⊢ a:A$}
\AxiomC{$Γ ⊢ A':s$}
\AxiomC{$A ⟶_β A'$}
\RightLabel{$s∈S$}
\TrinaryInfC{$Γ ⊢ a:A'$}
\DisplayProof & \textsc{(Conv)} \\[5pt]
\end{tabular}
#+end_figure

**** Universes
This can be generalized even more. Instantiating this system with an infinite
set of sorts $S=\{Type₀,Type₁,...\}$ instead of the set $S=\{⋆,□\}$ and setting
$A=\{(Type₀, Type₁), (Type₁,Type₂), ...\}$ leads to an infinite hierarchy of
/type universes/, and is in fact an interesting topic in the field of type
theory. Proof assistants commonly use such a hierarchy cite:bove08_atwork.

**** Type in Type
Going the other way around, simplifying $S$ to $S=\{⋆\}$ and setting
$A=\{(⋆,⋆)\}$, leads to an inconsistent logic system called λ⋆, also called a
system with a /Type in Type/ rule. This leads to paradoxes similar to the Russel's
paradox in set theory. [[inline:Maybe show Girard's paradox?]]

In the pedagogic implementations of dependently-typed λ calculi from which I
took inspiration, though, this was simply acknowledged, as using separate
universes introduces complexity but for many purposes the distinction is not as
important.

For the goal of this thesis--testing the characteristics of a runtime
system--the distinction is unimportant. In the rest of the text we will use the
inconsistent λ⋆-calculus, but with all the constructs mentioned in the preceding
type systems. We will now formally define these constructs, together with
several extensions to this system that will be useful in the context of
just-in-time compilation using Truffle, e.g., coproduct types, booleans, natural
numbers.

Proof assistants and other dependently-typed programming languages use systems
based on λΠω-calculus, which is called the Calculus of Constructions. They add
more extensions like induction or subtyping is a common one (more specific
function is a subtype of a general one $∀x.x→x$ vs $Nat→Nat$, object oriented
inheritance, refinement). Many of these are irrelevant to the goals of this
thesis, and we will discuss only some of them in the following section.

** Types
While it is possible to derive any types using only three constructs: Π-types
(dependent product), Σ-types (dependent sum), and $W$-types (inductive types),
that we haven't seen so far; we will define specific /"wired-in"/ types instead,
as they are more straightforward to both use and implement.

We will specify syntax and semantics of each type at the same time. For syntax,
we will define the terms and values, for semantics we will use four parts:
- type formation, a way to construct new types;
- term introduction (constructors), ways to construct terms of these types;
- term elimination (destructors), ways to use them to construct other terms;
- computation rules (ι-reduction rules), what happens when an introduced term is eliminated

The algorithms to normalize and type-check these terms will only be mentioned in
the following chapter, in this section we will solely focus on the syntax and semantics.

- cite:juan20_unif_thesis - well described contexts + language specification -
  can I take as inspiration?

*** Π-types
As mentioned above, the Π-type $Πa:A.B$, also called the /dependent product type/
or the /dependent function type/, is a generalization of the function type $A→B$.
Where the function type simply asserts that its corresponding function will
receive a value of a certain type as its argument, the Π-type makes the value
available in the rest of the type.

#+label: type-pi
#+CAPTION: Π-type semantics
#+ATTR_LaTeX: :options [!htb]
#+begin_figure latex
\centering
\begin{tabular}{cc}
\multicolumn{2}{c}{%
\AxiomC{$Γ ⊢ A:⋆$}
\AxiomC{$Γ, x:A ⊢ B:⋆$}
\RightLabel{\textbf{(Type-Pi)}}
\BinaryInfC{$Γ ⊢ Πx:A.B$}
\DisplayProof
} \\[15pt]
\AxiomC{$Γ,a:A ⊢ b:B$}
\RightLabel{\textbf{(Intro-Pi)}}
\UnaryInfC{$Γ ⊢ λx.b : Πx:A.B$}
\DisplayProof
&
\AxiomC{$Γ ⊢ f : Πx:A.B$}
\AxiomC{$Γ ⊢ a:A$}
\RightLabel{\textbf{(Elim-Pi)}}
\BinaryInfC{$Γ ⊢ f a : B[x≔a]$}
\DisplayProof
\\[15pt]
\multicolumn{2}{c}{%
\AxiomC{$Γ, a:A ⊢ b:B$}
\AxiomC{$Γ ⊢ a:A$}
\RightLabel{\textbf{(Eval-Pi)}}
\BinaryInfC{$Γ ⊢ (λx:A.b)a ⟶_β b[x≔a]$}
\DisplayProof
}
\end{tabular}
#+end_figure

While a very common example of a Π-type is the length-indexed vector
$Π(n:ℕ). Vec(ℝ, n)$, it is also possible to define a function with a /"dynamic"/
number of arguments like in the following listing. It is a powerful language feature
also for its programming uses, as it makes it possible to e.g. implement a
well-typed ~printf~ function that, when called as ~printf "\%d\%d"~, produces a
function $Nat → Nat → String$.

#+begin_export latex
\[\begin{array}{rcl}
foo & : & Π(b:Bool). if b then (Nat→Nat) else Nat \\
foo & = & Π(b:Bool). if b then (λx. x+1) else 0 \\[3pt]
foo true 0 ⟶_{βδ} 1 && foo false ⟶_{βδ} 0
\end{array}\]
#+end_export

**** Implicit arguments
The type-checker can infer many type arguments. Agda adds the concept of
implicit function arguments cite:bove08_atwork to ease the programmer's work and
mark inferrable type arguments in a function's type signature. Such arguments
can be specified when calling a function using a special syntax, but they are
not required cite:kovacs20_implicit. We will do the same, and as such we will
split the syntax of a Π-type back into three separate constructs.

#+label: syntax-pi
#+CAPTION: Π-type syntax
#+begin_figure latex
\[\begin{array}{ccll}
term & ≔ & a → b | (a:A)→b | \{a:A\}→b & \text{(abstraction)} \\
     & | & f a | f \{a\} | ⋯           & \text{(application)} \\
value & ≔ & Πa:A.b | ⋯
\end{array}\]
#+end_figure

In the plain /function type/ $A→B$ is simple to type but does not bind the value
of provided as the argument $A$. The /explicit Π-type/ $(a:A)→B$ binds the value
$a$ and makes it available to use inside $B$, and the /implicit Π-type/
$\{a:A\}→B$ marks the argument as one that type elaboration should be able to
infer from the surrounding context. The following is an example of the implicit
argument syntax, a polymorphic function $id$.

#+begin_export latex
\[\begin{array}{rclcl}
id         & : & \{A:⋆\}→A→A   & ≔ &          Π(x:A).x \\
id \{Nat\} & : & Nat→Nat & ⟶_{βδ} & λ(x:Nat).x \\
id 1       & : & Nat     & ⟶_{βδ} & 1
\end{array}\]
#+end_export

*** Σ-types
The Σ-type is also called the /dependent pair type/, or alternatively the
dependent tuple, product, or even sum type generalization of the coproduct type
Allows datatypes/records, Cat + Functor?
!!! $∃x:A,P(x) ≡ Σ_{x:A}B(x)$, and also $(a, b) : Σ_{x:A}. B(x)$
cite:abel11_sigma_unif

- cite:altenkirch08_pisigma, cite:altenkirch10_pisigma - "I had discovered the
  ΠΣ paper when finishing my thesis: too late, unfortunately"

#+label: type-sigma
#+CAPTION: Σ-type semantics
#+begin_figure latex
\centering
\begin{tabular}{cc}
\multicolumn{2}{c}{%
\AxiomC{$Γ ⊢ A : ⋆$}
\AxiomC{$Γ, x : A ⊢ B : ⋆$}
\RightLabel{\textbf{(Type-Sigma)}}
\BinaryInfC{$Γ ⊢ Σ_{x : A}B : ⋆$}
\DisplayProof
} \\[15pt]
\multicolumn{2}{c}{%
\AxiomC{$Γ ⊢ a : A$}
\AxiomC{$Γ, x : A ⊢ B : ⋆$}
\AxiomC{$Γ ⊢ b : B[x ≔ a]$}
\RightLabel{\textbf{(Intro-Sigma)}}
\TrinaryInfC{$Γ ⊢ (a, b) : Σ_{x : A}B$}
\DisplayProof
} \\[15pt]
\AxiomC{$Γ ⊢ p : Σ_{x : A}B$}
\RightLabel{\textbf{(Elim-Sigma1)}}
\UnaryInfC{$Γ ⊢ π₁ p : A$}
\DisplayProof &
\AxiomC{$Γ ⊢ p : Σ_{x : A}B$}
\RightLabel{\textbf{(Elim-Sigma2)}}
\UnaryInfC{$Γ ⊢ π₂ p : B[x ≔ fst p]$}
\DisplayProof \\[15pt]
\multicolumn{2}{c}{%
\AxiomC{$Γ ⊢ a : A$}
\AxiomC{$Γ, x : A ⊢ B : ⋆$}
\AxiomC{$Γ ⊢ b : B[x ≔ a]$}
\RightLabel{\textbf{(Eval-Sigma1)}}
\TrinaryInfC{$Γ ⊢ π₁ (a, b) ⟶_ι a : A$}
\DisplayProof
} \\[15pt]
\multicolumn{2}{c}{%
\AxiomC{$Γ ⊢ a : A$}
\AxiomC{$Γ, x : A ⊢ B : ⋆$}
\AxiomC{$Γ ⊢ b : B[x ≔ a]$}
\RightLabel{\textbf{(Eval-Sigma2)}}
\TrinaryInfC{$Γ ⊢ π₂ (a, b) ⟶_ι b : B$}
\DisplayProof
}
\end{tabular}
#+end_figure

**** Pair
As with the function type, given the expression $Σ(a:A).B(a)$, if $a$ does not
occur in the expression $B(a)$, then it is the non-dependent product type, pr
the pair. Like the function type $A→B$ that is the simplification of the type
$Π(a:A).B$, the pair type $A×B$ is a simplification of the type $Σ(a:A).B$.  The
pair type is useful to express an isomorphism from general programming between
[...]

#+begin_export latex
\[\begin{array}{rclrl}
A × B → C & ⇔ & A → B → C \\
    curry & ≔ & λ(f:A×B→C). & λ(x:A).λ(y:B). & f (x,y) \\
  uncurry & ≔ & λ(f:A→B→C). &      λ(x:A×B). & f (π₁ x) (π₂ y)
\end{array}\]
#+end_export

**** Tuple
The n-tuple is a generalization of the pair, an a non-dependent set of
values. otherwise expressible as a set of nested pairs, $\{t_i^{i∈1..n}\}$

**** Record
Records same as tuples, only with \{l_i=t_i\}, \{l_i:T_i\}
Record elimination can be via a pattern-match construct in programming languages
$let \{x,y\}=record in ⋯$, which we will not implement.

#+label: type-record
#+CAPTION: Record semantics
#+begin_figure latex
\centering
\begin{tabular}{cc}
\multicolumn{2}{c}{%
\AxiomC{$∀i∈\{1..n\} Γ ⊢ Tᵢ:⋆$}
\RightLabel{\textbf{(Type-Rec)}}
\UnaryInfC{$Γ ⊢ \{lᵢ:Tᵢ^{i∈\{1..n\}}\}:⋆$}
\DisplayProof } \\[15pt]
\multicolumn{2}{c}{%
\AxiomC{$∀i∈\{1..n\} Γ ⊢ tᵢ : Tᵢ$}
\RightLabel{\textbf{(Intro-Rec)}}
\UnaryInfC{$Γ ⊢ \{lᵢ=tᵢ^{i∈\{1..n\}}\} : \{lᵢ:Tᵢ^{i∈\{1..n\}}\}$}
\DisplayProof } \\[15pt]
\multicolumn{2}{c}{%
\AxiomC{$Γ ⊢ t : \{lᵢ:Tᵢ^{i∈\{1..n\}}\}$}
\RightLabel{\textbf{(Elim-Rec)}}
\UnaryInfC{$Γ ⊢ t.lᵢ : Tᵢ$}
\DisplayProof } \\[15pt]
\multicolumn{2}{c}{%
\AxiomC{$∀i∈\{1..n\} Γ ⊢ tᵢ : Tᵢ$}
\AxiomC{$Γ ⊢ t : \{lᵢ:Tᵢ^{i∈\{1..n\}}\}$}
\RightLabel{\textbf{(Eval-Rec)}}
\BinaryInfC{$Γ ⊢ \{lᵢ=tᵢ^{i∈\{1..n\}}\}.lᵢ ⟶_ι tᵢ : B$}
\DisplayProof
}
\end{tabular}
#+end_figure

Grammar that unifies all of these concepts: a Σ-type, a pair, an n-tuple, a
named record. A non-dependent n-tuple with corresponding numbered projections
$p.1$.  A dependent sum that doubles as a named record, $(a:A)×B$ is binds the
value $a:A$ in the rest of the type $B$, and on the value-level enables the
projection $obj.a$.

#+label: syntax-sigma
#+CAPTION: Σ-type syntax
#+begin_figure latex
\[\begin{array}{ccll}
term & ≔ & T₁×⋯×Tₙ | (l₁:T₁)×⋯×(lₙ:Tₙ)×T_{n+1} & \text{(types)} \\
     & | & (t₁, ⋯, tₙ) & \text{(constructor)} \\
     & | & t.i^{i∈\{1..n\}} | t.lₙ | ⋯ & \text{(destructors)} \\
value & ≔ & (t₁, ⋯, tₙ) | ⋯
\end{array}\]
#+end_figure

**** Coproduct
Sum types ~ dependent pair with a boolean.
These can be derived from the dependent pair:
the sum type $A+B$ is $Σ(a:Bool).if(a,A,B)$, a variant type $(l₁:A₁)+⋯+(lₙ:Aₙ)$
can be derived from finite naturals $Π(n:ℕ).Fin n$ like
$Π(n:ℕ).Σ(i:Fin n).natElim(i, ⋯)$ in the same way that C-like languages have
tagged union types.

For the purposes of our language, a binary sum type is useful, but inductive
variant types would require more involved constraint checking and so we will
ignore those, only using boolean-indexed sums.

Also, pattern matching, ~case-in~, ...

*** Value types
**** Finite sets
In PTS these are the types *0*, *1*, and *2*, with natural numbers defined using
induction over *2*. The type *0* (with zero inhabitants of the type) is the empty or
void type. Type *1* with a single inhabitant is the unit type, and the type *2* is
the boolean type.

- the language of terms and values is extended with a /constant/

**** Unit
- unit term, type, value
- type formation $Γ ⊢ unit : Unit$

**** Booleans
The type *2* has two inhabitants and can semantically be mapped to the boolean type.

#+label: type-bool
#+CAPTION: \texttt{Bool} semantics
#+begin_figure latex
\centering
\begin{tabular}{cc}
\multicolumn{2}{c}{%
\AxiomC{}
\RightLabel{\textbf{(Type-Nat)}}
\UnaryInfC{$⊢ Bool : ⋆$}
\DisplayProof
} \\[15pt]
\AxiomC{}
\RightLabel{\textbf{(Intro-True)}}
\UnaryInfC{$⊢ true : Bool$}
\DisplayProof &
\AxiomC{}
\RightLabel{\textbf{(Intro-False)}}
\UnaryInfC{$⊢ false : False$}
\DisplayProof
\\[15pt]
\multicolumn{2}{c}{%
\AxiomC{$Γ ⊢ a₁:A$}
\AxiomC{$Γ ⊢ a₂:A$}
\RightLabel{\textbf{(Elim-Bool)}}
\BinaryInfC{$Γ,x:Bool ⊢ if x a₁ a₂ : A$}
\DisplayProof
} \\[15pt]
\AxiomC{$Γ ⊢ a₁:A$}
\AxiomC{$Γ ⊢ a₂:A$}
\RightLabel{\textbf{(Eval-True)}}
\BinaryInfC{$Γ ⊢ if true a₁ a₂ ⟶_ι a₁ : A$}
\DisplayProof &
\AxiomC{$Γ ⊢ a₁:A$}
\AxiomC{$Γ ⊢ a₂:A$}
\RightLabel{\textbf{(Eval-False)}}
\BinaryInfC{$if false a₁ a₂ ⟶_ι a₂ : A$}
\DisplayProof
\end{tabular}
#+end_figure

**** Natural numbers
Infinite set. Also used in System T of Gödel: STLC + naturals + booleans
cite:bove08_atwork With naturals it is still strongly normalizing - introducing
naturals and booleans does not prevent termination

\[elim : Π(A : ℕ → Type).A 0 → (Π(n : ℕ).A n → A (succ n)) → Π(n : ℕ).A n\]
\[elim A z s 0 = z\]
\[elim A z s (suc n) = s n (elim A z s n) \]

\[zeros : Πn : ℕ. Vec ℕ n = natElim (Vec ℕ) nil (λn.λxs. cons 0 xs)\]

#+label: type-nat
#+CAPTION: \texttt{Nat} semantics
#+begin_figure latex
\centering
\begin{tabular}{cc}
\multicolumn{2}{c}{%
\AxiomC{}
\RightLabel{\textbf{(Type-Nat)}}
\UnaryInfC{$⊢ Nat : ⋆$}
\DisplayProof
} \\[15pt]
\AxiomC{}
\RightLabel{\textbf{(Intro-Zero)}}
\UnaryInfC{$⊢ zero : Nat$}
\DisplayProof &
\AxiomC{$Γ ⊢ n : Nat$}
\RightLabel{\textbf{(Intro-Succ)}}
\UnaryInfC{$Γ ⊢ succ n : Nat$}
\DisplayProof
\\[15pt]
\multicolumn{2}{c}{%
\AxiomC{$Γ ⊢ a₁:A$}
\AxiomC{$Γ,n:Nat ⊢ a₂:A$}
\RightLabel{\textbf{(Elim-Nat)}}
\BinaryInfC{$Γ,x:Nat ⊢ natElim x a₁ (λx.a₂)$}
\DisplayProof
} \\[15pt]
\multicolumn{2}{c}{%
\AxiomC{$Γ ⊢ a₁:A$}
\AxiomC{$Γ,n:Nat ⊢ a₂:A$}
\RightLabel{\textbf{(Eval-Zero)}}
\BinaryInfC{$Γ ⊢ natElim zero a₁ (λx.a₂) ⟶_ι a₁ : A$}
\DisplayProof
} \\[15pt]
\multicolumn{2}{c}{%
\AxiomC{$Γ ⊢ a₁:A$}
\AxiomC{$Γ,n:Nat ⊢ a₂:A$}
\AxiomC{$Γ ⊢ n:Nat$}
\RightLabel{\textbf{(Eval-Succ)}}
\TrinaryInfC{$natElim (succ n) a₁ (λx.a₂) ⟶_ι a₂[x≔n] : A$}
\DisplayProof
}
\end{tabular}
#+end_figure

[[todo:Alternatively, a dependent evaluator into Πn:Nat.T(n), ncatlab]]

*** μ-types
Equi-recursive types: generic recursion
Iso-recursive types with fold/unfold operations, and the type $μX.T$ are more
rigorous, but more involved. (p276 Pierce)
We'll use a simpler approach for value-level recursion only, to aid our
benchmarking. A fix function.

ff = λieio:{iseven:Nat→Bool, isodd:Nat→Bool}.
{iseven = λx:Nat.if iszero x then true else ieio.isodd (pred x),
isodd = λx:Nat.if iszero x then false else ieio.iseven (pred x)};

ff : {iseven:Nat→Bool,isodd:Nat→Bool} → {iseven:Nat→Bool, isodd:Nat→Bool}

r = fix ff;
r : {iseven:Nat→Bool, isodd:Nat→Bool}

Fix breaks the consistency of our logic system, but it is useful for testing the
characteristics of our runtime system.

It is possible to use fix to create a recursive local binding (letrec x:T=t in u ≡ let x = fix(λx:T.t) in u)

#+label: type-fix
#+CAPTION: \texttt{fix} semantics
#+begin_figure latex
\centering
\begin{tabular}{cc}
\multicolumn{2}{c}{%
\AxiomC{$Γ ⊢ f:A→A$}
\RightLabel{\textbf{(Type-Fix)}}
\UnaryInfC{$Γ ⊢ fix f : A$}
\DisplayProof
} \\[15pt]
\multicolumn{2}{c}{%
\AxiomC{$Γ,x:A ⊢ t:A$}
\RightLabel{\textbf{(Eval-Fix)}}
\UnaryInfC{$Γ ⊢ fix (λx.t) ⟶_β t[x≔(λx.t)] : A$}
\DisplayProof
}
\end{tabular}
#+end_figure

** Our syntax
Terms, values, context, ... - summarized.

Translated into an EBNF grammar, using the concrete syntax of the tool ANTLR, we
get a grammar 

*** Additional constructs
- primitives are implemented in the compiler, bound in the initial context, so
  are not mentioned here ("Nat", "Bool", ...)
- one additional construct that is a not mentioned above is the hole, which is a
  concept used in type inference
- also, polyglot (eval + type). An alternative way of specifying "eval" would be
  a function with an unknown type (hole), and with explicit Church-style term
  annotations.
- pragma, command

**** Let-in
- let term
- evaluation: let+substitution (E-LetV), β inside let binding (E-Let)
- typing intro: if $Γ⊢t:A$, $Γ,a:A⊢b:B$, then $let a =t in b : B$ (T-Let)

#+label: let-in
#+CAPTION: \texttt{let-in} semantics
#+begin_figure latex
\begin{prooftree}
\AxiomC{$Γ ⊢ a : A$}
\AxiomC{$Γ,x:A ⊢ b : B$}
\RightLabel{\textbf{(Type-Let)}}
\BinaryInfC{$Γ ⊢ \text{let} x=a \text{in} b:B$}
\end{prooftree}
\begin{prooftree}
\AxiomC{$Γ ⊢ v:A$}
\AxiomC{$Γ,x:A ⊢ e:B$}
\RightLabel{\textbf{(Eval-Let)}}
\BinaryInfC{$\text{let} x=v \text{in} e ⟶_ζ e[x≔v]$}
\end{prooftree}
#+end_figure

#+label: grammar
#+caption: Complete EBNF grammar (using ANTLR syntax)
#+begin_src antlr
  FILE
      : STMT (STMTEND STMT)* ;
  STMT
      : "{-#" PRAGMA "#-}"
      | ID ":" EXPR
      | ID ":" EXPR "=" EXPR
      | ID "=" EXPR
      | COMMAND EXPR
      ;
  EXPR
      : "let" ID ":" EXPR "=" EXPR "in" EXPR
      | "λ" LAM_BINDER "." EXPR
      | PI_BINDER+ "→" EXPR
      | ATOM ARG*
      ;
  LAM_BINDER
      : ID | "_"
      | "{" (ID | "_") "}"
      ;
  PI_BINDER
      : ATOM ARG*
      | "(" ID+ ":" EXPR ")"
      | "{" ID+ ":" EXPR "}"
      ;
  ARG
      : ATOM
      | "{" ID ("=" TERM)? "}"
      ;
  ATOM
      : "[" ID "|" FOREIGN "|" TERM "]"
      | EXPR "×" EXPR
      | "(" EXPR ("," EXPR)+ ")"
      | "(" EXPR ")"
      | ID "." ID
      | ID
      | NAT
      | "*"
      | "_"
      ;
  STMTEND : ("\n" | ";")+ ;
  ID : [a-zA-Z] [a-zA-Z0-9] ;
  SKIP : [ \t] | "--" [^\r\n]* | "{-" [^#] .* "-}" ;
  // pragma, command discussed in text
#+end_src

* Language implementation: Montuno
  :PROPERTIES:
  :CUSTOM_ID: interpreter
  :END:
** Introduction
We will first create an interpreter for Montuno as specified in the assignment,
and also because evaluation and elaboration algorithms from the literature are
quite naturally translated to a functional-style program, which is not really
possible in Truffle, the target implementation, where annotated classes are the
main building block.

The Truffle implementation, which we will see in the following chapters, has a
much higher conceptual overhead as we will need to care about low-level
implementation details, e.g. implementing the actual function calls. In this
interpreter, though, we will simply use the features of our host language.

We will use Kotlin as our language of choice, as it is a middle ground between
plain Java and functional JVM-based languages like Scala or Clojure. While the
main target language of Truffle is Java, Kotlin also supports class and object
annotations on which the Truffle DSL is based [fn:1]. Functional style, on the other
hand, makes our implementation of the algorithms simpler and more
concise [fn:6].

The choice of the platform (JVM) and the language (Kotlin) also clarifies the
choice of supporting libraries. In general, we are focused on the algorithmic
part of the implementation, and not on speed or conciseness, which means that we
can simplify our choices by using the most widely used libraries:
- Gradle as the build system,
- JUnit as the testing framework,
- ANTLR as the parser generator,
- JLine as the command-line interface library.

Truffle authors recommend against using many external libraries in the
internals of the interpreter, as the techniques the libraries use may not
work well with Truffle. This means that we need to design our own supporting
data structures based on the primitive structures provided directly by Java.

The overall program flow of our interpreter will not be unusual for
interpreters:
- receive some input from the user, either from a file, or from an interactive
  prompt;
- parse the textual content into an intermediate representation;
- if we are in batch mode, sequentially process the top-level statements,
  accumulating the entries processed so far into a global table of names. Given
  a top-level definition containing a type and a term, we will:
  - /check/ that the type provided has the kind ⋆
  - /infer/ the type of the term
  - /check/ whether the provided and inferred types can be unified
  - /simplify/ both the type and term, and store them into the global name-table.
- if we are in interactive mode, normalize the expression given and print it
  out;
- if we encounter an ~elaborate~ or ~normalize~ command, print out the full or
  normalized form of the expression.

#+label: main-sigs
#+caption: Simplified signatures of the principal functions
#+begin_src kotlin
fun infer(pre: PreTerm): Pair<Term, Val>
fun check(pre: PreTerm, wanted: Val): Term
fun eval(term: Term): Val
fun quote(value: Val): Term
#+end_src

Throughout this process, we will evaluate and quote ~Terms~ from and into
~Values~. The signatures of the most important functions are mentioned in Listing
ref:main-sigs. These are the main parts of our interpreter--and the main
component of evaluation is function application, which is what we will focus on
first.

** Parser
Lexical and syntactic analysis is not the focus of this work, so simply I chose
the most prevalent parsing library in Java-based languages which seems to be
ANTLR [fn:2]. It comes with a large library of languages and protocols from
which to take inspiration [fn:3], so creating a parser for our language was not
hard, despite me only having prior experience with parser combinator libraries
and not parser generators.

ANTLR provides has two recommended ways of consuming the result of parsing using
classical design patterns: a listener and a visitor. I have used neither as they
were needlessly verbose or limiting [fn:4].

I have instead implemented a custom recursive-descent AST transformation that
converts ~ParseContexts~ created by ANTLR into our ~Presyntax~ data type that we can
see in Listing ref:presyntax. This is actually a slightly simplified version
compared to the original as I have omitted the portion that tracks which
position of the input file corresponds to each subtree, which is later used for
type error reporting.

#+label: presyntax
#+caption: The Presyntax data type
#+begin_src kotlin
  sealed class TopLevel
  data class RDecl(val n: String, val type: PreTerm) : TopLevel()
  data class RDefn(val n: String, val type: PreTerm?, val term: PreTerm) : TopLevel()
  data class RTerm(val cmd: Command, val term: PreTerm) : TopLevel()

  sealed class PreTerm

  data class RVar(val n: String) : PreTerm()
  data class RNat(val n: Int) : PreTerm()
  object RU : PreTerm()
  object RHole : PreTerm()

  data class RApp(
      val icit: Icit, val rator: PreTerm, val rand: PreTerm
  ) : PreTerm()
  data class RLam(
      val name: String, val icit: Icit, val body: PreTerm
  ) : PreTerm()
  data class RFun(
      val domain: PreTerm, val codomain: PreTerm
  ) : PreTerm()
  data class RPi(
      val name: String, val icit: Icit, val type: PreTerm, val body: PreTerm,
  ) : PreTerm()
  data class RLet(
      val name: String, val type: PreTerm, val defn: PreTerm, val body: PreTerm,
  ) : PreTerm()
  data class RForeign(
      val lang: String, val eval: String, val type: PreTerm
  ) : PreTerm()
#+end_src

The data structures are represented in a way that Kotlin
recommends [fn:5]--using /data classes/. These are classes whose primary purpose
is to hold data, so called Data Transfer Objects (DTOs), and that have special
language support in Kotlin. We have a ~TopLevel~ class with three children that
represent: definitions that assign a value to a name, optionally with a type;
declarations (sometimes called postulates) that only assign a type to a name;
and commands like ~%normalize~ that we will see in later sections.

Their ancestor is a ~sealed class~ which tells the compiler there will only ever
be the subclasses defined in this module. In particular, this means that in any
pattern match on the type of a ~TopLevel~ object we only ever need to handle three
cases.

The remaining classes generally map to elements from the language syntax as
specified in the previous section: a λ abstraction, function application, Π
abstraction, a ~let~ local binding. The ~RFun~ class is a specialization of the ~RPi~
class that binds an unnamed, non-dependent type argument.

We will not use these classes immediately--only in Chapter ref:elaboration will
we implement a way to convert this pre-syntax into correct-by-construction
~Syntax~ objects which can be evaluated and quoted.

** Representing values
todo:RecapValues

The main requirement for a λ-calculus runtime system is fast function
evaluation, which is where we will start.

Closures as values of the λ-calculus

- cite:gratzer19_modal_types - normal values, closures

[[todo:Three constructs - lam, app, var (1 para)]]

A closure consists of an unapplied function, and an environment that /closes over/
the free variables used in the function.

HOAS is a specific representation of closures, wherein the function is
represented as function in the host language, using the host language's support
for closing over free variables.

While representing functions using HOAS produces very readable code and in some
cases e.g. on GHC produces code an order of magnitude faster than using explicit
closures, this is not possible in for us, where function calls need to be nodes
in the program graph and therefore objects, as we will see in Chapter
[[ref:truffle]], so we will represent closures using explicit closures in the pure
interpreter as well.

[[todo:Cite Kovacs benchmarks where HOAS on GHC wins]]

[[todo:HOAS vs Closure code example]]

[[todo:Single versus multi-argument (1 para)]]

[[todo:Resulting data structure (TLam, VLam, VCl) (1 figure)]]

*** Primitive operations
 A primop needs:
 - a symbol (nullary constructor)
 - a neutral head (when it gets stuck)
 - evaluation rule in eval (as many VLams as necessary to apply it, so that the
   computation rule can be tested)

 Not projections, though:
 - Fst:Term, (~eval env Fst = VLam \$ \v -> vFst v~),
 - projections should go to spines, not heads
 - neutrals should make access to the Ex that blocks computation as easy as possible,
 - see
   https://github.com/AndrasKovacs/setoidtt/blob/master/proto/Evaluation.hs#L293
   for an example

** Representing variables and environments
The way we define functions leads us to environments (Γ), which is a context
where we will look for variables.

[[todo:Named versus nameless representation (2 paras, example)]]

As we saw in [[inline:Where's WHNF?]], evaluation in λ-calculus is defined in terms
of α- and β-reductions. Verifying expression equivalence also uses variable
renaming. However, traversing the entire expression in the course of function
application and substituting variables is not efficient and there are several
alternative ways.

In typed λ calculus implementations in "real world" applications, evaluation
in an abstract machine or into a different semantic domain are used
instead. Then we have a clear distinction between terms (as the source code) and
runtime objects (on which we perform reductions). Machines like STG, Zinc, SECD,
... Normalization-by-evaluation works in this way, we eval/quote, [...]

*** Variables and names
Indices are widely used but drawback: use numbers instead of names, and a single
value can be referenced by multiple different numbers and keeps changing.

Levels less used, "reversed de Bruijn indexing", stable names.

cite:lescanne95_levels

Depth of a substitution - indices change during a renaming

#+label: variables
#+CAPTION: Normal forms in λ-calculus
#+begin_figure latex
\captionsetup{aboveskip=-1pt}
\begin{center}
\begin{tabular}{ccc}
& fix & succ \\
\textbf{Named} & $(λf.(λx.f (x x)) (λx.f (x x))) g$ & $λx.x (λy.x y)$ \\
\textbf{Indices}   & $(λ(λ1 (0 0) (λ1 (0 0)) g$ & $λ0 (λ1 0)$ \\
\textbf{Levels}    & $(λ(λ0 (1 1) (λ0 (1 1)) g$ & $λ0 (λ0 1)$ \\
\end{tabular}
\end{center}
#+end_figure

All three of them mentioned here work by replacing variable names with their
indices in a variable stack, informally /counting the lambdas/.

- de Bruijn indices - well-known, starting from the current innermost λ
- de Bruijn levels - less well-known, starting from the outer lambda, stable
- locally-nameless - dB indices for bound variables,, names for free variables

Given an environment stack, indices count from the start of the stack, levels
count form the bottom. Two ways of indexing the environment, indices useful for
a stable context, levels without a stable context.

- cite:kamareddine01_de_bruijn (pros & cons)
- cite:eisenberg20_stitch - motivation for indices/levels

In values, we have a known context, so it is easier to carry a level counter
around, and use levels in variables = Val → Term uses levels

In terms, we do not have a known context, so it makes sense to refer to use
indices. If we used levels, we would have to renumber them anyway, if we
e.g. performed an unfolding or otherwise used the term in a bigger context. =
Term → Val uses indices.

It is trivial to convert between them: given a context of size equal to the
current depth level $d$, level to index is $d - lvl - 1$ and index to level is
$d - ix - 1$.

*** Environments
[[todo:Environments - arrays, cons lists, stack, mutable/immutable (2 paras)]]

[[todo:Snippet of code - binding a variable, looking up a variable (1 figure)]]

** Normalization
[[todo:Equals (W) (H) NF, Single-variable functions - trivial transcription (1 para)]]

[[todo:What are our neutrals (1 para?)]]

[[todo:Eval Quote, very briefly (2 paras)]]

[[todo:Extensions (Pi, Sigma, Eta) (2 paras)]]

*** Normalization strategies
There are /normalization strategies/ that specify the order in which
sub-expressions are reduced. Common ones are /applicative order/ in which we first
reduce sub-expressions left-to-right, and then apply functions to them; and
/normal order/ in which we first apply the leftmost function, and only then reduce
its arguments. These closely correspond to the call-by-need and call-by-value
evaluation strategies, differing only in ...

We do not want to do η-reduction, as it might introduce non-termination or
undecidability (and in general is not compatible with subtyping
relations). η-expansion is sound for producing βη-long normal forms.

If we do not have a βη-long normal form and want to unify/compare (λx:A.M) with
N, you unify/compare \[N X\] and \[M[x≔X]\] for a fresh X. This is sufficient for
βη-equality (for the easy η-rule for Π/λ).

Normalization-by-evaluation:
- normalization = bring an expression with unknowns into a canonical form
- evaluation = compute the value of an expression relative to an environment
- machine code: normalizer ~ JIT compiler, evaluator ~ stack machine (env = stack)
- NbE = adapt an interpreter to simplify expressions with unknowns

NbE similar to machine code + equations from http://www.cse.chalmers.se/~abela/talkHabil2013.pdf
*** Evaluation
- cite:eisenberg20_stitch - exact algorithm description

cite:sestoft02_reduction

One evaluation strategy for one normal form each!
We will only use call-by-name to WHNF, and normal order to NF.

Careful, call-by-name ≅ normal order, not "is very closely related"

#+label: reduction-order
#+CAPTION: Reduction strategies for λ-calculus
#+begin_figure latex
\captionsetup[subfigure]{aboveskip=-1pt}
\begin{subfigure}[t]{.5\textwidth}\centering
\[x \xrightarrow{name} x\]
\begin{prooftree}
\AxiomC{\vphantom{$e \xrightarrow{norm} e'$}}
\UnaryInfC{$(λx.e) \xrightarrow{name} (λx.e)$}
\end{prooftree}
\begin{prooftree}
\AxiomC{$e₁ \xrightarrow{name} (λx.e)$}
\AxiomC{$e[x≔e₂] \xrightarrow{name} e'$}
\BinaryInfC{$(e₁ e₂) \xrightarrow{name} e'$}
\end{prooftree}
\begin{prooftree}
\AxiomC{$e₁ \xrightarrow{name} e'₁ ≢ λx.e$}
\UnaryInfC{$(e₁ e₂) \xrightarrow{name} (e'₁ e₂)$}
\end{prooftree}
\caption{Call-by-name to weak head normal form}
\end{subfigure}
\begin{subfigure}[t]{.5\textwidth}\centering
\[x \xrightarrow{norm} x\]
\begin{prooftree}
\AxiomC{$e \xrightarrow{norm} e'$}
\UnaryInfC{$(λx.e) \xrightarrow{norm} (λx.e')$}
\end{prooftree}
\begin{prooftree}
\AxiomC{$e₁ \xrightarrow{name} (λx.e)$}
\AxiomC{$e[x≔e₂] \xrightarrow{norm} e'$}
\BinaryInfC{$(e₁ e₂) \xrightarrow{norm} e'$}
\end{prooftree}
\begin{prooftree}
\AxiomC{$e₁ \xrightarrow{name} e'₁ ≢ λx.e$}
\AxiomC{$e'₁ \xrightarrow{norm} e''₁$}
\AxiomC{$e₂ \xrightarrow{norm} e'₂$}
\TrinaryInfC{$(e₁ e₂) \xrightarrow{norm} (e''₁ e₂)$}
\end{prooftree}
\caption{Normal order to normal form}
\end{subfigure}
#+end_figure

While reduction strategies mentioned (many pages back) are related to the
evaluation strategy and rewriting rules specify the behavior of an evaluator
exactly, "real-world" implementations of the λ-calculus do not use rewriting as
their primary method of evaluation.

What is commonly done is a translation of terms into another domain, which then
produces values, which are then compared or unified or whatever. This domain can
be an interpreter, compiled code, ...

First, however, we need to introduce evaluation models that connect reduction
strategies to terms used in implementation of programming
languages.

Call-by-value, otherwise called eager evaluation, corresponds to applicative
order reduction strategy cite:ariola97_cbn. Specifically, when executing a statement, its
sub-expressions are evaluated inside-out and immediately reduced to a value.
This leads to predictable program performance (the program will execute in the
order that the programmer wrote it, evaluating all expressions in order), but
this may lead to unnecessary computations performed: given an expression ~const
5 (ackermann 4 2)~, the value of ~ackermann 4 2~ will be computed but immediately
discarded, in effect wasting processor time.

Call-by-need, also lazy evaluation, is the opposite paradigm. An expression will
be evaluated only when its result is first accessed, not when it is created or
defined. Using call-by-need, the previous example will terminate immediately as
the calculation ~ackermann 4 2~ will be deferred and then discarded. However, it
also has some drawbacks, as the performance characteristics of programs may be
less predictable or harder to debug.

Call-by-value is the prevailing paradigm, used in all commonly used languages
with the exception of Haskell. It is sometimes necessary to defer the evaluation
of an expression, however, and lazy evaluation is emulated using closures or
zero-argument functions: e.g., in Kotlin a variable can be initialized using the
syntax ~val x by lazy { ackermann(4, 2) }~, and it will be initialized only if is
ever needed.

There is also an alternative paradigm, called call-by-push-value, which subsumes
both call-by-need and call-by-value as they can be directly translated to
CBPV--in the context of λ-calculus specifically. It does this by defining additional
operators /delay/ and /force/, one to create a /thunk/ that contains a deferred
computation, one to evaluate the thunk. Also notable is that it distinguishes between values
and computations: values can be passed around, but computations can only be
executed, or deferred.

Levy's call-by-push-value cite:levy99_cbpv formalism subsumes both (by means of
a translation strategy): it defines a single evaluation order in terms of
operations with a stack using two additional operators. It *distinguishes values
and computations*, operator $U$ (delay) creates a /thunk/, a delayed computation,
and operator $F$ (force) that forces its evaluation.

#+CAPTION: Call-by-push-value values and computations
#+begin_figure latex
\[\begin{array}{ccll}
A & ::= & U B | Σ(i∈I)A_i | 1 | A × A \\
B & ::= & F A | Π(i∈I)B_i | A → B
\end{array}\]
#+end_figure

- value of type $UB$ is a thunk producing a value of type $B$
- Σ is a pair (tag, Value)
- type A × A' is a value of type (V, V')
- type 1 is a 0-tuple
- computation of type $FA$ produces a value of type A
- computation Π pops a tag $i$ from operand stack, then is a computation of type $B$
- computation A → B pops a value of type A, then behaves as type B
[[todo:https://www.cs.bham.ac.uk/~pbl/papers/tlca99.pdf]]

*** Normalization
 *NbE*

 - well investigated in cite:lindley05_nbe_sml where there is a comprehensive of
   NbE techniques as applied to ML
 - also in cite:lindley05_nbe_sml there is a treatment of η reduction/expansion - READ

 - cite:altenkirch16_nbe - NbE for dependent types including sums, renamings!
 - cite:christiansen19_nbe_haskell - Haskell simple NbE

 - cite:gratzer19_modal_types - Tm, NfTm, NbTm; NfVal, NeVal, Val split, NbE well-written, normals, closures

 Normalization-by-evaluation in the style of Abel. Reminiscent of partial
 evaluation for λ calculus as we use "sticky" /neutral values/ instead of values we
 currently do not have. Provably confluent, a viable normalization strategy.

 As we use typed reduction rules, we do not need "subject reduction" algorithms(?)

 Conversion checking = type (or expression) equivalence checking, includes
 evaluation (NbE = full comparison of normal forms), checking equivalence "as
 described in the previous section"

 [[todo:Describe motivation for NbE, the process, what is a neutral, eval/quote (3 paras)]]

 \missingfigure{neutral terms}
*** Conversion / Equality / Equivalence checking
We will also need to compare two λ-terms to see if they are equal. There are two
general notions of equality, intensional and extensional. By intensional we mean
that they are written in the same way, whereas extensional equality means that
they behave in the same way with regards to an observer. While in general
programming languages, it is impractical to check whether two functions are
equal, whether intensionally or extensionally, we will need this later when we
are talking about type-checking: to compare two types expressed as λ-terms.

The usual notion of equality in λ-calculus is /α-equivalence of β-normal forms/,
meaning that we first reduce both expressions as much as possible and then
compare the symbols they are made of, ignoring differences in variable naming;
this is formally expressed as variable permutation.

** Elaboration
   :PROPERTIES:
   :CUSTOM_ID: elaboration
   :END:

[[todo:Approach - infer, check + eval/quote used, global contexts (2 paras)]]

[[todo:Metavariables and holes, sequential processing (1 para)]]

[[todo:How do we do unification (2 paras + 1 figure)]]
- cite:gundry13_pattern_tutorial - well-specified unification algorithm, pruning
- cite:abel11_sigma_unif - unifying sigma

*Bidirectional typing*
- cite:ferreira14_bidi - elaboration, bi-di
- cite:dunfield19_bidi - full rules, polarized??

Bidirectional typing (https://www.cl.cam.ac.uk/~nk480/bidir-survey.pdf) = now
standard approach, combines type-checking and inference, simpler to implement
even if inference is not required

[[todo:Intro, motivation, list alternatives, pros and cons (2 para)]]

[[todo:Sketch the process? Write out lambda + app rules? (half a page)]]

- elaboration uses metavariables, "holes" of unspecified type to solve
- unification instead of conversion checking - it solves metas
- metas are at the top-level, but to depend on local values, they are functions
- higher-order unification - to produce functions
- pattern unification, as higher-order is indecidable in general - distinct
  variables, not recursive

- spine is a list of arguments to apply
- neutrals - flexible is a meta with spine, rigid is local var with spine, flex
  is flex because it can be unblocked if the meta is solved can unblock
- meta forcing - brings is up to date with respect to the environment, up to WHNF
- meta solving should be destructive (sharing)
- renaming - maps variables from the spine to final de Bruijn level in the
  solution; is partial, requires depth of metacontext, and final context

** User interface
The user interface we will use is a command-line one: a multi-purpose command
that can start an interactive command-line session, execute a file, or
pre-compile a source file into a Native Image.

From my research, JLine is the library of choice for interactive command-line
applications in Java, so that is what I used for the REPL (Read-Eval-Print
Loop). It is a rather easy-to-use library: for the most basic use case, we only
need to provide a prompt string, and a callback. We can also add
auto-completion, a parser to process REPL commands, custom keybindings, built-in
pager or multiplexer, or even a scripting engine using Groovy.

Look at cite:garcia18_simple_lambda for other interaction modes:
- golden tests (~:verbose~ as a command, e.g.)
- :verbose = write out after every reduction
- :ski = express in terms of combinators
- interpreter + batch processor that reads REPL commands as well
- Jupyter kernel
- :types on, switches between untyped and simply-typed
- @@(expr) = print out as a proof (type derivation) using typing rules

[[todo:State keeping - load, reload + querying the global state (1 para)]]

commands:
- ~:l~ create a NameTable
- ~:r~ recreate a NameTable
- ~:t~ inferVar, print unfolded
- ~:nt~ inferVar, print folded
- ~:n~ inferVar type, gQuote term, show
- ~:e~ print elaboration output including all metas

\missingfigure{Example CLI session}

** Results
[[todo:Quick look at everything that this toy can do (2-3 examples?)]]

Evaluation, simplification, elaboration with holes, unification using eqRefl

Error reporting

* Adding JIT compilation to Montuno: MontunoTruffle
  :PROPERTIES:
  :CUSTOM_ID: truffle
  :END:
** Just-in-time compilation
- slow AST interpreter
- meta-tracing optimizing interpreter
  - PyPy
  - Records a path "trace" of interpreter execution
  - Programmer provides hints, but has no control over the paths traced (only
    some are)
  - unpredictable peak performance
- partial evaluation
  - old idea
  - offline and online partial evaluators
  - offline doesn't have deoptimization support
- Graal is "metacircular", it is a Java optimizer written in Java

- projections - partial evaluation
  - first = PE an interpreter with a program = executable
  - second = PE the specializer with an interpreter = compiler
  - third = PE the specializer with the specializer = compiler-maker

[[todo:Mention the general JIT theory - classical, tracing, rewriting]]

[[todo:Mention partial evaluation, Futamora projections (2 paras + 3 items + example)]]

[[todo:Where is the partial evaluation in Graal (1 para)]]

Graal compiles "hot code" to machine code by partially evaluating it. Partial
evaluation is TODO (Futamora). In particular, in dynamic languages it helps to
eliminate dynamic dispatch / megamorphic call overhead.

** GraalVM and the Truffle Framework
*** GraalVM
[[todo:Re-read, simplify (what is it, why we want to use it, what specifically do we use?)]]

*GraalVM* is an Oracle research project that was originally created as a
replacement for the HotSpot virtual machine written in C++. [[inline:Cite Oracle]]
It has since expanded to include other features novel to the Java world.

#+LABEL: graal
#+ATTR_LaTeX: :placement [!htb]
#+CAPTION: GraalVM and Truffle (source: oracle.com)
[[./img/graalvm.jpg]]

The project consists of several components which can be seen in ref:graal, the
main components being the following ones:

*Graal* is an optimizing just-in-time compiler based on partial evaluation. Graal
uses the JVM Compiler Interface which allows the main JVM to offload compilation
to external Java code. It can also use C1 (the old JVM JIT) which implies tiered
compilation (...disabled with ~-XX:-TieredCompilation~). When using the GraalVM,
the only JVMCI-compatible compiler is Graal, so automatically
used. [[inline:Simplify language]]

*SubstrateVM* is an alternative virtual machine that uses aggressive ahead-of-time
compilation [[inline:Cite SubstrateVM]] of Java bytecode into a standalone
executables, a so-called /Native Image. The project aims to guarantee fast
start-up times, relatively small binary files, and low memory footprint--as
opposed to slow start-up times due to JIT compilation and large memory usage
common to JVM-based languages.

*Truffle* (and *Truffle DSL*) is a language implementation framework, a set of
libraries that expose the internals of the Graal compiler to interpreter-based
language implementations. It promises that its users only need to write an
interpreter with a few framework-specific annotations in order to automatically
gain:
- access to all optimizations available on the Java Virtual Machine
- debugger support
- multi-language (/polyglot/) support between any other Java-based or
  Truffle-based languages (currently JavaScript, Python, Ruby, R, C, C++, WebAssembly)
- the ability to gradually add optimizations like program graph rewriting,
  node specializations, or inline instruction caching



[[todo:Images from file:///home/inuits/Downloads/graalvm-190721074229.pdf]]

[[todo:images from https://www.slideshare.net/jexp/polyglot-applications-with-graalvm]]

GraalVM is also intended to allow creating /polyglot applications/ easily,
applications that have their parts written in different languages. It is
therefore easy to e.g. call R to create visualizations for the results of a
Python program, or to call any Truffle language from Java.

This seems like a good middle ground between spending large amounts of time on
an optimized compiler, and just specifying the semantics of a program in an
interpreter that, however, will likely not run quickly.

While GraalVM/Truffle is open-source and released under GPL v2, an
enterprise edition that claims large performance improvements is released
commercially.

#+COMMENT: https://chrisseaton.com/truffleruby/jokerconf17/ (done)

Graal-compiled code still runs on the original HotSpot VM (written in C++)

Graal is also a graph optimizer, which takes the data-flow and instruction-flow
of the original program and is able to reorder them to improve performance. It
does what the original HotSpot VM would do, optimize JVM bytecode by reordering,
pre-compiling, or entirely rewriting instructions.

This graph is also extensible cite:duboscq13_graalir

- Canonicization: constant folding, simplification
- Global value numbering: prevents same code from being executed multiple times
- Lock coarsening: simplifies ~synchronized~ calls
- Register allocation: data-flow equals the registers required, optimize
- Scheduling: instruction-flow implies instruction order

Graal by itself is just better HotSpot, but there are other technologies in the
mix. SubstrateVM is an ahead-of-time compiler for Java which takes Java bytecode
and compiles is into a single binary, including the Graal runtime, pre-compiling
application code to greatly reduce warm up times (that are otherwise shared by
all JIT compilers).

In the ideal world, what GraalVM can do with the code by itself would be enough,
if that is not sufficient then we can add specializations, caches, custom
typecasts, ... We can also hand-tune the code, adding our hand-generated
bytecode into the mix.

*** Truffle
- trace-based optimization frameworks (RPython) vs partial evaluation frameworks (Truffle)
- JIT compilation for (suitably instrumented) interpreters
- Truffle provides a set of class and method annotations which reduce
  boilerplate and implementation effort. They drive a compile-time code
  generator

[[todo:Introduce Truffle specifically, polyglot, graph (de)optimization (3 paras + code sample)]]

Tiered compilation: C1, C2, Graal replaces C2

Multiple uses of Graal - it can function as instead of the C2 (server) compiler
or as a PE/AoT for SubstrateVM

Instruments, Chrome and other debugging support

Warm-up benchmark: http://macias.info/entry/201912201300_graal_aot.md

[[todo:It really is fast enough - FastR, Python benchmarks (2 paras + graph)]]

** Features common to Truffle languages
*** General features
[[todo:What does a typical Truffle language look like? (1 para)]]

[[todo:"We can look at a piece of Ruby code as a graph" (1 para + graph)]]

Another visualization option: Seafoam

#+COMMENT: https://norswap.com/truffle-tutorial/ (done)

[[todo:Introduce the canonical example - Literal and addition, trivial execute
fn (1 para, 1 figure)]]

Truffle: framework for implementing high-performance interpreters, in
  particular for dynamic languages, and AST interpreters. Each node implements
  the semantics of the operation/language construct. Graph flow - execution
  flows downwards, results flow upwards.

Graph manipulations:
- replace() to a more specific variant
- adopt() a new child node
- GraalVM aggressively inlines stable method calls into efficient machine code

- cite:wurthinger17_partial_eval - Truffle boundaries
- cite:wimmer17_deoptimization - deoptimization, on-stack replacement

*** Type specialization

[[todo:Type system]]

Basic case is type specialization - when an addition node only encounters
integers, there is no need to generate machine code for floats, doubles, or
operator overloads - only verified by fast checks. When these fail, the node is
de-optimized, and eventually re-compiled again.

Inline caching for e.g. method lookups, virtual method calls are typically only
ever invoked on a single class, which can be cached, and the dispatch node can
be specialized, perhaps even inline the
operation. (uninitialized/monomorphic/polymorphic/megamorphic = working with
jumptables + guard conditions)

Specializations are general, though, and nodes can go be specialized on
arbitrary conditions, using custom assumptions and /compilation final/ values. In
general, node states form a directed acyclic graph - "a node can ever become more
general".

Using a graph visualizer, we can look at this process on a simple example
commonly used to demonstrate this part of the Truffle framework: the ~+~
operation.

#+label: add-lang
#+begin_src kotlin
  abstract class LangNode : Node() {
      abstract fun execute(frame: VirtualFrame): Any
  }
  class IntLiteralNode(private val value: Long) : LangNode() {
      override fun execute(frame: VirtualFrame): Any = value
  }
  abstract class AddNode(
      @Child val left: LangNode,
      @Child val right: LangNode,
  ) : LangNode() {
      @Specialization
      fun addInt(left: Int, right: Int): Int = left + right
      @Specialization
      fun addString(left: String, right: String): String = left + right
      @Fallback
      fun typeError(left: Any?, right: Any?): Unit = throw TruffleException("type error")
  }
#+end_src

The Truffle framework is said to be a domain-specific language, which in this
case means a library, a set of annotations, and a code generator. This code
generator finds classes that inherit from the ~Node~ class and generates, among
others, the logic behind switching specializations.

The program graph is formed from a tree of Truffle ~Nodes~ from which we derive
our language-specific base class, ~LangNode~ in this case. We define two classes
that inherit from this class, one representing integer literals, and one for the ~+~
operator.

The abstract method ~execute~ in ~LangNode~ is the evaluation of this node. It takes
a ~VirtualFrame~, which represents a stack frame, and its return value is also the
return value of the node. In addition, methods starting with ~execute~ are special
in Truffle. Truffle will pick the most appropriate one based on return type
(with ~Any~ being the most general) and parameters.

In ~IntLiteralNode~ we directly override the method ~execute~, as there is only one
possible implementation. In ~AddNode~, however, we keep the class abstract and
do not implement ~execute~, and instead rely on Truffle to generate the appropriate
specialization logic.

Truffle will decide between the specializations based on parameter types, and on
user-provided guards (we will see further). Fallback specialization matches in
cases where no other one does. Names are irrelevant.

(Maybe show generated code?) Active an inactive specializations: can be multiple
active, execute method is based on state first, and only then on type checks -
smaller and possibly better optimized result. If no specialization matches, then
fall through to ~executeAndSpecialize~ which invalidates any currently compiled
using ~CompilerDirectives.transferToInterpreterAndInvalidate~ and sets state bits
for newly activated specializations.

(@Specialization(Replaces=[""]))


How to run? Need to wrap in a ~RootNode~, which represents executable things like
methods, functions and programs. Then create a ~CallTarget~ using
~Truffle.getRuntime().createCallTarget(root)~. Truffle uses CallTargets to record,
among others, how often a particular graph is called, and when to compile
it. Also it creates a VirtualFrame for this call target out of the provided
arguments.

IGV receives JIT compilation output - shows the Graal graphs produced during
optimization. Compilation is only triggered after a certain threshold of calls,
so we need to run a call target more than just once.

todo:Graal-graph

Another option: a CountNode (public int counter; execute = counter++). Green ==
state, grey is floating (not flow dependent), blue lines represent data flow
(data dependencies), red means control flow (order of operations)

*** Object Storage Model
cite:grimmer15_polyglot - Polyglot + OSM intro

- cite:mouton19_urilang - URI language, object-oriented

- OSM (Object Storage Model) - Frame (~typed HashMap)
- VirtualFrame - virtual/optimizable stack frame, "a function's/program's scope"
- MaterializedFrame - VirtualFrame in a specific form, not optimizable, can be
  stored in a ValueType

FrameDescriptor - shape of a frame
FrameSlot
FrameSlotKind

VirtualFrame - can be optimized, reordered

MaterializedFrame - an explicit Java Object on the heap, created from a
VirtualFrame by calling ~frame.materialize()~

Object Storage Model is a common way to organize data with layouts - Objects,
Shapes, Layouts.

TruffleRuby uses it for FFI to access Ruby objects directly as if they were C
~structs~. DynSem uses these to model frames/scopes instead of Frames, as it is a
meta-interpreter and uses Truffle Frames for its own data.

*** Dispatch
[[todo:dispatchNode, frames, argument passing, ExecutableNode, RootNode, CallTarget]]

- RootNode - can be made into a CallTarget. is at the root of a graph, "starting
  point of a function/program/builtin", "callable AST"

Direct/IndirectCallNode

VirtualFrames can be eliminated altogether, which results in highly efficient code

*** Caching
[[todo:@Cached()]]

*** Polyglot
[[todo:ValueTypes, InteropLibrary]]

cite:grimmer15_polyglot - Polyglot + OSM intro

*** Structure
[[todo:Common components - Launcher, LanguageRegistration, Nodes, Values, REPL (5 items)]]

Engine, Context, TruffleLanguage, Instrument

#+begin_src text
  N: unbounded
  P: N for exclusive, 1 for shared context policy
  L: number of installed languages
  I: number of installed instruments

  - 1 : Host VM Processs
   - N : Engine
     - N : Context
       - L : Language Context
     - P * L : TruffleLanguage
     - I : Instrument
       - 1 : TruffleInstrument
#+end_src

** Functional Truffle languages
*** Criteria
Evaluate languages on:
- overall project structure and runtime flow
- global/local names and environment handling
- calling convention
- lazy evaluation
- closure implementation
- graph manipulation, TruffleBoundaries, specializations

*** Truffled PureScript
#+COMMENT: https://github.com/slamdata/truffled-purescript/

Old project, but one of the only purely-functional Truffle languages.

Purescript is a derivative of Haskell, originally aimed at frontend
development. Specific to Purescript is eager evaluation order, so the Truffle
interpreter does not have to implement thunks/delayed evaluation.

Simple node system compared to other implementations:
- types are double and Closure (trivial wrapper around a RootCallTarget and a MaterializedFrame)
- VarExpr searches for a variable in all nested frames by string name
- Data objects are a HashMap
- ClosureNode materializes the entire current frame
- AppNode executes a closure, and calls the resulting function with a { frame, arg }
- CallRootNode copies its single argument to the frame
- IR codegen creates RootNodes for all top-level declarations, evaluates them,
  stores the result, saves them to a module Frame
- Abstraction == single-argument closure

*** Mumbler
An implementation of a Lisp

*** FastR
One of the larger Truffle languages

cite:stadler16_fastr

Replacement for GNU R, which was "made for statistics, not performance"

Faster without Fortran than with (no native FFI boundary, allows Graal to
optimize through it)

Interop with Python, in particular - scipy + R plots

Node replacement for specializing nodes, or when an assumption gets invalidated
and the node should be in a different state (AbsentFrameSlot,
ReplacementDispatchNode, CallSpecialNode, GetMissingValueNode, FunctionLookup.

#+begin_src kotlin
val ctx = Context.newBuilder("R").allowAllAccess(true).build();
ctx.eval("R", "sum").execute(arrayOf<Int>(1,2,3));
#+end_src

#+begin_src R
benchmark <- function(obj) {
    result <- 0L
    for (j in 1:100) {
       obj2 <- obj$objectFunction(obj)
       obj$intField <- as.integer(obj2$doubleField)
       for (i in 1:250) { result <- obj$intFunction(i, obj$intField) }
    }
    result
}
benchmark(.jnew("RJavaBench"))
#+end_src

Special features:
- Promises (call-by-need + eager promises)

*** Cadenza

- FrameBuilder - specialized MaterializedFrame
- Closure - rather convoluted-looking code

Generating function application looks like:
- TLam - creates Root, ClosureBody, captures to arr, arg/envPreamble
- Lam - creates Closure, BuilderFrame from all captures in frame
- Closure - is a ValueType, contains ClosureRootNode
- ClosureRootNode - creates a new VirtualFrame with subset of frame.arguments

*** Enso
A very late addition to this list, this is a project that originally rejected
Truffle (and dependent types in general, if I recall correctly) and used Haskell
instead. However, the project Luna was renamed to Enso, and rebuilt from scratch
using Truffle and Scala not long before my thesis deadline.

*** TruffleClojure
Implemented in a Master's thesis cite:feichtinger15_clojure

*** DynSem
- cite:vergu19_scopes - OSM for frames/scopes

* Language implementation: MontunoTruffle
  :PROPERTIES:
  :CUSTOM_ID: jit-interpreter
  :END:
** Introduction
Truffle is not primarily aimed at statically-typed languages or functional
languages. Its most easily accessible benefits lie in speculative optimization
of dynamically typed code and inline caches, where generic object-oriented code
can be specialized to a specific value type. Statically-typed languages have a
lot more information regarding the values that will flow through a function, and
e.g. GHC has a specific /specialization/ compiler pass.

However, there is a lot of overlap between the static optimizations done by
e.g. GHC and runtime optimizations done by Graal. An example would be
unfolding/inlining, where the compiler needs to make a single decision of
whether to replace a call to a function with its definition -- a decision that
depends on the size of the definition, whether they are in the same module, and
other heuristics cite:jones02_inliner. A Truffle interpreter would be able to
postpone the decision until execution time, when the definition could be inlined
if the call happened enough times.

[[todo:mental model = graph through which values flow, values may contain graphs]]

Its execution model is a tree of nodes where each node has a single operation
~execute~ with multiple specializations. The elaboration/evaluation algorithm from
the previous chapter, however, has several interleaved algorithms (infer, check,
evaluate, quote) that we first need to graft on to the Truffle execution model.

There are also several features that we require that are not a natural fit for
it, but where we can find inspiration in other Truffle languages. In particular,
lazy evaluation (FastR promises), partial function application (Enso), ???

We also have several options with regard to the depth of embedding: The most
natural fit for Truffle is term evaluation, where a term could be represented as
a value-level Term, and a CallTarget that produces its value with regard to the
current environment. We can also embed the bidirectional elaboration algorithm
itself, as a mixture of infer/check nodes.

In dynamic interpreters that Truffle is aimed at, it is easy to think of the
interpreter structure as "creating a graph through which values flow".

[[todo:Move this elsewhere ↓]]

There are several concerns here:
- algorithmic improvement is asymptotic -- the better algorithm, the better we
  can optimize it
- Truffle's optimization is essentially only applicable to "hot code", code that
  runs many times, e.g. in a loop
- We need to freely switch between Term and Value representations using
  eval/quote

The representation is also quite different from the functional interpreter where
we have used functions and data classes, as in Truffle, all values and operations
need to be classes.

[[todo:Restructure, clarify]]
Specific changes:
- everything is a class, rewrite functions/operations as classes/nodes
- annotations everywhere
- function dispatch is totally different
- lazy values need to be different
- ???

** Parser
ParsingRequest/InlineParsingRequest

Unfortunately, Truffle requires that there is no access to the interpreter state
during parsing, which means that we need to perform elaboration inside of a
~ProgramRootNode~ itself, "during runtime" per se.

[[todo: need to perform elaboration inside a programRootNode (not while parsing)]]

[[todo:stmt;stmt;expr -> return a value]]

** Representing values
As with the previous interpreter, we will start with function calls.

[[todo:dispatch, invoke, call Nodes, argument schema (copying), ?]]

[[todo:eta is TailCallException (2 para + example)]]

Passing arguments - the technical problem of copying arguments to a new stack
frame in the course of calling a function.

Despite almost entirely re-using the Enso implementation of function calls, with
the addition of implicit type parameters and without the feature of default
argument values,

I will nonetheless keep my previous analysis of calling conventions in
functional Truffle languages here, as it was an important part of designing an
Truffle interpreter and I spent not-insignificant amounts of time on it.

I have discovered Enso only a short while before finishing my thesis, and had to
incorporate the technologically-superior solution

Several parts of creating an AST for function calls:
- determining the position of arguments on the original stack - or evaluating
  and possibly forcing the arguments
- determining the argument's position on the stack frame of the function
- using this position in the process of inferring the new function call
- dispatch, invoke, call nodes???

*** Value types
 [[todo:Data classes with call targets]]

 [[todo:...depends on what will work]]

 Term and Val are ValueTypes that contain a callTarget - eval/quote(?)

 We could use Objects/Shapes/Layouts for dependent sums or non-dependent named
 coproducts.

** Representing environments and variables
[[todo:We need to use arrays, Collections are not recommended]]

[[todo:Frames and frame descriptors for local/global variable]]

[[todo:References, indices, uninitialized references]]

** Normalization
*** Evaluation order
 [[todo:We need to defer computations as late as possible - unused values that
 will be eliminated (1 para)]]

 [[todo:CBPV concepts, thunks with CallTargets (3 paras, example)]]

*** Calling convention
 [[todo:the need for the distinction - in languages with currying]]

 [[todo:the eval/apply paper is a recipe for a stack-based implementation of
 currying and helpful in our case when we need explicitly manage our stack via
 Frames as opposed to the interpreter where we relied on the host language for
 this functionality]]

 known/unknown calls, partially/fully/over-saturated calls

 cite:marlow04_fast_curry

 push-enter - arguments are pushed onto the stack, the function then takes as
 many as it requires

 eval-apply - the caller sees the arity of the function and then decides whether
 it is over-applied (evaluates the function and creates a continuation), appllied
 exactly (EVAL), or under-applied (creates a PAP, a closure-like value)

 [[todo:exactly describe the rules from eval-apply paper KNOWNCALL, EXACT, CALLK, PAP]]

** Elaboration
** User interface
REPL needs to be implemented as a ~TruffleInstrument~, it needs to modify and
otherwise interact with the language context.

** Polyglot
Demonstrate calling Montuno from other languages

Demonstrate Montuno's eval construct

Demonstrate Montuno's FFI construct - requires projections/accessors

** Implementation
[[todo:How to connect this together?]]

evaluation phases - translate to Code, run typecheck, run eval vs glued, ???

[[todo:How to edit the REPL]]

[[todo:Language registration in mx/gu]]

** Results
[[todo:Same evaluation as in previous section]]

[[todo:how fast are we now? Are there any space/time leaks?]]

[[todo:show graphs: id, const, const id; optimized graphs]]

* Optimizations: Making MontunoTruffle fast
   :PROPERTIES:
   :CUSTOM_ID: optimizations
   :END:
** Possible performance problem sources
[[todo:Reiterate JGross again]]

[[todo:how to find out whether X is relevant to us or not? How to prove the effect of JIT?]]

[[todo:Show asymptotes - binders, terms, sizes]]
[[todo:Show the graphs - large values, many iterations (warmup), sharing]]
** Possible optimizations
[[todo:What does Enso do, optimization phases?]]
[[todo:What can we do?]]

Hash consing = sharing structurally equal values in the environment. See below from Kmett:
https://gist.github.com/ekmett/6c64c3815f9949d067d8f9779e2347ef

Inlining, let-floating

Avoid thunk chaining: box(box(box(() => x))

"Immutable, except to simplify" + assumptions
Maximize evaluation sharing - globals, cache, ?

- cite:blaguszewski10_llvm - potential optimizations, LLVM impl, closures
- cite:gross14_coq_experience - Coq experience, a few reasons, comparison
- cite:gross21_performance - a lot of reasons in Coq
- cite:eisenberg20_stitch - CSE

Ruby uses threads, can we? Automatic parallelism
- cite:reid98_resumable_holes - concurrency & parallelism in GHC evaluation
- cite:hughes82_supercombinators - CAFs? Lazy evaluation?

Think about the fast vs slow path!

[[todo:Show before and afters for each optimization]]

- cite:zheng17_deoptimization - reasons for deoptimization

OSM in DynSem:
- DynSem also had to consider concept mapping: a program graph starts with
  generic node operations that immediately specialize to language-specific
  operations during their first execution
- HashMaps are efficient, but bring downsides. The Graal compiler cannot see
  inside the HashMap methods, and so cannot analyze data flow in them and use it
  to optimize them.
- DynSem also had to deal with runtime specification of environment manipulation
  as this is also supplied by the language specification. Also split between
  local short-lived values inside frames, and long-lived heap variables.
- Relevant to us is their use of the Object Storage Model, which they use to
  model variable scoping which is the processed into fixed-shape stack frames
  (separate from the Truffle Frames, this is a meta-interpreter). OSM's use case
  is ideal for when all objects of a certain type have a fixed shape. This is
  ideal for us, as tuples and named records have, by definition, a fixed shape
  (unlike Ruby etc. we do not support dynamic object reshaping, obviously).
- They did it separately from the Virtual/MaterializedFrame functionality to
  avoid the overhead of MaterializedFrames that Graal cannot optimize away.
- Truffle/Graal discourage the use of custom collections, and instead push
  developers towards Frames (which support by-name lookups) and Objects (same).

To enhance compilation specialization/inlining:
- Visualizations of call graphs - whether or not node children are stable calls
- Most DynSem calls are not stable calls, they are dispatched on runtime based
  on arguments - something that Graal does not see as stable (CompilationFinal)
- Two types of rules: mono- and polymorphic. based on whether they are called
  with different types of values at runtime. Poly- are not inlined
- DynSem found two types: dynamic dispatch (meta-interpreter depended on runtime
  info), and structural dispatch (based on the program AST and not on
  values). This is similar to our EvalNode, QuoteNode and similar, which depend
  on the type of the value
- Overloaded rules--rules with the same input shape--are merged into a single
  FusedRule node and iterated over with @ExplodeLoop.
- For mono/polymorphic rules, they use an assumption that a rule is monomorphic,
  specialize the rule, and recompile if it becomes polymorphic.
- Inlining nodes - polymorphic rules reduced to a set of monomorphic rules - a
  rule from the registry is cloned in an uninitialized state in a monomorphic
  call site and "inlined" (in a CompilationFinal field)
- They use a central registry of CallTargets that contain rules that they can
  clone and adopt locally if necessary to specialize--we can do the same!
- Disadvantages: there is more to compile and inline by Graal, instead of a
  CallTarget, they use a child. Likely to take longer to stabilize, but faster
  in the end.
** Glued evaluation
An optimization technique that attempts to avoid even more computation.

Parallel operation on two types of values, glued and local. Glued are lazily
evaluated to a fully unfolded form; local are eagerly computed to a head-normal
form but not fully unfolded, to prevent size explosions. This results in better
performance in a large class of programs, although it is not an asymptotic
improvement, as we have a small eagerly evaluated term for quoting, and a large
lazily evaluated for conversion checking.

This is another case of specialization: we have two operations to perform on the
same class of values, but each operation has its own requirements; in this case,
on the size of the terms as in quoting we want a small folded value but require
the full term for conversion checking.

cite:kaposi19_gluing

https://eutypes.cs.ru.nl/eutypes_pmwiki/uploads/Meetings/Kovacs_slides.pdf

** Splitting/type specializations/dict passing
** Optimizing function dispatch
[[todo:lambda merging, eta expansion]]

** Caching, sharing
[[todo:sharing common values, multiple references to the same object]]

** Common FP optimizations
[[todo:floating, inlining by hand]]

** Specializations
*** Truffle recommended optimizations
The optimization workflow recommended by the Truffle developers is as follows:
1. Run with a profiler to sample the application and identify responsible
   compilation units. Use a sampling delay (--cpusampler.Delay=MILLISECONDS) to
   only profile after warmup. See the Profiling guide.
2. Understand what is being compiled and look for deoptimizations. Methods that
   are listed to run mostly in the interpreter likely have a problem with
   deoptimization.
3. Simplify the code as much as possible where it still shows the performance
   problem.
4. Enable performance warnings and list boundary calls.
5. Dump the Graal graph of the responsible compilation unit and look at the
   phase After TruffleTier.
   1. Look at the Graal graphs at the phases After TruffleTier and After
      PartialEscape and check if it is what you would expect. If there are nodes
      there that you do not want to be there, think about how to guard against
      including them. If there are more complex nodes there than you want, think
      about how to add specialisations that generate simpler code. If there are
      nodes you think should be there in a benchmark that are not, think about
      how to make values dynamic so they are not optimized away.
6. Search for Invoke nodes in the Graal IR. Invoke nodes that are not
   representing guest language calls should be specialized away. This may not be
   always possible, e.g., if the method does I/O.
7. Search for control flow splits (red lines) and investigate whether they
   result from control flow caused by the guest application or are just
   artifacts from the language implementation. The latter should be avoided if
   possible.
8. Search for indirections in linear code (Load and LoadIndexed) and try to
   minimize the code. The less code that is on the hot-path the better.

---
[[todo:More on splitting!]]

- ~--engine.TraceCompilation~ prints a line for each method compilation
- ~--engine.TraceCompilationDetail~ prints a line for compilation queuing, start, and finish
- ~--engine.TraceCompilationAST~ prints the entire compiled AST
- ~--engine.TraceInlining~ prints inlining decision details
- ~--engine.TraceSplitting~ prints splitting decisions
- ~--engine.TraceTransferToInterpreter~ prints a stack trace for each explicit invalidation
- ~--engine.TracePerformanceWarnings=(call|instanceof|store|all)~
  - ~call~ prints when PE cannot inline a call
  - ~instanceof~ prints when PE cannot resolve virtual ~instanceof~ to a specific type
  - ~store~ prints when PE store location argument is not compilation final
- ~--engine.CompilationStatistics~ prints total compilation statistics
- ~--engine.CompilationStatisticDetails~ prints compilation histograms for each node
- ~--engine.TraceMethodExpansion=truffleTier~ prints a tree of all expanded Java methods
- ~--engine.TraceNodeExpansion=truffleTier~ prints a tree of all expanded Nodes
- ~--engine.MethodExpansionStatistics=truffleTier~ prints total Graal nodes produced by a method
- ~--engine.NodeExpansionStatistics=truffleTier~ also includes Graal specializations
- ~--engine.InstrumentBoundaries~ prints info about boundaries encountered (should be minimized)
- ~--engine.InstrumentBranches~ prints info about branch profiles
- ~--engine.SpecializationStatistics~ prints details about specializations performed
- ~--vm.XX:+TraceDeoptimization~ prints all deoptimizations
- ~--vm.XX:+TraceDeoptimizationDetails~ prints all deoptimizations with details

It is also possible to control what is being compiled, add details to IGV graphs
dumped, and print the exact assembly produced: see
https://github.com/oracle/graal/blob/master/truffle/docs/Optimizing.md.

*** How to debug specializations
*Specialization histogram:* If compiled with
~-Atruffle.dsl.GenerateSpecializationStatistics=true~ and executed with
~--engine.SpecializationHistogram~, Truffle DSL will compile the nodes in a
special way and show a table of the specializations performed during the execution of a
program.

Example shown at
https://github.com/oracle/graal/blob/master/truffle/docs/SpecializationHistogram.md,
maybe include the table?

*Slow path only:* If compiled with ~-Atruffle.dsl.GenerateSlowPathOnly=true~,
Truffle will only execute the last, most generic specialization, and will
ignore all fast path specializations.
** (Profiling)
[[todo:then, what tools to use to find the problems]]

*** Ideal Graph VIsualizer
A graphical program that serves to visualize the process of Truffle graph
optimization. When configured correctly, the IGV will receive the results of all
partial evaluations.

*** CPU Sampler
#+COMMENT: https://github.com/oracle/graal/blob/master/truffle/docs/Profiling.md

Running the language launcher with the options ~--cpusampler
--cpusampler.Delay=MILLISECONDS~ will start the CPU sampler. This tool serves to
profile the guest language (as opposed to the regular JDK Async Profiler which
will profile the entire process.

~--cpusampler.Delay~ helps to not include warm-up time in the results.

Using additional options (~--cpusampler --cpusampler.SampleInternal
--cpusampler.Mode=roots --cpusampler.Output=json~) and postprocessing the
generated JSON with an additional script we
can create a so-called flamegraph with the results of the sampling.

#+COMMENT: https://github.com/eregon/FlameGraph/blob/graalvm/stackcollapse-graalvm.rb

* Evaluation
  :PROPERTIES:
  :CUSTOM_ID: evaluation
  :END:
We want to evaluate a few programs of equivalent functionality, as evaluated by
elaboration, type-checking, and simplification in a number of dependently-typed
languages.

We also want to evaluate the performance of general β-normalization which only
requires a functional language--this is secondary, however.

We are mainly interested in asymptotic behaviors and not on constant
factors. Just-in-time compiled languages especially suffer from long warm-up
times, which means that common evaluation of "repeatedly running a command" will
not perform well.

** Subjects
Subjects for elaboration: Agda, Idris, Coq, GHC. Also cooltt, smalltt, redtt, Lean, ?

Subjects for normalization: as above, but also Cadenza (STLC), Clojure, GHC,
Scala, OCaml, ML, Eta, Frege, ?

Also subjects: Montuno, MontunoTruffle, and possibly other optimized versions.

** Workload
 - Nats - large type elaboration, call-by-need test
 - Nats - type-level calculation
 - Nats - value-level calculation
 - Nats - equality/forcing
 - Functions - nested function elaboration, implicits
 - Functions - embedded STLC?
 - pairs - large type elaboration, call-by-need test
 - pairs - nested accessors

- typical programs - see typical Agda
- computation-heavy tests (numerics,)
- memory-heavy tests (id id id...)
- nofib suite? (typecheck, circsim, infer, anna)
- hanoi, sort an array using trees
- use FFI in benchmarks - externalize a FFT?

Mention sources (this is from smalltt ^^)

Brief descriptions, what does each one evaluate/stress?

** Methodology
We need to use in-language support, if available. We want to avoid measuring
interpreter start-up, program parsing time, and other confounders.

To measure: memory usage (curve), compilation speed (in a type-heavy test),
evaluation speed (in a compute-heavy test)

hyperfine to benchmark - measures speed (what about ~prof~?)
memory profile from stderr output

Krun benchmark runner + its warmup_stats functionality for statistical analysis
of steady states, number of iterations it took to stabilize.

memory profile from GHC's RTS for agda/idris/smalltt (+RTS -p)
(what about coq? - https://github.com/coq/coq/blob/master/dev/doc/profiling.txt)
Graal's default memory profiler

Mention specific parameters (X iterations, machine specs, ?)

** Results
[[todo:Results]]

One-to-one evaluation and discussion of directly comparable subjects, confidence
intervals, likely causes of improvements/regressions, iterations to
steady-state.

** Discussion
Size of codebase

Effort required

Effect produced

Is this road viable?

** Next work
FFI, tooling

RPython, K Framework - exploration

SPMD on Truffle, Array languages

More type extensions OR totality (as a proof assistent)

Finite types, universes, no type in type, HoTT, CoC

[[todo:Is this useful at all? What's the benefit for the world? (in evaluation)]]

next work: LF, techniques, extensions, real language

* Conclusion
We tried X to do Y. It went well and we fulfilled the assignment.

As a side effect, I produced a reference book for functional/dependent
language implementation.

Original goal was X, it grew to encompass Y, Z as well.

* (bibliography, start of appendix)                           :ignoreheading:

#+BEGIN_EXPORT latex
\makeatletter
\def\@openbib@code{\addcontentsline{toc}{chapter}{Bibliography}}
\makeatother
\begin{flushleft}
#+END_EXPORT

bibliographystyle:bibstyle
bibliography:bibliography.bib

#+BEGIN_EXPORT latex
\end{flushleft}
\iftwoside\cleardoublepage\fi
\appendix
\appendixpage
\iftwoside\cleardoublepage\fi
\startcontents[chapters]
% \setlength{\parskip}{0pt}
% \printcontents[chapters]{l}{0}{\setcounter{tocdepth}{2}}
% \setlength{\parskip}{0.5\bigskipamount}
\iftwoside\cleardoublepage\fi
#+END_EXPORT

* Contents of the attached data storage
...
* Grammar
#+include: "../montuno/src/main/antlr/montuno/Montuno.g4" src antlr -n

* Footnotes

[fn:9] The elements of $R$ are written as $(s₁,s₂)$, which is equivalent to $(s₁,s₂,s₂)$.

[fn:8] FastR is between 50 to 85x faster than GNU R, depending on the source. cite:fumero17_jit_gpu

[fn:7] Unfortunately, there are no officially published benchmarks, but a number
of articles claim that TruffleRuby is 10-30x faster than the official C
implementation. cite:shopify2020

[fn:6]Kotlin authors claim 40% reduction in the number of lines of code, (from
https://kotlinlang.org/docs/faq.html)
[fn:5]https://kotlinlang.org/docs/idioms.html
[fn:4] In particular, ANTLR-provided visitors require that all return values
share a common super-class. Listeners don't allow return values and would
require explicit parse tree manipulation.
[fn:3]https://github.com/antlr/grammars-v4/
[fn:2]https://www.antlr.org/
[fn:1]Even though Kotlin seems not to be recommended by Truffle authors, there
are several languages implemented in it, which suggests there are no severe
problems.  "[...] and Kotlin might use abstractions that don't properly
partially evaluate." (from https://github.com/oracle/graal/issues/1228)
