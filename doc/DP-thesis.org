#+BEGIN_COMMENT
Normostrana = 1800c ≈ 250 slov
org-word-count
org-wc-display

16.4. 2303
17.4. 3023

minimum: 12,500
target: 17,500
maximum: 30,000
#+END_COMMENT

* (front matter)                                              :ignoreheading:
#+LANGUAGE: en
#+OPTIONS: texht:nil toc:nil author:nil ':t
#+LATEX_CLASS: fitthesis
#+LATEX_CLASS_OPTIONS: [english,zadani,odsaz]
# print = B&W links and logo
# cprint = B&W links, color logo
#+BIND: org-latex-title-command ""
#+BIND: org-latex-default-figure-position "tbh"
#+LaTeX_HEADER: \input{metadata}
#+BEGIN_EXPORT latex
\maketitle
\setlength{\parskip}{0pt}
{\hypersetup{hidelinks}\tableofcontents}
\iftotalfigures\listoffigures\fi
\iftotaltables\listoftables\fi
\iftotallistings\listoflistings\fi
\iftwoside\cleardoublepage\fi
\setlength{\parskip}{0.5\bigskipamount}
#+END_EXPORT

* Změnit název: "Montuno: Efficient elaboration of dependent types using JIT compiler"
#+BEGIN_EXPORT latex
\listoftodos
#+END_EXPORT

* Introduction
** (Old)
When creating small experimental or research languages, writing a compiler may
be too much effort for the expected gain. On the other hand an interpreter is
usually not as performant as its creators may require for more computationally
intensive tasks.

There is a potential third way, proposed by Yoshihiko Futamura in the 1970s,
called the Futamura projection (or partial program evaluation), wherein an
interpreter is specialized in conjunction with the source code of a program,
yielding an executable. Some parts of the interpreter may be specialized, some
optimized, some left off entirely. Depending on the quality of the specializer,
the gains may be several orders of magnitude.

The goal of my thesis is to evaluate whether the GraalVM/Truffle platform is
suitable enough to act as a specializer for functional languages, in particular
for the dependently-typed lambda calculus.  To illustrate in Figure
\ref{fig:futamora}, the question is whether the path \textit{Native
Image\textrightarrow Result} is fast enough compared to the path
\textit{Executable\textrightarrow Result}.

#+BEGIN_EXPORT latex
\begin{figure}
\centering
\begin{tikzcd}
{} & Program
 \arrow[bend right]{ld}{Compiler}
 \arrow[bend right=67]{dd}{Interpreter}
 \arrow[bend left]{rd}{Partial Evaluation}
 \arrow[bend left=67]{dd}{JIT} & {} \\
Executable \arrow[bend right]{rd}{Run} & {} & Native\ Image \arrow[bend left]{ld}{Run}
 \\ {} & Result & {}
\end{tikzcd}
\caption{Methods of program execution}
\label{fig:futamora}
\end{figure}
#+END_EXPORT

Truffle has already been used rather successfully for the (mostly) imperative
languages Ruby, Python, R, Java, and WebAssembly, but (purely-)functional
languages differ in their evaluation model and in particular the required
allocation throughput, so it is still an open question whether GraalVM is a good
enough fit.

The desired outcome---at least, of the first part of my thesis---is a set of
implementations, and a set of benchmarks demonstrating a positive or a negative
result.  If the result is positive, there are many potential follow-up tasks:
implementing a different, more complex language, maybe a language to be
interpreted into the dependently-typed lambda calculus to attempt the approach
implemented in the /Collapsing Tower of Interpreters/ cite:amin2017collapsing,
or experimenting with different runtime models - all depending on the results of
this preliminary proof of concept.

In the best case, the JIT-compiled program would be as close in performance to a
program processed by a hand-crafted compiler as possible (not including JIT
warm-up), and I would spend the second half of my thesis on different topics
(like provably-correct program transformations) instead of hand-optimizing the
primitive operations - I should find out which it is going to be as soon in the
second term as possible.

As far as I am aware, there are no other native just-in-time compiled
implementations of the dependently-typed lambda calculus, with the exception of
the preliminary investigations done by the originator of this idea
cite:kmett_2019, although there are a few projects implementing a lambda
calculus directly to the Java Virtual Machine byte code..

** New
*Motivation* What is the raison d'\^{e}tre of your project? Why should anyone care? No general meaningless claims. Make bulletproof arguments for the importance of your work.

*Problem definition* What exactly are you solving? What is the core and what is a bonus? What parameters should a proper solution of the problem have? Define the problem precisely and state how its solution should be evaluated.

*Existing solutions* Discuss existing solutions, be fair in identifying their strengths and weaknesses. Cite important works from the field of your topic. Try to define well what is the \textit{state of the art}. You can include a Section 2 titled ``Background'' or ``Previous Works'' and have the details there and make this paragraph short. Or, you can enlarge this paragraph to a whole page. In many scientific papers, \emph{this} is the most valuable part if it is written properly.

*Our solution* Make a quick outline of your approach -- pitch your solution.  The solution will be described later in detail, but give the reader a very quick overview now.

*Contributions* Sell your solution. Pinpoint your achievements. Be fair and objective.

The aim of this work is to create an efficient compiler for a language with
dependent types. Compiling dependently-typed languages, compared to simply-typed
or untyped languages, comes with large performance penalties, often making large
non-trivial programs hard to write due to the compiler running out of memory or
taking minutes to compile.
https://github.com/agda/agda/issues/514#issuecomment-129023737
https://github.com/idris-lang/Idris2/issues/964
On the other hand, programming language research projects place more and more
demands on the compiler (cubical, univalence, ... CITE)

The goal of this project is to create a compiler for a dependently-typed
language that outperforms existing compilers at, in particular, the speed of
type-checking and compilation; evaluation speed of the resulting program is not
as important for our benchmarks. Special attention will be on the asymptotics,
as the implementation platform brings rather large constant factors.

This work is a successor to the Cadenza project (cite) which implements a
simply-typed lambda calculus with extensions in the Truffle framework. While it
is unfinished and did not show promising performance compared to other
simply-typed lambda calculus implementations according to its creator Edward
Kmett, my work attempts to apply its ideas to the dependently-typed lambda
calculus, where the presence of type-level computation should lead to larger
gains.

[[comment::inline This work builds on the pedagogic and algorithmic work on DTT by A.K.]]

I have not found another attempt to apply just-in-time compilation in the
implementation of a language with dependent types and in type-level computations
in particular, despite the fact that adding just-in-time compilation to the
type-checking phase of compilation seems natural in the context of
dependently-typed languages where type-level computation is available. I assume
that it is due to the fact that the Truffle project is aimed primarily at
imperative languages and mapping functional concepts is not straightforward
(comment:luna) - and implementing a standalone just-in-time compiler is not
straightforward.

Improving the algorithms for type-checking is an active area of research, but
implementing them in a more dynamic manner (beyond just changing the data
structures) is not something I have found.

--- Using the technique of just-in-time compilation we can avoid the performance
problems associated with dependent type-checking by rewriting inefficient parts
of the control-flow graph during compilation.

--- The Truffle language implementation framework gives us a way of turning an
interpreter into a compiler with few changes, and allows us to rewrite the
control-flow graph and just-in-time compilation process during runtime.

--- This is a novel approach to a problem all current dependently-typed
languages face and, if successful, will bring immediate benefits to programming
language implementers.

[[comment:In addition, easy prototyping, stepping block for other projects (efficient LF/Twelf, ...)]]

smalltt a step in another direction

* Language Design ≡ Designing a DTLC
** Dependent types
Intro dependent types + motivating example

hot ML research area (cubical, path, ...) (look in Kovacs' materials)

*** Π-type
dependent product type, dependent function type

Π(n:N). Vec(R, n) ~ dependent function type
Π(n:N). R ~ regular function type

*** Σ-type
dependent sum type, dependent pair type

(a, b) : Σ(x:A). B(x) ~ dependent sum type

*** First order dependent types
Generalizing the function space to the dependent function space
a -> b => (a : A) -> b

** Lambda Calculus
To present the dependently-typed lambda calculus well, we will build up to it
starting from the untyped lambda calculus.

*** Untyped Lambda Calculus
The untyped lambda calculus is a simple language consisting of just three kinds
of forms: variables, function application, and abstraction.

#+CAPTION: Untyped lambda calculus
#+ATTR_LaTeX: :options [!htpb]
#+begin_figure latex
\[\begin{array}{ccll}
e & ::= & x            & \text{variable} \\
  & |   & e_1~e_2      & \text{application} \\
  & |   & \lambda x. e & \text{abstraction}
\end{array}\]
#+end_figure

syntax, semantics, usage

*** Simply-Typed Lambda Calculus
The simply-typed lambda calculus introduces the concept of types, which have
separate syntax and semantics from the language of terms.

The language of terms is extended with a fourth kind of term, type annotation.

The language of types is the universal or base type, and an arrow from between
types.

[[comment:This language is not turing-complete, must terminate :inline]]

#+CAPTION: Simply typed lambda calculus
#+ATTR_LaTeX: :options [!htpb]
#+begin_figure latex
\[\begin{array}{ccll}
e & ::= & x           & \text{variable} \\
  & |   & e_1~e_2      & \text{application} \\
  & |   & \lambda x. e & \text{abstraction} \\
  & |   & x:\tau     & \text{annotation}
\end{array}\]
\[\begin{array}{ccll}
\tau & ::= & \alpha           & \text{base type} \\
     & |   & \tau\rightarrow\tau' & \text{composite type}
\end{array}\]
#+end_figure

syntax, semantics, type checking, type inference

*** Dependently-Typed Lambda Calculus

\missingfigure{Lambda cube}

Lambda cube

The dependently typed lambda calculus merges the languages of terms and types together
simplifying the grammar.

The capabilities of typed lambda calculi can be described by the lambda cube

terms on terms

terms on types (STLC)

types on types (1st order DTLC)

types on terms (CoC)

This brings many algorithmic difficulties which I sidestep by choosing the
first-order dependently-typed lambda calculus with the /type-in-type/ rule.

#+CAPTION: Dependently typed lambda calculus
#+ATTR_LaTeX: :options [!htpb]
#+begin_figure latex
\[\begin{array}{ccll}
e & ::= & x      & \text{variable} \\
  & |   & e_1~e_2 & \text{application} \\
  & |   & λ x. e  & \text{abstraction} \\
  & |   & x:τ     & \text{annotation} \\
  & |   & *       & \text{the type of types} \\
  & |   & ∀ x:ρ.ρ & \text{dependent function space}
\end{array}\]
#+end_figure
...

directions, decidability, where is DTLC

implicits, holes, metacontext

** Syntax
In language implementation, the syntax that the programmer writes, the surface
syntax, is then parsed, simplified, and de-sugared into the core language, which
is what is actually used for evaluation and type-checking.

#+ATTR_LaTeX: :placement [!htpb]
#+CAPTION: The constant function in LambdaPi
#+begin_src text
let const = (\ a b x y -> x) :: forall (a :: *) (b :: *) . a -> b -> a
#+end_src

[[comment:grammar :inline]]

** Semantics
- Lambda calculus equivalences:
  - α-equivalence = structural, allows α-renaming
  - β-equivalence = allows β-reduction (function application step) ==
    α-equivalence of β-normal forms
  - η-equivalence = allows η-reduction (f ≡ λx. f x)

- Type checking = determining whether a program is well-typed

- Type inference = the process of obtaining the type of an expression from its
  parts or implicits (Hindley-Milner is well-known, ~inference rules~)

- Elaboration = convert a partially specified expression into a complete,
  type-correct form (http://leodemoura.github.io/files/elaboration.pdf)

- Bidirectional typing (https://www.cl.cam.ac.uk/~nk480/bidir-survey.pdf) = now
  standard approach, combines type-checking and inference, simpler to implement
  even if inference is not required

- Conversion checking = type (or expression) equivalence checking, includes
  evaluation (NbE = full comparison of normal forms)

*** Inference rules
[[comment:rules -> graphs :inline]]

*** Evaluation rules
[[comment:rules -> graphs :inline]]

This looks nice: https://homepages.inf.ed.ac.uk/wadler/papers/mpc-2019/unraveling.pdf

*** Extensions
- built-ins / primitives / wired-in types
- FFI with truffle
- threads, ... - ignored

* Language Implementation ≡ Implementing a DTLC: Montuno
** Syntax representation
*** Closures
HOAS vs Closure

HOAS impossible in Truffle, functions and function calls need to be objects
HOAS = using the host language's support functions
therefore explicit closures

*** De Bruijn
Given an environment stack, indices count from the start of the stack, levels
count form the bottom. Two ways of indexing the environment, indices useful for
a stable context, levels without a stable context.

Both remote the problem of substitution, can be converted by subtraction.

We use both, indices when quoting evaluated values.

** Type-checking and elaboration
what is elaboration, why is it slow, glued, nondet, cite Kovacs

*** Type-checking principles
bidi, nbe

** Evaluation principles
*** Evaluation model
call-by-need

*** Evaluation model of function application
closely related to closure abstraction

push-enter - arguments are pushed onto the stack, the function then takes as
many as it requires

eval-apply - the caller sees the arity of the function and then decides whether
it is over-applied (evaluates the function and creates a continuation), appllied
exactly (EVAL), or under-applied (creates a PAP, a closure-like value)

-- exactly describe the rules from eval-apply paper KNOWNCALL, EXACT, CALLK, PAP
-- known application ( = known arity), unknown function

** Project overview
[[comment:Gradle, justify choice of Kotlin, JUnit, ... :inline]]

ANTLR is the parser to use in JVM

Several ways to consume: listener, visitor - I've used AST transformation which
is the most compact and most familiar to other DTLC implementations which are
usually in functional languages

(listener - enter/exit function calls, visitor is similar, toAst uses recursive calls)

#+include: "../montuno/src/main/antlr/montuno/Montuno.g4" src antlr -n

** Parser

** Data model

** Pure Interpreter
modeled using plain functions, using Kotlin's lazy evaluation features

* Evaluation methodology ≡ Designing DTLC benchmarks
cooltt, smalltt, redtt, cubicaltt, Agda, Idris, ...

We're interested in asymptotics
** Methodology
equivalent programs for a few dependently-typed languages

memory profile from GHC's RTS for agda/idris/smalltt (+RTS -p)
(what about coq? - https://github.com/coq/coq/blob/master/dev/doc/profiling.txt)
Graal's default memory profiler

hyperfine to benchmark - measures speed
(what about ~prof~?)
memory profile from stderr output

  - from SmallTT project, from Idris project
  - memory usage (curve)
  - compilation speed (type-heavy test)
  - evaluation speed (compute-heavy test)

*** Design
In theory, the dependently-typed lambda calculus does not support general
recursion, as the associated type theories are strongly normalizing. We can work
around this using special builtin constructs, such as the explicit fixpoint
combinator, or mutually recursive ~letrec~.

The /Y/ combinator $Y=λf.(λ x.f(x x)) (λx.f (x x))$ is natural in functional
languages but typing it is impossible without special support for general
recursion in types. (see mpc2019
https://homepages.inf.ed.ac.uk/wadler/papers/mpc-2019/unraveling.pdf which uses
explicit wrap/unwrapping operations)

*** Specific test cases:
- Nats - large type elaboration, call-by-need test
- Nats - type-level calculation
- Nats - value-level calculation
- Nats - equality/forcing
- pairs - large type elaboration, call-by-need test
- pairs - nested accessors
- function types - embed STLC?

SmallTT, Coq, Agda, GHC - for comparison

* Introducing gradual just-in-time compilation ≡ Adding Truffle to Montuno: $Montuno_{\mathbb{T}}$
** Self-optimizing interpreters
*** Classical

*** Tracing

*** Rewriting
** GraalVM and the Truffle Framework
*** Intro: ecosystem, purpose, benefits
The GraalVM project is originally written as a replacement for the HotSpot
virtual machine written in C++.

The project consists of several components: Graal, a JIT compiler, Substrate VM,
a separate virtual machine, Truffle, an interpreter runtime library, Truffle
DSL, a code generator which makes Truffle itself usable.

Also other tooling, mx for installing SDKs and compiling Truffle languages, gu
as a language manager for downloading and installing external languages.

Graal uses the JVMCI JVM Compiler Interface which allows the main JVM to offload
compilation to external Java code. It can also use C1 (the old JVM JIT) which
implies tiered compilation (...disabled with ~-XX:-TieredCompilation~). When using
the GraalVM, the only JVMCI-compatible compiler is Graal, so automatically used.

Graal-compiled code still runs on the original HotSpot VM (written in C++)


Graal is also a graph optimizer, which takes the data-flow and instruction-flow
of the original program and is able to reorder them to improve performance. It
does what the original HotSpot VM would do, optimize JVM bytecode by reordering,
pre-compiling, or entirely rewriting instructions.

- Canonicization: constant folding, simplification
- Global value numbering: prevents same code from being executed multiple times
- Lock coarsening: simplifies ~synchronized~ calls
- Register allocation: data-flow equals the registers required, optimize
- Scheduling: instruction-flow implies instruction order
#+COMMENT: https://chrisseaton.com/truffleruby/jokerconf17/

Graal by itself is just better HotSpot, but there are other technologies in the
mix. SubstrateVM is an ahead-of-time compiler for Java which takes Java bytecode
and compiles is into a single binary, including the Graal runtime, pre-compiling
application code to greatly reduce warm up times (that are otherwise shared by
all JIT compilers).

Truffle is a library that uses Graal to allow interpreters to use Graal's
optimization technology - the promise is "fast, polyglot applications by writing just an
interpreter".

In the ideal world, what GraalVM can do with the code by itself would be enough,
if that is not sufficient then we can add specializations, caches, custom
typecasts, ... We can also hand-tune the code, adding our hand-generated
bytecode into the mix.

(Also, some fighting with Java 9 modules (closed compilation units with explicit
imports and module interfaces, needed to explicitly disable export control.)


------------------------

 *GraalVM* is a just-in-time optimizing compiler for the Java bytecode. *Truffle* is
 a set of libraries that expose the internals of the GraalVM compiler, intended
 for easy implementation of other languages. So far JavaScript, Python, Ruby, R,
 and WebAssembly have Truffle implementations, and therefore can run on the JVM.

 GraalVM is also intended to allow creating /polyglot applications/ easily,
 applications that have their parts written in different languages. It is
 therefore easy to e.g. call R to create visualizations for the results of a
 Python program, or to call any Truffle language from Java.

 There is also the option to compile a /Native Image/ to eliminate most program
 start-up costs associated with a just-in-time compiler, pre-compiling the
 program partially (ahead-of-time).

 From the point of view of a programmer, Truffle makes it possible to write an
 interpreter, and then slowly add optimizations like program graph rewriting,
 node specializations, inline instruction caching or others. This seems like a
 good middle ground between spending large amounts of time on an optimized
 compiler, and just specifying the semantics of a program in an interpreter that,
 however, will likely not run quickly.

 While GraalVM/Truffle is open-source and released under GPL v2, an
 enterprise edition that claims large performance improvements is released
 commercially.

 #+ATTR_LaTeX: :placement [!htb]
 #+CAPTION: GraalVM and Truffle (source: oracle.com)
 [[./img/graalvm.jpg]]

 JIT options - specialization, deoptimization
 Use cases
 Potential optimizations
*** Graal
replacement for HotSpot VM, implemented in Java (self-hosting)

 marketing texts

 diagrams

 Hotspot's JIT vs Graal's

*** Truffle
 Intro + motivating multilanguage snippet

[[comment:Self-optimizing AST interpreters (tracing, JIT) :inline]]

 features with code samples

 benchmarks for other languages

** Truffle Interpreter
evaluation phases - translate to Code, run typecheck, run eval vs glued, ???

slightly impractical compared to the functional version, subclasses with methods
instead of plain functions -> required for truffle annotations

truffle specifics

inline cache, tail call, trampoline (continuations)

!! show their effect on program graphs

native image

*** Frames
FrameDescriptor - shape of a frame
FrameSlot
FrameSlotKind

VirtualFrame - can be optimized, reordered

MaterializedFrame - an explicit Java Object on the heap, created from a
VirtualFrame by calling ~frame.materialize()~

* Optimizations ≡ Making $Montuno_{\mathbb{T}}$ fast

** Profiling
*** Ideal Graph VIsualizer
A graphical program that serves to visualize the process of Truffle graph
optimization. When configured correctly, the IGV will receive the results of all
partial evaluations.

*** CPU Sampler
#+COMMENT: https://github.com/oracle/graal/blob/master/truffle/docs/Profiling.md

Running the language launcher with the options ~--cpusampler
--cpusampler.Delay=MILLISECONDS~ will start the CPU sampler. This tool serves to
profile the guest language (as opposed to the regular JDK Async Profiler which
will profile the entire process.

~--cpusampler.Delay~ helps to not include warm-up time in the results.

Using additional options (~--cpusampler --cpusampler.SampleInternal
--cpusampler.Mode=roots --cpusampler.Output=json~) and postprocessing the
generated JSON with an additional script we
can create a so-called flamegraph with the results of the sampling.

#+COMMENT: https://github.com/eregon/FlameGraph/blob/graalvm/stackcollapse-graalvm.rb


** Specializations
*** Debugging specializations
*Specialization histogram:* If compiled with
~-Atruffle.dsl.GenerateSpecializationStatistics=true~ and executed with
~--engine.SpecializationHistogram~, Truffle DSL will compile the nodes in a
special way and show a table of the specializations performed during the execution of a
program.

Example shown at
https://github.com/oracle/graal/blob/master/truffle/docs/SpecializationHistogram.md,
maybe include the table?

*Slow path only:* If compiled with ~-Atruffle.dsl.GenerateSlowPathOnly=true~,
Truffle will only execute the last, most generic specialization, and will
ignore all fast path specializations.

* Results ≡ Is $Montuno_{\mathbb{T}}$ fast enough?

* Conclusion
next work: LF, techniques, extensions, real language


* (bibliography, start of appendix)                           :ignoreheading:

#+BEGIN_EXPORT latex
\makeatletter
\def\@openbib@code{\addcontentsline{toc}{chapter}{Bibliography}}
\makeatother
\begin{flushleft}
#+END_EXPORT

bibliographystyle:bibstyle
bibliography:bibliography.bib

#+BEGIN_EXPORT latex
\end{flushleft}
\iftwoside\cleardoublepage\fi
\appendix
\appendixpage
\iftwoside\cleardoublepage\fi
\startcontents[chapters]
% \setlength{\parskip}{0pt}
% \printcontents[chapters]{l}{0}{\setcounter{tocdepth}{2}}
% \setlength{\parskip}{0.5\bigskipamount}
\iftwoside\cleardoublepage\fi
#+END_EXPORT

* Contents of the attached data storage
...
