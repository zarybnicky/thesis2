#+BEGIN_COMMENT
Normostrana = 1800c ≈ 250 slov
org-word-count
org-wc-display

16.4. 2303w
17.4. 3023w
18.4. 3600w? (Σ36p)
19.4. 4000w? (Σ40p)
22.4. 5200w
24.4. 6700w

minimum: 12,500
target: 17,500
maximum: 30,000
#+END_COMMENT
* (front matter)                                              :ignoreheading:
#+LANGUAGE: en
#+OPTIONS: texht:nil toc:nil author:nil ':t
#+LATEX_CLASS: fitthesis
#+LATEX_CLASS_OPTIONS: [english,zadani,odsaz]
#+EXCLUDE_TAGS: noexport
# print = B&W links and logo
# cprint = B&W links, color logo
#+BIND: org-latex-title-command ""
#+BIND: org-latex-prefer-user-labels t
#+BIND: org-latex-default-figure-position "tbh"
#+LaTeX_HEADER: \input{metadata}
#+BEGIN_EXPORT latex
\maketitle
\setlength{\parskip}{0pt}
{\hypersetup{hidelinks}\tableofcontents}
\iftotalfigures\listoffigures\fi
\iftotaltables\listoftables\fi
\iftotallistings\listoflistings\fi
\listoftodos
\iftwoside\cleardoublepage\fi
\setlength{\parskip}{0.5\bigskipamount}
#+END_EXPORT

* (Recommended thesis structure)                                   :noexport:
1. "Úvod do knížečky", jedna strana, bez podkapitol
2. "Co bylo třeba vystudovat?" - "Je tato informace potřebná k pochopení toho, co jsem udělal?"
3. "Nové myšlenky" - rozhodl jsem, vymyslel, rozvrhl, odvodil, vylepšil, navrhl, ...
4. "Implementace a vyhodnocení" - udělal jsem, posbíral data, výsledky
5. "Závěr" - žádné nové poznatky, možnosti pokračování, "naladit oponenta"

* (Casual outline)                                                 :noexport:
*Story:*
- Let's build a fast Agda!
- Why Agda, what's at its core?
- Oh, dependent types? What's that good for, how does it work?
- Why are all of these languages functional?
- Lambda calculus and everything - that's a lot of formalisms...
- How to build it? Fast λs, that's a start, then build it up.
- So, does it work? What can I use it for, does it fulfill the design goals?
- Why is it so slow? Asymptotics...
- So, Truffle should make it fast, what specifically? What's the inspiration?
- How do we start, what needs to be changed?
- Wow, that's a lot of concepts, slow down, explain!
- So, does this changed work function at all? What about polyglots? What UIs?
- What can we make improve, was this just the start?
- Did we fulfill our goals, is this useful at all?
- If this is somebody's starting point, what will they need to know?

*Original goal:*
- efficient βη-normalization runtime
- glued, lazy as optimistic improvements
- Show the asymptotics on Peano, Church numbers

*Side outcome:* compilation of all the relevant concepts.

* Introduction
#+COMMENT: Motivation: why this project? Why should anyone care?

Proof assistants like Coq, Agda or Idris, or other languages with dependent
types allow programmers to write provably correct-by-construction code in a
manner similar to a dialog with the compiler cite:norell08_agda_tutorial. They
also face serious performance issues when applied to problems or systems on a
large-enough scale cite:gross21_performance. Their performance grows
exponentially with the number of lines of code in the worst case
cite:nawaz19_survey_provers, which is a significant barrier to their use. While
many of the performance issues are fundamentally algorithmic, a better runtime
system would improve the rest. However, custom runtime systems or more capable
optimizing compilers are time-consuming to build and maintain. This thesis seeks
to answer the question of whether just-in-time compilation can help to improve the
performance of such systems.

Moving from custom runtime systems to general language platforms like e.g., the
Java Virtual Machine (JVM) or RPython [[inline:Cite]], has improved the performance
of several dynamic languages: project like TruffleRuby, FastR, or PyPy. It has
allowed these languages to re-use the optimization machinery provided by these
platforms, improve their performance, and simplify their runtime systems.

#+COMMENT: Problem definition: What exactly are you solving? What is the core and what is a bonus? What parameters should a proper solution to the problem have?

As there are no standard benchmarks for dependently typed languages, we design a
small, dependently-typed core language to see if using specific just-in-time
(JIT) compilation techniques produces asymptotic runtime improvements in the
performance of βη-normalization, which is a large part of the elaboration
process and is also the part that can most likely benefit from JIT compilation. inline:Nongoals

#+COMMENT: Existing solutions: be fair in identifying their strengths and weaknesses. Cite important works from the field of your topic. Try to define well what is the state of the art.

State of the art proof assistants like Coq, Agda, Idris, or others is what we
can compare our results with. There is also a large number of research projects
being actively developed in this area; Lean is a notable one that I found too
late in my thesis to incorporate its ideas. However, the primary evaluation will
be against the most well established proof assistants.

inline:reformulate As for the languages that use Truffle, the language implementation framework
that allows interpreters to use the JIT optimization capabilities of GraalVM, an
alternative implementation of the Java Virtual Machine: there are numerous
general-purpose functional languages, the most prominent of which are
TruffleRuby and FastR. Both were reimplemented on the Truffle platform,
resulting in significant performance improvements[fn:7][fn:8]. We will
investigate the optimization techniques they used, and reuse those that are
applicable to our language.

There is also a number of functional languages on the Java Virtual Platform that
do not use the Truffle platform, like Clojure, Scala or Kotlin, as well as
purely functional languages like Eta or Frege. All of these languages compile
directly to JVM byte code: we may compare our performance against their
implementation, but we would not be able to use their optimization
techniques. As far as I am aware, there are no implementations of the
dependently-typed lambda calculus that use JIT compilation.

The closest project to this one is Cadenza cite:kmett_2019, which served as the
main inspiration for this thesis. Cadenza is an implementation of the
simply-typed lambda calculus on the Truffle framework. While it is unfinished
and did not show as promising performance compared to other simply-typed lambda
calculus implementations as its author hoped, this project applies similar ideas
to the dependently-typed lambda calculus, where the presence of type-level
computation should lead to larger gains.

#+COMMENT: Our solution: Make a quick outline of your approach, pitch your solution

In this thesis, I will use the Truffle framework to evaluate how well are the
optimizations provided by the just-in-time compiler GraalVM suitable to the
domain of dependently-typed languages. GraalVM helps to turn slow interpreter
code into efficient machine code by means of /partial evaluation./ [[inline:Cite
GraalVM]] During partial evaluation, specifically the second Futamura projection
cite:latifi19_futamura, an interpreter is specialized together with the source
code of a program, yielding executable code. Parts of the interpreter could be
specialized, some optimized, and some could be left off entirely. Depending on
the quality of the specializer, this may result in performance gains of several
orders of magnitude.

Truffle makes this available to language creators, they only need to create an
interpreter for their language. It also allows such interpreters to take
advantage of GraalVM's /polyglot/ capabilities, and directly interoperate with
other JVM-based languages, their code and values
inline:CitePolyglot. Development tooling can also be derived for Truffle
languages rather easily cite:stolpe19_environment. Regardless of whether Truffle
can improve their performance, both of these features would benefit
dependently-typed or experimental languages.

#+COMMENT: Contributions: Sell your solution. Pinpoint your achievements. Be fair and objective.

While this project was originally intended just as a λΠ calculus compiler and an
efficient runtime, it has ended up much larger due to a badly specified
assignment. I also needed to study type theory and type checking and elaboration
algorithms that I have used in this thesis, and which form a large part of
chapters ref:lambda and ref:interpreter.

Starting from basic λ calculus theory and building up to the systems of the
lambda cube, we specify the syntax and semantics of a small language that I
refer to as Montuno (Chapter ref:lambda). We go through the principles of λ
calculus evaluation, type checking and elaboration, implement an interpreter for
Montuno in a functional style (Chapter ref:interpreter), and design a set of
benchmarks to evaluate the language's performance (Chapter ref:bench).  In the
second half of the thesis, we evaluate the capabilities offered by Truffle and
the peculiarities of Truffle languages (Chapter ref:truffle), implement an
interpreter for Montuno on the Truffle framework (Chapter ref:jit-interpreter),
and apply various JIT optimizations to it (Chapter ref:optimizations). After
evaluating our overall results, we close with a large list of possible follow-up
work (Chapter ref:evaluation).

* Language specification: λΠ calculus with extensions
  :PROPERTIES:
  :CUSTOM_ID: lambda
  :END:
** Introduction
[[todo:To reiterate, the core language of proof assistants (1 para)]]

[[todo:More powerful than simple type systems but potentially not decidable (1 para)]]

[[todo:Why not reuse Agda's core language? Goal is minimal lang + extensions (2 paras)]]

ref:rosetta
ref:curry-howard

#+label: rosetta
#+CAPTION: The correspondence between mathematical logic and type theory
#+ATTR_LaTeX: :options [!htpb]
#+begin_figure latex
\begin{center}
\begin{tabular}{c|c}
Logic & Type theory \\\hline\\[-1em]
\shortstack{$⊤$ \\ true} &
\shortstack{$()$ \\ unit type} \\[7pt]
\shortstack{$⊥$ \\ false} &
\shortstack{$∅$ \\ empty type} \\[7pt]
\shortstack{$p ∧ q$ \\ conjunction} &
\shortstack{$a × b$ \\ sum type} \\[7pt]
\shortstack{$p ∨ q$ \\ disjunction} &
\shortstack{$a + b$ \\ product type} \\[7pt]
\shortstack{$p ⇒ q$ \\ implication} &
\shortstack{$a → b$, $a^b$ \\ exponential (function) type} \\[7pt]
\shortstack{$∀x ∈ A, p$ \\ universal quantification} &
\shortstack{$Π_{x : A}B(x)$ \\ dependent product type} \\[7pt]
\shortstack{$∃x ∈ A, p$ \\ existential quantification} &
\shortstack{$Σ_{x : A}B(x)$ \\ dependent sum type} \\[7pt]
\end{tabular}
\end{center}
#+end_figure

#+label: curry-howard
#+CAPTION: Curry-Howard correspondence
#+ATTR_LaTeX: :options [!htpb]
#+begin_figure latex
\begin{center}
\begin{tabular}{c|c}
\shortstack{
\AxiomC{}
\UnaryInfC{$Γ₁, α, Γ₂ ⊢ α$}
\DisplayProof \\ axiom} &
\shortstack{
\AxiomC{}
\UnaryInfC{$Γ₁, x : α, Γ₂ ⊢ x : α$}
\DisplayProof \\ variable} \\[7pt]

\shortstack{
\AxiomC{$Γ, α ⊢ β$}
\UnaryInfC{$Γ ⊢ α → β$}
\DisplayProof \\ implication introduction} &
\shortstack{
\AxiomC{$Γ, x : α ⊢ t : β$}
\UnaryInfC{$Γ ⊢ λx. t: α → β$}
\DisplayProof \\ abstraction} \\[7pt]

\shortstack{
\AxiomC{$Γ ⊢ α → β$}
\AxiomC{$Γ ⊢ α$}
\BinaryInfC{$Γ ⊢ β$}
\DisplayProof \\ modus ponens} &
\shortstack{
\AxiomC{$Γ ⊢ t : α → β$}
\AxiomC{$Γ ⊢ u : α$}
\BinaryInfC{$Γ ⊢ t u : β$}
\DisplayProof \\ application} \\[7pt]
\end{tabular}
\end{center}
#+end_figure

** λ calculus
The untyped lambda calculus is a simple language consisting of just three kinds
of forms: variables, function application, and abstraction. [...]

Introuced in XXXX, originally intended as a vehicle to model computatoinal
science, since then used fo modelling XYZ

Several types of notation, Church, Curry, de Bruijn, ... We will use Church.

[[todo:Why λ-calculus, what is it (2 para)]]

[[todo:Describe syntax (1 para)]]

#+CAPTION: Untyped λ-calculus
#+ATTR_LaTeX: :options [!htpb]
#+begin_figure latex
\[\begin{array}{ccll}
e & ::= & x            & \text{variable} \\
  & |   & e_1~e_2      & \text{application} \\
  & |   & \lambda x. e & \text{abstraction}
\end{array}\]
#+end_figure

[[todo:Examples - Church encoding (three columns, trivial examples)]]
** λ→ calculus
[[todo:Typed λ-calculus + syntax (3 para)]]

The simply-typed λ-calculus introduces the concept of types, which have separate
syntax and semantics from the language of terms. The language of terms is
extended with a fourth kind of term, type annotation. The language of types is
the universal or base type, and an arrow from between types.

[[todo:This language is not turing-complete, must terminate]]

#+CAPTION: Simply typed lambda calculus
#+ATTR_LaTeX: :options [!htpb]
#+begin_figure latex
\[\begin{array}{ccll}
e & ::= & x     & \text{variable} \\
  & |   & e_1~e_2 & \text{application} \\
  & |   & λ x. e & \text{abstraction} \\
  & |   & x:τ     & \text{annotation}
\end{array}\]
\[\begin{array}{ccll}
\tau & ::= & α           & \text{base type} \\
     & |   & τ → τ' & \text{composite type}
\end{array}\]
#+end_figure

[[todo:What are derivation rules, describe the symbols (1 para)]]

\missingfigure{Write out the simplest derivation rules (5-6 items, half a page)}

[[todo:Mention explicit substitution, that in the above it is just a part of the metatheory (para)]]

\missingfigure{Write out explicit substitution example (1 figure)}

[[todo:Typing rules - same as the above, just describe types (1 short para)]]

[[todo:Close the above, motivating example what is it good for? (1 para + 1 figure)]]
** Lambda cube
[[todo:Brief lambda cube intro show what's out there (3 para)]]

\missingfigure{Lambda cube}

[[todo:Derivation rules added by the lambda cube + options, 2 columns]]

[[todo:In the rest of the thesis we will work with λω (1 para)]]
** Extensions
[[todo:Intro - comparison table with logic/tt, Pi/Sigma, forall/exists, ... (3 paras)]]

*** Π-types
dependent product type, dependent function type

[[todo:generalization of the function space (2 paras)]]

$Π(n:ℕ). Vec(ℝ, n)$ vs $Π(n:ℕ). ℝ$

[[todo:grammar, specific example (2 figures)]]

Allows inductive types, eliminators, Church

*** Σ-types
dependent sum type, dependent pair type

[[todo:Dependent tuple/product/sum (2 para)]]

$(a, b) : Σ_{x:A}. B(x)$

[[todo:grammar, specific example (2 figures)]]

Allows datatypes/records, Cat + Functor?

*** μ-types
[[todo:Recursive type, box/unbox (2 paras)]]

[[todo:Type-level recursion, fixpoint, grammar (2 paras)]]

\missingfigure{Motivating example - from PiSigma?}

*** Type in Type
[[todo:Universes, what is the hierarchy used for (2 para)]]

[[todo:Weakening (cite PiSigma) but practical simplification (2 para)]]

\missingfigure{Counter-example with multiple universes}

*** Implicit arguments
[[todo:In brief - arguments that the compiler will fill in for us, type arguments
Agda et al. have it. (2 paras)]]

[[todo:Figure - implicit/braced arguments of an ~id~]]

** Operations
*** Reduction
[[todo:α, β, η, ... (3 items)]]

[[todo:Same for all of the calculi (right?)]]

- Equivalences:
  - α-equivalence = structural, allows α-renaming
  - β-equivalence = allows β-reduction (function application step) ==
    α-equivalence of β-normal forms
  - η-equivalence = allows η-reduction (f ≡ λx. f x)

[[todo:Equality, equivalence, structural, nominal (2 para)]]

*** Normal forms
[[todo:Normal forms - nf, hnf, whnf (3 items, describe)]]

\missingfigure{Show off a normal form derivation - which operation is used when}

*** Evaluation models
[[todo:Evaluation models - CBN, CBV, CBPV (3 paras, 1 figure)]]

Call-by-need - a computation happens in the place where a value is used, and not
created (~const 5 expensiveFunc~ doesn't /force/ the expensive computation)

Call-by-value - a computation happens immediately, whenever a value is created
(in the previous example both 5 and ~expensiveFunc~ would first be evaluated
before calling ~const~)

Call-by-push-value - formalism subsuming both (by means of a translation
strategy), defining a single evaluation order in terms of operations with a
stack - two additional operators, *distinguishes values and computations*
(including dependent types, in the original 1999 paper), operator $U$ (delay)
creating a /thunk/, and operator $F$ (force) that forces the computation of a
delayed value/thunk.

#+CAPTION: Call-by-push-value values and computations
#+begin_figure latex
\[\begin{array}{ccll}
A & ::= & U B | Σ(i∈I)A_i | 1 | A × A \\
B & ::= & F A | Π(i∈I)B_i | A → B
\end{array}\]
#+end_figure

- value of type $UB$ is a thunk producing a value of type $B$
- Σ is a pair (tag, Value)
- type A × A' is a value of type (V, V')
- type 1 is a 0-tuple
- computation of type $FA$ produces a value of type A
- computation Π pops a tag $i$ from operand stack, then is a computation of type $B$
- computation A → B pops a value of type A, then behaves as type B
[[todo:https://www.cs.bham.ac.uk/~pbl/papers/tlca99.pdf]]

** Type checking, inference, elaboration
- type checking ( = determining whether a program is well-typed)
- type inference ( = the process of obtaining the type of an expression from its
  parts or implicits; Hindley-Milner is well-known)
- elaboration = convert a partially specified expression into a complete,
  type-correct form (http://leodemoura.github.io/files/elaboration.pdf)

[[todo:Describe the nomenclature (3 para)]]

*** Bidirectional typing
Bidirectional typing (https://www.cl.cam.ac.uk/~nk480/bidir-survey.pdf) = now
standard approach, combines type-checking and inference, simpler to implement
even if inference is not required

[[todo:Intro, motivation, list alternatives, pros and cons (2 para)]]

[[todo:Sketch the process? Write out lambda + app rules? (half a page)]]

*** Conversion checking
Conversion checking = type (or expression) equivalence checking, includes
evaluation (NbE = full comparison of normal forms), checking equivalence "as
described in the previous section"

[[todo:Describe motivation for NbE, the process, what is a neutral, eval/quote (3 paras)]]

\missingfigure{neutral terms}

*** Glued values

[[todo:Performance optimization, unfolding choice]]
** Our syntax
[[todo:Putting this all together, we get...]]

#+begin_src antlr
  FILE
      : STMT (STMTEND STMT)* ;
  STMT
      : "{-#" PRAGMA "#-}"
      | ID ":" EXPR
      | ID ":" EXPR "=" EXPR
      | ID "=" EXPR
      | COMMAND EXPR
      ;
  EXPR
      : "let" ID ":" EXPR "=" EXPR "in" EXPR
      | "λ" LAM_BINDER "." EXPR
      | PI_BINDER+ "→" EXPR
      | ATOM ARG*
      ;
  LAM_BINDER
      : ID | "_"
      | "{" (ID | "_") "}"
      ;
  PI_BINDER
      : ATOM ARG*
      | "(" ID+ ":" EXPR ")"
      | "{" ID+ ":" EXPR "}"
      ;
  ARG
      : ATOM
      | "{" ID ("=" TERM)? "}"
      ;
  ATOM
      : "[" ID "|" FOREIGN "|" TERM "]"
      | EXPR "×" EXPR
      | "(" EXPR ("," EXPR)+ ")"
      | "(" EXPR ")"
      | ID "." ID
      | ID
      | NAT
      | "*"
      | "_"
      ;
  STMTEND : ("\n" | ";")+ ;
  ID : [a-zA-Z] [a-zA-Z0-9] ;
  SKIP : [ \t] | "--" [^\r\n]* | "{-" [^#] .* "-}" ;
  // pragma, command discussed in text
#+end_src

[[todo:The complete inference and evaluation rules are in Appendix X]]

This looks nice: https://homepages.inf.ed.ac.uk/wadler/papers/mpc-2019/unraveling.pdf

[[todo:Extend with builtins/primitives/wired-in types, FFI with truffle, hole?]]
* Language implementation: Montuno
  :PROPERTIES:
  :CUSTOM_ID: interpreter
  :END:
** Introduction
We will first create an interpreter for Montuno as specified in the assignment,
and also because evaluation and elaboration algorithms from the literature are
quite naturally translated to a functional-style program, which is not really
possible in Truffle, the target implementation, where annotated classes are the
main building block.

The Truffle implementation, which we will see in the following chapters, has a
much higher conceptual overhead as we will need to care about low-level
implementation details, e.g. implementing the actual function calls. In this
interpreter, though, we will simply use the features of our host language.

We will use Kotlin as our language of choice, as it is a middle ground between
plain Java and functional JVM-based languages like Scala or Clojure. While the
main target language of Truffle is Java, Kotlin also supports class and object
annotations on which the Truffle DSL is based [fn:1]. Functional style, on the other
hand, makes our implementation of the algorithms simpler and more
concise [fn:6].

The choice of the platform (JVM) and the language (Kotlin) also clarifies the
choice of supporting libraries. In general, we are focused on the algorithmic
part of the implementation, and not on speed or conciseness, which means that we
can simplify our choices by using the most widely used libraries:
- Gradle as the build system,
- JUnit as the testing framework,
- ANTLR as the parser generator,
- JLine as the command-line interface library.

Truffle authors recommend against using many external libraries in the
internals of the interpreter, as the techniques the libraries use may not
work well with Truffle. This means that we need to design our own supporting
data structures based on the primitive structures provided directly by Java.

The overall program flow of our interpreter will not be unusual for
interpreters:
- receive some input from the user, either from a file, or from an interactive
  prompt;
- parse the textual content into an intermediate representation;
- if we are in batch mode, sequentially process the top-level statements,
  accumulating the entries processed so far into a global table of names. Given
  a top-level definition containing a type and a term, we will:
  - /check/ that the type provided has the kind ⋆
  - /infer/ the type of the term
  - /check/ whether the provided and inferred types can be unified
  - /simplify/ both the type and term, and store them into the global name-table.
- if we are in interactive mode, normalize the expression given and print it
  out;
- if we encounter an ~elaborate~ or ~normalize~ command, print out the full or
  normalized form of the expression.

#+label: main-sigs
#+caption: Simplified signatures of the principal functions
#+begin_src kotlin
fun infer(pre: PreTerm): Pair<Term, Val>
fun check(pre: PreTerm, wanted: Val): Term
fun eval(term: Term): Val
fun quote(value: Val): Term
#+end_src

Throughout this process, we will evaluate and quote ~Terms~ from and into
~Values~. The signatures of the most important functions are mentioned in Listing
ref:main-sigs. These are the main parts of our interpreter--and the main
component of evaluation is function application, which is what we will focus on
first.

** Representing functions
The main requirement for a λ calculus runtime system is fast function
evaluation, which is where we will start.

[[todo:Three constructs - lam, app, var (1 para)]]

A closure consists of an unapplied function, and an environment that /closes over/
the free variables used in the function.

HOAS is a specific representation of closures, wherein the function is
represented as function in the host language, using the host language's support
for closing over free variables.

While representing functions using HOAS produces very readable code and in some
cases e.g. on GHC produces code an order faster than using explicit closures,
this is not possible in for us, where function calls need to be nodes in the
program graph and therefore objects, as we will see in Chapter [[ref:truffle]], so
we will represent closures using explicit closures in the pure interpreter as well.

[[todo:Cite Kovacs benchmarks where HOAS on GHC wins]]

[[todo:HOAS vs Closure code example]]

[[todo:Single versus multi-argument (1 para)]]

[[todo:Resulting data structure (TLam, VLam, VCl) (1 figure)]]

** Representing environments and variables
The way we define functions leads us to environments (Γ), which is a context
where we will look for variables.

[[todo:Named versus nameless representation (2 paras, example)]]

As we saw in [[inline:Where's WHNF?]], evaluation in λ-calculus is defined
in terms of α- and β-reductions. Verifying expression equivalence also uses
variable renaming. However, traversing the entire expression in the course of
function application and applying substitution is not efficient and there are
several alternative ways.

[[todo:Find de Bruijn motivating example]]
[[todo:Find side-by-side dB examples]]

All three of them mentioned here work by replacing variable names with their
indices in a variable stack, informally /counting the lambdas/.

- de Bruijn indices - well-known, starting from the current innermost λ
- de Bruijn levels - less well-known, starting from the outer lambda, stable
- locally-nameless - dB indices for bound variables,, names for free variables

Given an environment stack, indices count from the start of the stack, levels
count form the bottom. Two ways of indexing the environment, indices useful for
a stable context, levels without a stable context.

We use both, indices when quoting evaluated values.

[[todo:Environments - arrays, cons lists, stack, mutable/immutable (2 paras)]]

[[todo:Snippet of code - binding a variable, looking up a variable (1 figure)]]

** Evaluation algorithm
[[todo:Equals (W) (H) NF, Single-variable functions - trivial transcription (1 para)]]

[[todo:What are our neutrals (1 para?)]]

[[todo:Eval Quote, very briefly (2 paras)]]

[[todo:Extensions (Pi, Sigma, Eta) (2 paras)]]

** Parser
Lexical and syntactic analysis is not the focus of this work, so simply I chose
the most prevalent parsing library in Java-based languages which seems to be
ANTLR [fn:2]. It comes with a large library of languages and protocols from
which to take inspiration [fn:3], so creating a parser for our language was not
hard, despite me only having prior experience with parser combinator libraries
and not parser generators.

ANTLR provides has two recommended ways of consuming the result of parsing using
classical design patterns: a listener and a visitor. I have used neither as they
were needlessly verbose or limiting [fn:4].

I have instead implemented a custom recursive-descent AST transformation that
converts ~ParseContexts~ created by ANTLR into our ~Presyntax~ data type that we can
see in Listing ref:presyntax. This is actually a slightly simplified version
compared to the original as I have omitted the portion that tracks which
position of the input file corresponds to each subtree, which is later used for
type error reporting.

#+label: presyntax
#+caption: The ~Presyntax~ data type
#+begin_src kotlin
  sealed class TopLevel
  data class RDecl(val n: String, val type: PreTerm) : TopLevel()
  data class RDefn(val n: String, val type: PreTerm?, val term: PreTerm) : TopLevel()
  data class RTerm(val cmd: Command, val term: PreTerm) : TopLevel()

  sealed class PreTerm

  data class RVar(val n: String) : PreTerm()
  data class RNat(val n: Int) : PreTerm()
  object RU : PreTerm()
  object RHole : PreTerm()

  data class RApp(
      val icit: Icit, val rator: PreTerm, val rand: PreTerm
  ) : PreTerm()
  data class RLam(
      val name: String, val icit: Icit, val body: PreTerm
  ) : PreTerm()
  data class RFun(
      val domain: PreTerm, val codomain: PreTerm
  ) : PreTerm()
  data class RPi(
      val name: String, val icit: Icit, val type: PreTerm, val body: PreTerm,
  ) : PreTerm()
  data class RLet(
      val name: String, val type: PreTerm, val defn: PreTerm, val body: PreTerm,
  ) : PreTerm()
  data class RForeign(
      val lang: String, val eval: String, val type: PreTerm
  ) : PreTerm()
#+end_src

The data structures are represented in a way that Kotlin
recommends [fn:5]--using /data classes/. These are classes whose primary purpose
is to hold data, so called Data Transfer Objects (DTOs), and that have special
language support in Kotlin. We have a ~TopLevel~ class with three children that
represent: definitions that assign a value to a name, optionally with a type;
declarations (sometimes called postulates) that only assign a type to a name;
and commands like ~%normalize~ that we will see in later sections.

Their ancestor is a ~sealed class~ which tells the compiler there will only ever
be the subclasses defined in this module. In particular, this means that in any
pattern match on the type of a ~TopLevel~ object we only ever need to handle three
cases.

The remaining classes generally map to elements from the language syntax as
specified in the previous section: a λ abstraction, function application, Π
abstraction, a ~let~ local binding. The ~RFun~ class is a specialization of the ~RPi~
class that binds an unnamed, non-dependent type argument.

We will not use these classes immediately--only in Chapter ref:elaboration will
we implement a way to convert this pre-syntax into correct-by-construction
~Syntax~ objects which can be evaluated and quoted.

** Type checking and elaboration
   :PROPERTIES:
   :CUSTOM_ID: elaboration
   :END:

[[todo:Approach - infer, check + eval/quote used, global contexts (2 paras)]]

[[todo:Metavariables and holes, sequential processing (1 para)]]

[[todo:How do we do unification (2 paras + 1 figure)]]

[[todo:Glued values - optimistic optimization (3 paras + 1 figure)]]

** User interface
REPL

[[todo:This is our UI (plus batch CLI launcher), what does JLine require (2 paras)]]

[[todo:State keeping - load, reload + querying the global state (1 para)]]

commands:
- ~:l~ create a NameTable
- ~:r~ recreate a NameTable
- ~:t~ inferVar, print unfolded
- ~:nt~ inferVar, print glued
- ~:n~ inferVar type, gQuote term, show
- ~:e~ print elaboration output including all metas

\missingfigure{Example CLI session}

** Results
[[todo:Quick look at everything that this toy can do (2-3 examples?)]]

Evaluation, simplification, elaboration with holes, unification using eqRefl

Error reporting

* Designing elaboration and evaluation benchmarks
  :PROPERTIES:
  :CUSTOM_ID: bench
  :END:
** Tasks
equivalent programs for a few dependently-typed languages

[[todo: Agda, Idris + cooltt, smalltt, redtt, cubicaltt, ... (2 paras)]]

[[todo:Also mainstream functional languages - Clojure, GHC, ... (1 para)]]

[[todo:We're interested in asymptotics - effects of JIT, not constant factors (1 para)]]

- evaluation, normal form, simplification
- elaboration, filling holes
- Are there known problems/benchmarks?

** Specific tasks
 - Nats - large type elaboration, call-by-need test
 - Nats - type-level calculation
 - Nats - value-level calculation
 - Nats - equality/forcing
 - Functions - nested function elaboration, implicits
 - Functions - embedded STLC?
 - pairs - large type elaboration, call-by-need test
 - pairs - nested accessors

** Specific languages
 SmallTT, Coq, Agda, GHC - for comparison

memory profile from GHC's RTS for agda/idris/smalltt (+RTS -p)
(what about coq? - https://github.com/coq/coq/blob/master/dev/doc/profiling.txt)
Graal's default memory profiler

hyperfine to benchmark - measures speed
(what about ~prof~?)
memory profile from stderr output

  - from SmallTT project, from Idris project
  - memory usage (curve)
  - compilation speed (type-heavy test)
  - evaluation speed (compute-heavy test)

** Results: a starting point
[[todo:Results]]

* Adding JIT compilation to Montuno: MontunoTruffle
  :PROPERTIES:
  :CUSTOM_ID: truffle
  :END:
** Just-in-time compilation
[[todo:Mention the general JIT theory - classical, tracing, rewriting]]

[[todo:Mention partial evaluation, Futamora projections (2 paras + 3 items + example)]]

[[todo:Where is the partial evaluation in Graal (1 para)]]

Graal compiles "hot code" to machine code by partially evaluating it. Partial
evaluation is TODO (Futamora). In particular, in dynamic languages it helps to
eliminate dynamic dispatch / megamorphic call overhead.

** GraalVM and the Truffle Framework
*** GraalVM
[[todo:Re-read, simplify (what is it, why we want to use it, what specifically do we use?)]]

*GraalVM* is an Oracle research project that was originally created as a
replacement for the HotSpot virtual machine written in C++. [[inline:Cite Oracle]]
It has since expanded to include other features novel to the Java world.

#+LABEL: graal
#+ATTR_LaTeX: :placement [!htb]
#+CAPTION: GraalVM and Truffle (source: oracle.com)
[[./img/graalvm.jpg]]

The project consists of several components which can be seen in ref:graal, the
main components being the following ones:

*Graal* is an optimizing just-in-time compiler based on partial evaluation. Graal
uses the JVM Compiler Interface which allows the main JVM to offload compilation
to external Java code. It can also use C1 (the old JVM JIT) which implies tiered
compilation (...disabled with ~-XX:-TieredCompilation~). When using the GraalVM,
the only JVMCI-compatible compiler is Graal, so automatically
used. [[inline:Simplify language]]

*SubstrateVM* is an alternative virtual machine that uses aggressive ahead-of-time
compilation [[inline:Cite SubstrateVM]] of Java bytecode into a standalone
executables, a so-called /Native Image. The project aims to guarantee fast
start-up times, relatively small binary files, and low memory footprint--as
opposed to slow start-up times due to JIT compilation and large memory usage
common to JVM-based languages.

*Truffle* (and *Truffle DSL*) is a language implementation framework, a set of
libraries that expose the internals of the Graal compiler to interpreter-based
language implementations. It promises that its users only need to write an
interpreter with a few framework-specific annotations in order to automatically
gain:
- access to all optimizations available on the Java Virtual Machine
- debugger support
- multi-language (/polyglot/) support between any other Java-based or
  Truffle-based languages (currently JavaScript, Python, Ruby, R, C, C++, WebAssembly)
- the ability to gradually add optimizations like program graph rewriting,
  node specializations, or inline instruction caching

[[todo:Images from file:///home/inuits/Downloads/graalvm-190721074229.pdf]]

[[todo:images from https://www.slideshare.net/jexp/polyglot-applications-with-graalvm]]

GraalVM is also intended to allow creating /polyglot applications/ easily,
applications that have their parts written in different languages. It is
therefore easy to e.g. call R to create visualizations for the results of a
Python program, or to call any Truffle language from Java.

This seems like a good middle ground between spending large amounts of time on
an optimized compiler, and just specifying the semantics of a program in an
interpreter that, however, will likely not run quickly.

While GraalVM/Truffle is open-source and released under GPL v2, an
enterprise edition that claims large performance improvements is released
commercially.

#+COMMENT: https://chrisseaton.com/truffleruby/jokerconf17/ (done)

Graal-compiled code still runs on the original HotSpot VM (written in C++)

Graal is also a graph optimizer, which takes the data-flow and instruction-flow
of the original program and is able to reorder them to improve performance. It
does what the original HotSpot VM would do, optimize JVM bytecode by reordering,
pre-compiling, or entirely rewriting instructions.

- Canonicization: constant folding, simplification
- Global value numbering: prevents same code from being executed multiple times
- Lock coarsening: simplifies ~synchronized~ calls
- Register allocation: data-flow equals the registers required, optimize
- Scheduling: instruction-flow implies instruction order

Graal by itself is just better HotSpot, but there are other technologies in the
mix. SubstrateVM is an ahead-of-time compiler for Java which takes Java bytecode
and compiles is into a single binary, including the Graal runtime, pre-compiling
application code to greatly reduce warm up times (that are otherwise shared by
all JIT compilers).

In the ideal world, what GraalVM can do with the code by itself would be enough,
if that is not sufficient then we can add specializations, caches, custom
typecasts, ... We can also hand-tune the code, adding our hand-generated
bytecode into the mix.

*** Truffle
[[todo:Introduce Truffle specifically, polyglot, graph (de)optimization (3 paras + code sample)]]

[[todo:It really is fast enough - FastR, Python benchmarks (2 paras + graph)]]

** Features common to Truffle languages
*** General features
[[todo:What does a typical Truffle language look like? (1 para)]]

[[todo:"We can look at a piece of Ruby code as a graph" (1 para + graph)]]

Another visualization option: Seafoam

#+COMMENT: https://norswap.com/truffle-tutorial/ (done)

[[todo:Introduce the canonical example - Literal and addition, trivial execute
fn (1 para, 1 figure)]]

*** Type specialization

[[todo:Type system]]

Basic case is type specialization - when an addition node only encounters
integers, there is no need to generate machine code for floats, doubles, or
operator overloads - only verified by fast checks. When these fail, the node is
de-optimized, and eventually re-compiled again.

Inline caching for e.g. method lookups, virtual method calls are typically only
ever invoked on a single class, which can be cached, and the dispatch node can
be specialized, perhaps even inline the operation.

Specializations are general, though, and nodes can go be specialized on
arbitrary conditions, using custom assumptions and /compilation final/ values. In
general, node states form a directed acyclic graph - "a node can ever become more
general".

Using a graph visualizer, we can look at this process on a simple example
commonly used to demonstrate this part of the Truffle framework: the ~+~
operation.

#+label: add-lang
#+begin_src kotlin
  abstract class LangNode : Node() {
      abstract fun execute(frame: VirtualFrame): Any
  }
  class IntLiteralNode(private val value: Long) : LangNode() {
      override fun execute(frame: VirtualFrame): Any = value
  }
  abstract class AddNode(
      @Child val left: LangNode,
      @Child val right: LangNode,
  ) : LangNode() {
      @Specialization
      fun addInt(left: Int, right: Int): Int = left + right
      @Specialization
      fun addString(left: String, right: String): String = left + right
      @Fallback
      fun typeError(left: Any?, right: Any?): Unit = throw TruffleException("type error")
  }
#+end_src

The Truffle framework is said to be a domain-specific language, which in this
case means a library, a set of annotations, and a code generator. This code
generator finds classes that inherit from the ~Node~ class and generates, among
others, the logic behind switching specializations.

The program graph is formed from a tree of Truffle ~Nodes~ from which we derive
our language-specific base class, ~LangNode~ in this case. We define two classes
that inherit from this class, one representing integer literals, and one for the ~+~
operator.

The abstract method ~execute~ in ~LangNode~ is the evaluation of this node. It takes
a ~VirtualFrame~, which represents a stack frame, and its return value is also the
return value of the node. In addition, methods starting with ~execute~ are special
in Truffle. Truffle will pick the most appropriate one based on return type
(with ~Any~ being the most general) and parameters.

In ~IntLiteralNode~ we directly override the method ~execute~, as there is only one
possible implementation. In ~AddNode~, however, we keep the class abstract and
don't implement ~execute~, and instead rely on Truffle to generate the appropriate
specialization logic.

Truffle will decide between the specializations based on parameter types, and on
user-provided guards (we'll see further). Fallback specialization matches in
cases where no other one does. Names are irrelevant.

(Maybe show generated code?) Active an inactive specializations: can be multiple
active, execute method is based on state first, and only then on type checks -
smaller and possibly better optimized result. If no specialization matches, then
fall through to ~executeAndSpecialize~ which invalidates any currently compiled
using ~CompilerDirectives.transferToInterpreterAndInvalidate~ and sets state bits
for newly activated specializations.

(@Specialization(Replaces=[""]))


How to run? Need to wrap in a ~RootNode~, which represents executable things like
methods, functions and programs. Then create a ~CallTarget~ using
~Truffle.getRuntime().createCallTarget(root)~. Truffle uses CallTargets to record,
among others, how often a particular graph is called, and when to compile
it. Also it creates a VirtualFrame for this call target out of the provided
arguments.

IGV receives JIT compilation output - shows the Graal graphs produced during
optimization. Compilation is only triggered after a certain threshold of calls,
so we need to run a call target more than just once.

todo:Graal-graph

Another option: a CountNode (public int counter; execute = counter++). Green ==
state, grey is floating (not flow dependent), blue lines represent data flow
(data dependencies), red means control flow (order of operations)

*** Object storage
- OSM (Object Storage Model) - Frame (~typed HashMap)
- VirtualFrame - virtual/optimizable stack frame, "a function's/program's scope"
- MaterializedFrame - VirtualFrame in a specific form, not optimizable, can be
  stored in a ValueType

FrameDescriptor - shape of a frame
FrameSlot
FrameSlotKind

VirtualFrame - can be optimized, reordered

MaterializedFrame - an explicit Java Object on the heap, created from a
VirtualFrame by calling ~frame.materialize()~

*** Dispatch
[[todo:dispatchNode, frames, argument passing, ExecutableNode, RootNode, CallTarget]]

- RootNode - can be made into a CallTarget. is at the root of a graph, "starting
  point of a function/program/builtin", "callable AST"

*** Caching
[[todo:@Cached()]]

*** Polyglot
[[todo:ValueTypes, InteropLibrary]]

*** Structure
[[todo:Common components - Launcher, LanguageRegistration, Nodes, Values, REPL (5 items)]]

Engine, Context, TruffleLanguage, Instrument

#+begin_src text
  N: unbounded
  P: N for exclusive, 1 for shared context policy
  L: number of installed languages
  I: number of installed instruments

  - 1 : Host VM Processs
   - N : Engine
     - N : Context
       - L : Language Context
     - P * L : TruffleLanguage
     - I : Instrument
       - 1 : TruffleInstrument
#+end_src

** Functional Truffle languages
*** Truffled PureScript
#+COMMENT: https://github.com/slamdata/truffled-purescript/

Old project, but one of the only purely-functional Truffle languages.

Purescript is a derivative of Haskell, originally aimed at frontend
development. Specific to Purescript is eager evaluation order, so the Truffle
interpreter does not have to implement thunks/delayed evaluation.

Simple node system compared to other implementations:
- types are double and Closure (trivial wrapper around a RootCallTarget and a MaterializedFrame)
- VarExpr searches for a variable in all nested frames by string name
- Data objects are a HashMap
- ClosureNode materializes the entire current frame
- AppNode executes a closure, and calls the resulting function with a { frame, arg }
- CallRootNode copies its single argument to the frame
- IR codegen creates RootNodes for all top-level declarations, evaluates them,
  stores the result, saves them to a module Frame
- Abstraction == single-argument closure

*** Mumbler
An implementation of a Lisp

*** FastR
One of the larger Truffle languages

Replacement for GNU R, which was "made for statistics, not performance"

Faster without Fortran than with (no native FFI boundary, allows Graal to
optimize through it)

Interop with Python, in particular - scipy + R plots

#+begin_src kotlin
val ctx = Context.newBuilder("R").allowAllAccess(true).build();
ctx.eval("R", "sum").execute(arrayOf<Int>(1,2,3));
#+end_src

#+begin_src R
benchmark <- function(obj) {
    result <- 0L
    for (j in 1:100) {
       obj2 <- obj$objectFunction(obj)
       obj$intField <- as.integer(obj2$doubleField)
       for (i in 1:250) { result <- obj$intFunction(i, obj$intField) }
    }
    result
}
benchmark(.jnew("RJavaBench"))
#+end_src

Special features:
- Promises (call-by-need + eager promises)

*** Cadenza

- FrameBuilder - specialized MaterializedFrame
- Closure - rather convoluted-looking code

Generating function application looks like:
- TLam - creates Root, ClosureBody, captures to arr, arg/envPreamble
- Lam - creates Closure, BuilderFrame from all captures in frame
- Closure - is a ValueType, contains ClosureRootNode
- ClosureRootNode - creates a new VirtualFrame with subset of frame.arguments

*** Enso
A very late addition to this list, this is a project that originally rejected
Truffle (and dependent types in general, if I recall correctly) and used Haskell
instead. However, the project Luna was renamed to Enso, and rebuilt from scratch
using Truffle and Scala not long before my thesis deadline.

* Language implementation: MontunoTruffle
  :PROPERTIES:
  :CUSTOM_ID: jit-interpreter
  :END:
** Introduction
Truffle is not primarily aimed at statically-typed languages or functional
languages. Its most easily accessible benefits lie in speculative optimization
of dynamically typed code and inline caches, where generic object-oriented code
can be specialized to a specific value type. Statically-typed languages have a
lot more information regarding the values that will flow through a function, and
e.g. GHC has a specific /specialization/ compiler pass.

However, there is a lot of overlap between the static optimizations done by
e.g. GHC and runtime optimizations done by Graal. An example would be
unfolding/inlining, where the compiler needs to make a single decision of
whether to replace a call to a function with its definition -- a decision that
depends on the size of the definition, whether they are in the same module, and
other heuristics (inline:https://www.microsoft.com/en-us/research/wp-content/uploads/2002/07/inline.pdf). A Truffle interpreter would be able to postpone the decision
until execution time, when the definition could be inlined if the call happened
enough times.

[[todo:mental model = graph through which values flow, values may contain graphs]]

Its execution model is a tree of nodes where each node has a single operation
~execute~ with multiple specializations. The elaboration/evaluation algorithm from
the previous chapter, however, has several interleaved algorithms (infer, check,
evaluate, quote) that we first need to graft on to the Truffle execution model.

There are also several features that we require that are not a natural fit for
it, but where we can find inspiration in other Truffle languages. In particular,
lazy evaluation (FastR promises), partial function application (Enso), ???

We also have several options with regard to the depth of embedding: The most
natural fit for Truffle is term evaluation, where a term could be represented as
a value-level Term, and a CallTarget that produces its value with regard to the
current environment. We can also embed the bidirectional elaboration algorithm
itself, as a mixture of infer/check nodes.

[[todo:Move this elsewhere ↓]]

There are several concerns here:
- algorithmic improvement is asymptotic -- the better algorithm, the better we
  can optimize it
- Truffle's optimization is essentially only applicable to "hot code", code that
  runs many times, e.g. in a loop
- We need to freely switch between Term and Value representations using
  eval/quote

The representation is also quite different from the functional interpreter where
we've used functions and data classes, as in Truffle, all values and operations
need to be classes.

[[todo:Restructure, clarify]]
Specific changes:
- everything is a class, rewrite functions/operations as classes/nodes
- annotations everywhere
- function dispatch is totally different
- lazy values need to be different
- ???

** Parser
ParsingRequest/InlineParsingRequest

[[todo: need to perform elaboration inside a programRootNode (not while parsing)]]

[[todo:stmt;stmt;expr -> return a value]]

** Representing functions
As with the previous interpreter, we will start with function calls.

[[todo:dispatch, invoke, call Nodes, argument schema (copying), ?]]

[[todo:eta is TailCallException (2 para + example)]]

Passing arguments - the technical problem of copying arguments to a new stack
frame in the course of calling a function.

Despite almost entirely re-using the Enso implementation of function calls, with
the addition of implicit type parameters and without the feature of default
argument values,

I will nonetheless keep my previous analysis of calling conventions in
functional Truffle languages here, as it was an important part of designing an
Truffle interpreter and I spent not-insignificant amounts of time on it.

I have discovered Enso only a short while before finishing my thesis, and had to
incorporate the technologically-superior solution

Several parts of creating an AST for function calls:
- determining the position of arguments on the original stack - or evaluating
  and possibly forcing the arguments
- determining the argument's position on the stack frame of the function
- using this position in the process of inferring the new function call
- dispatch, invoke, call nodes???

** Representing environments and variables
[[todo:We need to use arrays, Collections are not recommended]]

[[todo:Frames and frame descriptors for local/global variable]]

[[todo:References, indices, uninitialized references]]

** Evaluation order
[[todo:We need to defer computations as late as possible - unused values that
will be eliminated (1 para)]]

[[todo:CBPV concepts, thunks with CallTargets (3 paras, example)]]

** Calling convention
[[todo:the need for the distinction - in languages with currying]]

[[todo:the eval/apply paper is a recipe for a stack-based implementation of
currying and helpful in our case when we need explicitly manage our stack via
Frames as opposed to the interpreter where we relied on the host language for
this functionality]]

cite:marlow04_fast_curry

push-enter - arguments are pushed onto the stack, the function then takes as
many as it requires

eval-apply - the caller sees the arity of the function and then decides whether
it is over-applied (evaluates the function and creates a continuation), appllied
exactly (EVAL), or under-applied (creates a PAP, a closure-like value)

[[todo:exactly describe the rules from eval-apply paper KNOWNCALL, EXACT, CALLK, PAP]]

[[todo:known application ( = known arity), unknown function]]

** Value types
[[todo:Data classes with call targets]]

[[todo:...depends on what will work]]

** Polyglot
[[todo:...depends on whether I'll do polyglot or not]]

** Implementation
[[todo:How to connect this together?]]

evaluation phases - translate to Code, run typecheck, run eval vs glued, ???

[[todo:How to edit the REPL]]

[[todo:Language registration in mx/gu]]

** Results
[[todo:Same evaluation as in previous section]]

[[todo:how fast are we now? Are there any space/time leaks?]]

[[todo:show graphs: id, const, const id; optimized graphs]]

* Optimizations: Making MontunoTruffle fast
   :PROPERTIES:
   :CUSTOM_ID: optimizations
   :END:
** Possible performance problem sources
[[todo:Reiterate JGross again]]

[[todo:how to find out whether X is relevant to us or not? How to prove the effect of JIT?]]

[[todo:Show asymptotes - binders, terms, sizes]]
[[todo:Show the graphs - large values, many iterations (warmup), sharing]]
** Possible optimizations
[[todo:What does Enso do?]]
[[todo:What can we do?]]

[[todo:Show before and afters for each optimization]]

** Optimizing function dispatch
[[todo:lambda merging, eta expansion]]

** Caching, sharing
[[todo:sharing common values, multiple references to the same object]]

** Common FP optimizations
[[todo:floating, inlining by hand]]

** Specializations
*** Truffle recommended optimizations
The optimization workflow recommended by the Truffle developers is as follows:
1. Run with a profiler to sample the application and identify responsible
   compilation units. Use a sampling delay (--cpusampler.Delay=MILLISECONDS) to
   only profile after warmup. See the Profiling guide.
2. Understand what is being compiled and look for deoptimizations. Methods that
   are listed to run mostly in the interpreter likely have a problem with
   deoptimization.
3. Simplify the code as much as possible where it still shows the performance
   problem.
4. Enable performance warnings and list boundary calls.
5. Dump the Graal graph of the responsible compilation unit and look at the
   phase After TruffleTier.
   1. Look at the Graal graphs at the phases After TruffleTier and After
      PartialEscape and check if it is what you would expect. If there are nodes
      there that you don't want to be there, think about how to guard against
      including them. If there are more complex nodes there than you want, think
      about how to add specialisations that generate simpler code. If there are
      nodes you think should be there in a benchmark that are not, think about
      how to make values dynamic so they are not optimized away.
6. Search for Invoke nodes in the Graal IR. Invoke nodes that are not
   representing guest language calls should be specialized away. This may not be
   always possible, e.g., if the method does I/O.
7. Search for control flow splits (red lines) and investigate whether they
   result from control flow caused by the guest application or are just
   artifacts from the language implementation. The latter should be avoided if
   possible.
8. Search for indirections in linear code (Load and LoadIndexed) and try to
   minimize the code. The less code that is on the hot-path the better.

---
[[todo:More on splitting!]]

- ~--engine.TraceCompilation~ prints a line for each method compilation
- ~--engine.TraceCompilationDetail~ prints a line for compilation queuing, start, and finish
- ~--engine.TraceCompilationAST~ prints the entire compiled AST
- ~--engine.TraceInlining~ prints inlining decision details
- ~--engine.TraceSplitting~ prints splitting decisions
- ~--engine.TraceTransferToInterpreter~ prints a stack trace for each explicit invalidation
- ~--engine.TracePerformanceWarnings=(call|instanceof|store|all)~
  - ~call~ prints when PE cannot inline a call
  - ~instanceof~ prints when PE cannot resolve virtual ~instanceof~ to a specific type
  - ~store~ prints when PE store location argument is not compilation final
- ~--engine.CompilationStatistics~ prints total compilation statistics
- ~--engine.CompilationStatisticDetails~ prints compilation histograms for each node
- ~--engine.TraceMethodExpansion=truffleTier~ prints a tree of all expanded Java methods
- ~--engine.TraceNodeExpansion=truffleTier~ prints a tree of all expanded Nodes
- ~--engine.MethodExpansionStatistics=truffleTier~ prints total Graal nodes produced by a method
- ~--engine.NodeExpansionStatistics=truffleTier~ also includes Graal specializations
- ~--engine.InstrumentBoundaries~ prints info about boundaries encountered (should be minimized)
- ~--engine.InstrumentBranches~ prints info about branch profiles
- ~--engine.SpecializationStatistics~ prints details about specializations performed
- ~--vm.XX:+TraceDeoptimization~ prints all deoptimizations
- ~--vm.XX:+TraceDeoptimizationDetails~ prints all deoptimizations with details

It is also possible to control what is being compiled, add details to IGV graphs
dumped, and print the exact assembly produced: see
https://github.com/oracle/graal/blob/master/truffle/docs/Optimizing.md.

*** How to debug specializations
*Specialization histogram:* If compiled with
~-Atruffle.dsl.GenerateSpecializationStatistics=true~ and executed with
~--engine.SpecializationHistogram~, Truffle DSL will compile the nodes in a
special way and show a table of the specializations performed during the execution of a
program.

Example shown at
https://github.com/oracle/graal/blob/master/truffle/docs/SpecializationHistogram.md,
maybe include the table?

*Slow path only:* If compiled with ~-Atruffle.dsl.GenerateSlowPathOnly=true~,
Truffle will only execute the last, most generic specialization, and will
ignore all fast path specializations.
** (Profiling)
[[todo:then, what tools to use to find the problems]]

*** Ideal Graph VIsualizer
A graphical program that serves to visualize the process of Truffle graph
optimization. When configured correctly, the IGV will receive the results of all
partial evaluations.

*** CPU Sampler
#+COMMENT: https://github.com/oracle/graal/blob/master/truffle/docs/Profiling.md

Running the language launcher with the options ~--cpusampler
--cpusampler.Delay=MILLISECONDS~ will start the CPU sampler. This tool serves to
profile the guest language (as opposed to the regular JDK Async Profiler which
will profile the entire process.

~--cpusampler.Delay~ helps to not include warm-up time in the results.

Using additional options (~--cpusampler --cpusampler.SampleInternal
--cpusampler.Mode=roots --cpusampler.Output=json~) and postprocessing the
generated JSON with an additional script we
can create a so-called flamegraph with the results of the sampling.

#+COMMENT: https://github.com/eregon/FlameGraph/blob/graalvm/stackcollapse-graalvm.rb

* Evaluation (and next work): Is $Montuno_{Truffle}$ fast enough?
  :PROPERTIES:
  :CUSTOM_ID: evaluation
  :END:
** Evaluation
[[todo:Total evaluations, performance matrix]]

[[todo:Compare asymptotics now, with Agda, ...]]
** Next work
FFI, tooling

More type extensions OR totality (as a proof assistent)

Finite types, universes, no type in type, HoTT, CoC

[[todo:Is this useful at all? What's the benefit for the world? (in evaluation)]]

next work: LF, techniques, extensions, real language

* Conclusion
We tried X to do Y. It went well and we fulfilled the assignment.

As a side effect, I produced a reference book for functional/dependent
language implementation.

Original goal was X, it grew to encompass Y, Z as well.

* (bibliography, start of appendix)                           :ignoreheading:

#+BEGIN_EXPORT latex
\makeatletter
\def\@openbib@code{\addcontentsline{toc}{chapter}{Bibliography}}
\makeatother
\begin{flushleft}
#+END_EXPORT

bibliographystyle:bibstyle
bibliography:bibliography.bib

#+BEGIN_EXPORT latex
\end{flushleft}
\iftwoside\cleardoublepage\fi
\appendix
\appendixpage
\iftwoside\cleardoublepage\fi
\startcontents[chapters]
% \setlength{\parskip}{0pt}
% \printcontents[chapters]{l}{0}{\setcounter{tocdepth}{2}}
% \setlength{\parskip}{0.5\bigskipamount}
\iftwoside\cleardoublepage\fi
#+END_EXPORT

* Contents of the attached data storage
...
* Grammar
#+include: "../montuno/src/main/antlr/montuno/Montuno.g4" src antlr -n

* Footnotes

[fn:8] FastR is between 50 to 85x faster than GNU R, depending on the source. cite:fumero17_jit_gpu

[fn:7] Unfortunately, there are no officially published benchmarks, but a number
of articles claim that TruffleRuby is 10-30x faster than the official C
implementation. cite:shopify2020

[fn:6]Kotlin authors claim 40% reduction in the number of lines of code, (from
https://kotlinlang.org/docs/faq.html)
[fn:5]https://kotlinlang.org/docs/idioms.html
[fn:4] In particular, ANTLR-provided visitors require that all return values
share a common super-class. Listeners don't allow return values and would
require explicit parse tree manipulation.
[fn:3]https://github.com/antlr/grammars-v4/
[fn:2]https://www.antlr.org/
[fn:1]Even though Kotlin seems not to be recommended by Truffle authors, there
are several languages implemented in it, which suggests there are no severe
problems.  "[...] and Kotlin might use abstractions that don't properly
partially evaluate." (from https://github.com/oracle/graal/issues/1228)
