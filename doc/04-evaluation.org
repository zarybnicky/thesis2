* (front matter)                                              :ignoreheading:
#+LANGUAGE: en
#+OPTIONS: texht:nil toc:nil author:nil ':t H:4 num:3
#+LATEX_CLASS: fitthesis
#+LATEX_CLASS_OPTIONS: [english,zadani,odsaz]
#+EXCLUDE_TAGS: noexport
#+BIND: org-latex-title-command ""
#+BIND: org-latex-prefer-user-labels t
#+BIND: org-latex-default-figure-position "htb"

#+latex: \setcounter{chapter}{4}
* Evaluation
  :PROPERTIES:
  :CUSTOM_ID: evaluation
  :END:
We now have two interpreters, one written in pure Kotlin that uses the general
JIT compilation provided by JVM, the other delegating term evaluation to
Truffle. It is now time to evaluate their performance on a number of both
elaboration and evaluation tasks. We will also briefly compare their performance
with the performance of state-of-the-art proof assistants on the same tasks.

The primary goal is to evaluate the general time and memory characteristics of
the systems and how they vary with the size of the term and on the number of
binders (λ- and Π-abstractions). For this purpose, we will construct a set of
benchmarks involving simple expressions, whose size we can easily vary.

A secondary goal is to investigate the costs associated with a runtime system
based on the JVM, and how they may be eliminated. We will also prepare a suite
of benchmarks by translating a number of test cases from common performance test
suites, and compare this system's performance with other functional languages.

** Workload
The evaluation workload will be split into two parts: elaboration tasks, that
test the combined performance of our infer/check bidirectional typing algorithm,
normalization-by-evaluation, and unification; and normalization tasks, that only
test the performance of normalization-by-evaluation. These benchmark tasks were
partially adapted from the work of András Kovács cite:smalltt, and partially
extrapolated from the evaluation part of Jason Gross's doctoral thesis
cite:gross21_performance.

*** Normalization
Both normalization and elaboration tasks need to involve terms that can be
easily made larger or smaller. A typical workload involves Church-encoded terms,
naturals in particular, as these involve λ-abstraction and application. They
will be tested in the first set of tasks: evaluation of a large natural number
to a unit value. These will be first evaluated on Church-encoded naturals, and
then on the built-in type /Nat/ that is backed by a Java integer.

#+label:norm
#+caption: Benchmark tasks: large Church naturals
#+attr_latex: :options [htb]
#+begin_figure latex
\begin{subfigure}[t]{.7\textwidth}\centering
\begin{minted}{text}
Nat : Unit = (N : Unit) → (N → N) → N → N
zero : Nat = λ N s z. z
succ : Nat → Nat = λ a N s z. s (a N s z)
mul : Nat → Nat → Nat = λ a b N s z. a N (b N s) z
forceNat : Nat → Unit = λn. n _ (λx.x) Unit
\end{minted}
\caption{Church-encoded naturals and \texttt{forceNat}}
\end{subfigure}
\begin{subfigure}[t]{.3\textwidth}\centering
\begin{minted}{text}
n10    = mul n2 n5
n20    = mul n2 n10
-- ...
n20M   = mul n2 n10M
\end{minted}
\caption{Large Church numbers}
\end{subfigure}
#+end_figure

*** Elaboration
Elaboration will test not only the NbE part of our system, but also type
inference and checking. We will use not only deeply nested function application
of Church-encoded terms, but also terms with large types: those that contain
many or deeply nested function arrows or Π-types. The first task is the
elaboration of a polymorphic /id/ function repeatedly applied to itself, as this
produces meta-variables whose solution doubles in size with each application.
The second task especially tests unification: the task is to unify two large
Church-encoded natural numbers, to check them for equivalence.

#+label:norm2
#+caption: Benchmark tasks: elaboration
#+attr_latex: :options [htb]
#+begin_figure latex
\begin{subfigure}[t]{.5\textwidth}\centering
\begin{minted}{text}
idTy : {A} → A → A
id : idTy = λ x. x
test : idTy = id id id [...] id
\end{minted}
\caption{Church-encoded naturals and \texttt{forceNat}}
\end{subfigure}
\begin{subfigure}[t]{.5\textwidth}\centering
\begin{minted}{text}
Eq : Nat -> Nat -> Unit
   = λ x y. (P : Nat → Unit) → P x → P y
x : Eq n20Mb n20M = \_ x.x
\end{minted}
\caption{Large Church numbers}
\end{subfigure}
#+end_figure

** Methodology
There are many ways how we can measure each language's performance on these
tasks. The main concern is that Montuno and MontunoTruffle are JIT-compiled
languages that need a significant amount of warm-up: the first iterations will take
significantly longer than the iterations that happen after warm-up, after all
code is JIT-optimized.

There are several options for eliminating the influence of JIT warm-up on
performance measurements: the first is to measure an "empty run", and simply
subtract the times from benchmarking runs. This eliminates JIT start-up times,
but does not eliminate the time required for warming-up of the user system
itself. For this reason, there are various tools for either statistical analysis
of the results that discard all but the stable-state performance, or that simply
measure a large number of iterations.

This, however, measures the performance of the system on a single computational
task. The user-visible delay between starting the program and seeing results is
what interests the users of a program, especially in an interpreter for a
programming language, that may need to run often, for a tight feedback loop
during program development.

For this reason, I have selected the option of measuring whole-system
performance including start-up and warm-up times, but with a large number of
iterations of the benchmark tasks, so their influence on the overall runtime is
also visible. Aside from measuring the time it takes to normalize or elaborate
an expression, we will also measure the peak memory usage using the system tool
~time -v~.

These benchmarks were run on my personal computer, a laptop with 32GBs of RAM
and a 16 core processor running at 1.4GHz (AMD Ryzen 7 PRO 4750U).  No other
programs were running on the machine at the time of benchmarking, to eliminate
external influences.

** Results
The measurements were performed using ten iterations of the program
(accomplished by adding a loop to the program code), as an average of five
measurements.

#+label:id-stress
#+caption:Results of evaluation on the \textit{id} stress test
#+attr_latex: :options[htb]
#+begin_figure latex
\centering
\begin{tikzpicture}
\begin{axis}[
    xlabel={Meta-variables (id function invocations)},
    ylabel={Time [s] / Memory [10s of kB]},
    xmin=0, xmax=700,
    ymin=0, ymax=80,
    legend pos=north east,
    ymajorgrids=true,
    grid style=dashed,
    scale=1.4
]
\addplot[color=blue, mark=square] coordinates {
(2,5.33) (100,7.1) (300,7.85) (500,8.5) (700,9.4)
};
\addlegendentry{Truffle/CPU}
\addplot[color=blue, mark=circle] coordinates {
(2,38) (100,44.3) (300,46.5) (500,53) (700,59)
};
\addlegendentry{Truffle/Memory}
\addplot[color=red, mark=square] coordinates {
(2,5.4) (100,6.3) (300,7.3) (500,8.7) (700,9.7)
};
\addlegendentry{Pure/CPU}
\addplot[color=red, mark=circle] coordinates {
(2,37) (100,43) (300,50) (500,51) (700,56)
};
\addlegendentry{Pure/Memory}

\end{axis}
\end{tikzpicture}
#+end_figure

Figure ref:id-stress shows the results of evaluating the system on the
$idStress$ benchmark, measuring the performance of solving meta-variables.  The
results are disappointing. Unfortunately, I have only started stress-testing the
system in the last parts of the implementation, otherwise the project might have
taken a different direction.

The original benchmarks in the SmallTT project cite:smalltt included comparison
of up to one million binders and not only 700, but the reason for not including
more than 700 is that the elaboration overflowed the stack, using too deep
recursive function calls. Overall, this shows how recursive descent tree
transformation algorithms are unsuitable for the Java Virtual Machine, and does
not say too much about the performance of the system overall.

Program warm-up (a trivial run that only checks the base $id$ definition) takes
over five seconds, going up to ten seconds for the largest run. The long warm-up
is to be expected from a JVM-based system. However, even compared to the two
years old benchmarks of systems like Agda or Coq from the SmallTT project, the
performance of these systems are extremely disappointing.

Rewriting recursive descent algorithms into stack based ones is not a small
undertaking, so instead of trying to push the deadline even further, I have
searched for possible causes. I have found another project of Kovács', a recent
one that compares the suitability of different platforms for elaboration
cite:kovacs_norm, and which has added JVM-based systems to its comparison two
months ago. In that evaluation, JVM-based platforms have just as disappointing
performance as my solution.

Instead of the many planned measurements and evaluations, I have instead
attempted to analyze possible performance bottlenecks: largely, they come down
to the capability of the JVM to handle recursive functions, which is a
long-standing issue with the JVM capabilities. Trivial optimizations did not
help, and non-trivial optimizations were out of my time range.

** Discussion
Instead of general programming platforms, languages with dependent types usually
use the platforms of functional languages, e.g., GHC Haskell and its Spineless
Tagless G-Machine (STG), OCaml and the Zinc abstract machine, or Idris 2 that
uses Chez Scheme.

These platforms are optimized for fast function calls, currying, tail-calls, and
non-eager (lazy) evaluation strategy, all of which needs to be emulated manually
on the JVM. I will follow with a brief list of optimizations that I have
attempted to apply on the Montuno interpreters:

- Replacing functions for tree transformations with object methods on the term
  and value nodes themselves. This halved the stack usage.
- Applying λ-merging to closures, introducing the concept of the arity of a
  closure: the body of a closure is not called until all arguments it expects
  are supplied, which applies mostly to nested ~VLam~ or ~VPi~ terms. This helped in
  some test cases, but not in the general case.
- Removing lazy evaluation. Surprisingly, this improved the performance of the
  system the most, as the overhead caused by wrapping and unwrapping values in
  closures was removed. This is an optimization that helps in general system
  performance, but its effects are often unpredictable.
- Unfolding recursive helper functions for processing linked lists of data into
  iterative ones, using ~for~ cycles. This helped slightly with stack usage, but
  not as much as I'd expected.

After looking for more optimization opportunities, I have found an analysis in
the Enso cite:enso system, where they have encountered the same problem, and
used an interesting work-around. Each thread in the JVM has a separate stack
space limit, so instead of working around the stack limit, they introduce a
heuristic that tracks stack usage [fn:1], and spawns a new thread with the
currently running computation.

Although in my research, I have primarily focused on the optimization
opportunities of Truffle and GraalVM, I have disregarded the optimizations
required for the system itself that is written Kotlin, and often directly used
the same style that I am used to from working in Haskell and similar languages.

** Next work
From my research, it seems that Truffle can bring interesting benefits to
programming language implementers, andn perhaps even for implementing the
runtime systems of dependent types, but not for the style of algorithms that are
used in dependent type elaboration without significantly more effort put into
their optimization and adaptation for the JVM platform.

I have started this thesis as a follow up to the Cadenza project
cite:kmett_2019 that asked whether Truffle can be useful to a simply-typed
λ-calculus implementation. Its results were slightly disappointing, but the
project's author suggested a follow-up project to me as a thesis topic likely to
produce a positive result. I will have to conclude from the performance of this
project that /cannot/ benefit a runtime system for dependent types, at least not
using the approach that I have taken.

When finishing my thesis, I have discovered that parallel to my work on this
thesis, the author has started a new project called TruffleSTG[fn:2], that attempts to
apply the knowledge from these two projects to another domain, that is even more
likely to benefit from Truffle. The STG is the abstract machine used by Haskell
and other languages that compile to Haskell. Instead of implementing the entire
elaboration system in Kotlin or another JVM-based language, TruffleSTG uses the
GHC Haskell machinery to process dependent types, and only uses Truffle as a
compiler. This takes the approach from my thesis to the extreme. Where I have
used Truffle as a backend that directly communicates with the elaboration
process, TruffleSTG communicates with GHC using GHC-WPC interface files, meaning
that it only attempts to act as a runtime system for Haskell and nothing more.

The language implemented in the course of this thesis has its limitations, the
user interface is incomplete, and the system likely has some bugs that would be
discovered when used on a larger scale, all of which would be good to fix and
publish this project as inspiration for future endeavors in this area.

However, I believe a very useful follow-up to this project would be a rigorous
evaluation of the different platforms that can be used for both functional
languages, and for language with dependent types, extending the benchmarks by
Kovacs cite:kovacs_norm, and standardizing them as a set of common elaboration
tasks, which can then be used to compare the performance of proof assistants and
systems with dependent types.

* Bibliography                                                :ignoreheading:
bibliographystyle:abbrv
bibliography:bibliography.bib
* Footnotes

[fn:2]TruffleSTG, https://github.com/acertain/trufflestg 

[fn:1] Source: https://enso.org/docs/developer/enso/runtime/unbounded-recursion.html 
