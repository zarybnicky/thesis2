* (front matter)                                              :ignoreheading:
#+LANGUAGE: en
#+OPTIONS: texht:nil toc:nil author:nil ':t H:4 num:3
#+LATEX_CLASS: fitthesis
#+LATEX_CLASS_OPTIONS: [english,zadani,odsaz]
#+EXCLUDE_TAGS: noexport
#+BIND: org-latex-title-command ""
#+BIND: org-latex-prefer-user-labels t
#+BIND: org-latex-default-figure-position "htb"

#+latex: \setcounter{chapter}{4}

* Evaluation
  :PROPERTIES:
  :CUSTOM_ID: evaluation
  :END:
We now have two interpreters, one written in pure Kotlin that uses the general
JIT compilation provided by JVM, the other delegating term evaluation to
Truffle. It is now time to evaluate their performance on a number of both
elaboration and evaluation tasks. We will also compare their performance with
the performance of state-of-the-art proof assistants on the same tasks.

The primary goal is to evaluate the general time and memory characteristics of
the systems and how they vary with the size of the term and on the number of
binders (λ- and Π-abstractions). For this purpose, we will construct a set of
benchmarks involving simple expressions, whose size we can easily vary.

A secondary goal is to investigate the costs associated with a runtime system
based on the JVM, and how they may be eliminated. We will also prepare a suite
of benchmarks by translating a number of test cases from common performance test
suites, and compare this system's performance with other functional languages.

** Subjects
For the comparison of elaboration and normalization, we will use
state-of-the-art proof assistants: I have chosen Coq, Agda, Idris, as they use a
dependent type theory at their core. These will be compared to both Montuno and
MontunoTruffle on both elaboration and normalization tasks.

**** Coq
Coq is an interactive theorem prover that has been popularized by the proof of
the four-color theorem cite:gonthier08_fourcolor. Coq is a tactics-based theorem
prover, meaning that the programmer will use a meta-language of various decision
procedures (tactics) instead of writing proofs manually. It uses the Calculus of
Inductive Constructions at its core that extends the Calculus of Constructions
(λΠω-calculus) introduced in Chapter ref:lambda. Its performance has been the
subject of research cite:gross14_coq_experience,gross21_performance, and we will
use some of the conclusions reached in later, when discussing optimizations in
Chapter ref:optimizations.

**** Agda
Agda is a dependently-typed programming language that can also be used as a
proof assistant. It uses a Haskell-like syntax, and its workflow is centered
around meta-variables and their gradual refinement in a dialog with the compiler
cite:norell08_agda_tutorial.

**** Idris
Idris is a total, dependently-typed programming language
cite:brady13_idris. Like Agda, it also uses a Haskell-like syntax and supports
an interactive workflow. Like Coq, proofs in Idris use a tactics
meta-language. Recently, a second version of the language was released as a
self-hosting language Idris 2; we will attempt to use both in our evaluation.

**** Cadenza
The project to which Montuno is a successor, Cadenza implements a simply-typed
λ-calculus cite:kmett_2019 using GraalVM and Truffle. It does not use
elaboration, but we can evaluate its performance on normalization tasks.

** Workload
The main evaluation workload will be split into two parts: elaboration tasks,
that test the combined performance of our infer/check bidirectional typing
algorithm, normalization-by-evaluation, and unification; and normalization
tasks, that only test the performance of normalization-by-evaluation. These
benchmark tasks were partially adapted from the work of András Kovács
cite:kovacs_norm, partially extrapolated from the evaluation part of Jason
Gross's doctoral thesis cite:gross21_performance.

The third, additional part is a number of computational tasks to evaluate the
time and memory performance of the Montuno interpreters. The tasks were mostly
adapted from the ~nofib~ benchmark suite cite:partain93_nofib that is used for
evaluating the performance of GHC, and from a suite of simple benchmark programs
cite:kostya comparing performance across 24 languages. The usual language
benchmarks, e.g. the "Benchmarks Game" were disregarded, as their implementation
would be too involved.

*** Normalization
Both normalization and elaboration tasks need to involve terms that can be
easily made larger or smaller. A typical workload involves Church-encoded terms,
naturals in particular, as these involve λ-abstraction and application. They
will be tested in the first set of tasks: evaluation of a large natural number
to a unit value, and calculating the Ackermann number followed by evaluating it
to a unit value. These will be first evaluated on Church-encoded naturals, and
then on the built-in type /Nat/ that is backed by a Java integer.

The second set of tasks is the evaluation of a term to a normal form. Where the
first two tasks only use the /eval/ part of the NbE algorithm, this next set also
uses /quote/ to produce a large normal form of a term, and not to evaluate to a
single value. This is evaluated on a large Church-encoded natural number, on a
deep Church-encoded binary tree.

\missingfigure{(a) forceNat (b) n2M term}

\missingfigure{(a) ackermann (b) binary tree}

*** Elaboration
Elaboration will test not only the NbE part of our system, but also type
inference and checking. We will use not only deeply nested function application
of Church-encoded terms, but also terms with large types: those that contain
many or deeply nested function arrows or Π-types. The first task is the
elaboration of a polymorphic /id/ function repeatedly applied to itself, as this
produces meta-variables whose solution doubles in size with each application.
The second task is similar, the elaboration of a large Σ-type encoding a /Nat/
indexed vector. Lastly, the third task especially tests unification: the
task is to unify two large Church-encoded natural numbers, to check them for
equivalence.

\missingfigure{(a) id id id id with and without implicits (b) Sigma vector}

\missingfigure{Eq, refl, n2Mb}

*** Computation
The last part consists of computational tasks, where we can compare the
performance of Montuno with other, non-dependent languages. As Montuno does not
implement complex data structures or memory buffers, we will need to limit the
types of tasks to primarily computational ones.

I have selected the following: Fibonacci number computation, the previously
implemented Ackermann function, solving the N Queens problem, matrix
multiplication, and a Brainfuck interpreter.

\missingfigure{(a) Brainfuck. (b) matrix multiplication in Montuno}

** Methodology
There are many ways how we can measure each language's performance on these
tasks. The main concern is that Montuno and MontunoTruffle are JIT-compiled
languages that need a significant amount of warm-up: the first iterations will take
significantly longer than the iterations that happen after warm-up, after all
code is JIT-optimized.

For this reason, we cannot use whole-program measurement using commands like
~time~, which measures the entire run of a program including any interpreter
start-up time, parsing, and other tasks unrelated to elaboration or
normalization. We will need to use in-language support for measuring elaboration
times in those languages that support it, and in those that do not, we will need
to postprocess measurements with such confounders.

Aside from measuring the time it takes to normalize or elaborate an expression,
we will also measure the peak memory usage using the system tool ~time -v~.

\missingfigure{(a),(b),(c),(d) Commands for Coq, Agda, Idris, Montuno}

*** Configuration
\missingfigure{Table with machine specs}

** Preliminary results
*** Normalization
\missingfigure{A single large bar graph, grouped by language, four tasks in a group}

#+comment: https://tex.stackexchange.com/questions/156964/guide-to-draw-charts-basic-pie-bar-from-data
#+comment: https://github.com/MartinThoma/LaTeX-examples/blob/master/tikz/bar-chart-grouping/bar-chart-grouping.tex
#+comment: https://latexdraw.com/bar-charts-in-latex-step-by-step-tikz-tutorial/#t-1611605009264
#+begin_export latex
\pgfplotstableread[row sep=\\,col sep=&]{
    interval & carT & carD & carR \\
    0--2     & 1.2  & 0.1  & 0.2  \\
    2--5     & 12.8 & 3.8  & 4.9  \\
    5--10    & 15.5 & 10.4 & 13.4 \\
    10--20   & 14.0 & 17.3 & 22.2 \\
    20--50   & 7.9  & 21.1 & 27.0 \\
    50+      & 3.0  & 22.3 & 28.6 \\
    }\norm
#+end_export

#+label:norm-results
#+caption: Preliminary results of normalization tasks
#+begin_figure latex
\begin{tikzpicture}
    \begin{axis}[
            ybar,
            bar width=.5cm,
            width=\textwidth,
            height=.5\textwidth,
            legend style={at={(0.5,1)},
                anchor=north,legend columns=-1},
            symbolic x coords={0--2,2--5,5--10,10--20,20--50,50+},
            xtick=data,
            nodes near coords,
            nodes near coords align={vertical},
            ymin=0,ymax=35,
            ylabel={\%},
        ]
        \addplot table[x=interval,y=carT]{\norm};
        \addplot table[x=interval,y=carD]{\norm};
        \addplot table[x=interval,y=carR]{\norm};
        \legend{Trips, Distance, Energy}
    \end{axis}
\end{tikzpicture}
#+end_figure

[...]

*** Elaboration
\missingfigure{A single large bar graph, grouped by language, three tasks in a group}

[...]

*** Computation
\missingfigure{A single large bar graph, grouped by language, four tasks in a group}

[...]

* Optimizations: Making MontunoTruffle fast
   :PROPERTIES:
   :CUSTOM_ID: optimizations
   :END:
** Possible performance problem sources
Reiterate JGross

how to find out whether X is relevant to us or not? How to prove the effect of JIT?

Show asymptotes - binders, terms, sizes

Show the graphs - large values, many iterations (warmup), sharing

** Possible optimizations
Show before and afters for each optimization

What does Enso do, optimization phases?

What can we do?

Hash consing = sharing structurally equal values in the environment. See below from Kmett:
https://gist.github.com/ekmett/6c64c3815f9949d067d8f9779e2347ef

Inlining, let-floating

Avoid thunk chaining: box(box(box(() => x))

Frame slot clearing - simplifies Graal's role, as Graal tracks dataflow, and
this shortens an object's lifetime

Static optimization  - changing the structure of the interpreter so that it
would be faster even without JIT

Dynamic optimization - using more Truffle-specific features, so that Graal can
more efficiently optimize the code: CompilerDirectives, BranchProfiles,
TruffleBoundaries, inline caches, ControlFlowExceptions

"Immutable, except to simplify" + assumptions
Maximize evaluation sharing - globals, cache, ?

- cite:blaguszewski10_llvm - potential optimizations, LLVM impl, closures
- cite:gross14_coq_experience - Coq experience, a few reasons, comparison
- cite:gross21_performance - a lot of reasons in Coq
- cite:eisenberg20_stitch - CSE

Ruby uses threads, can we? Automatic parallelism
- cite:reid98_resumable_holes - concurrency & parallelism in GHC evaluation
- cite:hughes82_supercombinators - CAFs? Lazy evaluation?

Think about the fast vs slow path!

- cite:zheng17_deoptimization - reasons for deoptimization

OSM in DynSem:
- DynSem also had to consider concept mapping: a program graph starts with generic node operations that immediately specialize to language-specific operations during their first execution
- HashMaps are efficient, but bring downsides. The Graal compiler cannot see inside the HashMap methods, and so cannot analyze data flow in them and use it to optimize them.
- DynSem also had to deal with runtime specification of environment manipulation as this is also supplied by the language specification. Also split between local short-lived values inside frames, and long-lived heap variables.
- Relevant to us is their use of the Object Storage Model, which they use to model variable scoping which is the processed into fixed-shape stack frames (separate from the Truffle Frames, this is a meta-interpreter). OSM's use case is ideal for when all objects of a certain type have a fixed shape. This is ideal for us, as tuples and named records have, by definition, a fixed shape (unlike Ruby etc. we do not support dynamic object reshaping, obviously).
- They did it separately from the Virtual/MaterializedFrame functionality to avoid the overhead of MaterializedFrames that Graal cannot optimize away.
- Truffle/Graal discourage the use of custom collections, and instead push developers towards Frames (which support by-name lookups) and Objects (same).

To enhance compilation specialization/inlining:
- Visualizations of call graphs - whether or not node children are stable calls
- Most DynSem calls are not stable calls, they are dispatched on runtime based on arguments - something that Graal does not see as stable (CompilationFinal)
- Two types of rules: mono- and polymorphic. based on whether they are called with different types of values at runtime. Poly- are not inlined
- DynSem found two types: dynamic dispatch (meta-interpreter depended on runtime info), and structural dispatch (based on the program AST and not on values). This is similar to our EvalNode, QuoteNode and similar, which depend on the type of the value
- Overloaded rules--rules with the same input shape--are merged into a single FusedRule node and iterated over with @ExplodeLoop.
- For mono/polymorphic rules, they use an assumption that a rule is monomorphic, specialize the rule, and recompile if it becomes polymorphic.
- Inlining nodes - polymorphic rules reduced to a set of monomorphic rules - a rule from the registry is cloned in an uninitialized state in a monomorphic call site and "inlined" (in a CompilationFinal field)
- They use a central registry of CallTargets that contain rules that they can clone and adopt locally if necessary to specialize--we can do the same!
- Disadvantages: there is more to compile and inline by Graal, instead of a CallTarget, they use a child. Likely to take longer to stabilize, but faster in the end.

** Tools
The results of Montuno need to be further evaluated. Graal and Truffle provide

*** Ideal Graph Visualizer
A graphical program that serves to visualize the process of Truffle graph optimization. When configured correctly, the IGV will receive the results of all partial evaluations.

*** CPU Sampler
#+COMMENT: https://github.com/oracle/graal/blob/master/truffle/docs/Profiling.md

Running the language launcher with the options ~--cpusampler --cpusampler.Delay=MILLISECONDS~ will start the CPU sampler. This tool serves to profile the guest language (as opposed to the regular JDK Async Profiler which will profile the entire process.

~--cpusampler.Delay~ helps to not include warm-up time in the results.

Using additional options (~--cpusampler --cpusampler.SampleInternal --cpusampler.Mode=roots --cpusampler.Output=json~) and postprocessing the generated JSON with an additional script we can create a so-called flamegraph with the results of the sampling.

#+COMMENT: https://github.com/eregon/FlameGraph/blob/graalvm/stackcollapse-graalvm.rb

*** Renaissance
cite:prokopec19_renaissance

- a set of benchmarks and measurement tools
- measures: synchronized, object.wait, object.notify, atomic ops, park
  operations, average cpu usage, cache misses, objects allocated, arrays
  allocated, method calls, dynamic method calls
- needs us to package it in a special way, but useful to compare between truffle
  optimization versions
- https://github.com/Gwandalff/krun-dsl-benchmarks is an alternative that has
  examples with Truffle, measures only s/op
** Glued evaluation
An optimization technique that attempts to avoid even more computation.

Parallel operation on two types of values, glued and local. Glued are lazily evaluated to a fully unfolded form; local are eagerly computed to a head-normal form but not fully unfolded, to prevent size explosions. This results in better performance in a large class of programs, although it is not an asymptotic improvement, as we have a small eagerly evaluated term for quoting, and a large lazily evaluated for conversion checking.

This is another case of specialization: we have two operations to perform on the same class of values, but each operation has its own requirements; in this case, on the size of the terms as in quoting we want a small folded value but require the full term for conversion checking.

cite:kaposi19_gluing

https://eutypes.cs.ru.nl/eutypes_pmwiki/uploads/Meetings/Kovacs_slides.pdf

** Splitting
type specializations/dict passing

** Function dispatch
lambda merging

eta expansion

** Caching and sharing
Sharing computation and common values

Multiple references to the same object

let-floating

inlinable functions

** Specializations

**** Truffle recommended optimizations
The optimization workflow recommended by the Truffle developers is as follows:
1. Run with a profiler to sample the application and identify responsible compilation units. Use a sampling delay (--cpusampler.Delay=MILLISECONDS) to only profile after warmup. See the Profiling guide.
2. Understand what is being compiled and look for deoptimizations. Methods that are listed to run mostly in the interpreter likely have a problem with deoptimization.
3. Simplify the code as much as possible where it still shows the performance problem.
4. Enable performance warnings and list boundary calls.
5. Dump the Graal graph of the responsible compilation unit and look at the phase After TruffleTier.
   1. Look at the Graal graphs at the phases After TruffleTier and After PartialEscape and check if it is what you would expect. If there are nodes there that you do not want to be there, think about how to guard against including them. If there are more complex nodes there than you want, think about how to add specialisations that generate simpler code. If there are nodes you think should be there in a benchmark that are not, think about how to make values dynamic so they are not optimized away.
6. Search for Invoke nodes in the Graal IR. Invoke nodes that are not representing guest language calls should be specialized away. This may not be always possible, e.g., if the method does I/O.
7. Search for control flow splits (red lines) and investigate whether they result from control flow caused by the guest application or are just artifacts from the language implementation. The latter should be avoided if possible.
8. Search for indirections in linear code (Load and LoadIndexed) and try to minimize the code. The less code that is on the hot-path the better.

---
Add more info on splitting!!

- ~--engine.TraceCompilation~ prints a line for each method compilation
- ~--engine.TraceCompilationDetail~ prints a line for compilation queuing, start, and finish
- ~--engine.TraceCompilationAST~ prints the entire compiled AST
- ~--engine.TraceInlining~ prints inlining decision details
- ~--engine.TraceSplitting~ prints splitting decisions
- ~--engine.TraceTransferToInterpreter~ prints a stack trace for each explicit invalidation
- ~--engine.TracePerformanceWarnings=(call|instanceof|store|all)~
  - ~call~ prints when PE cannot inline a call
  - ~instanceof~ prints when PE cannot resolve virtual ~instanceof~ to a specific type
  - ~store~ prints when PE store location argument is not compilation final
- ~--engine.CompilationStatistics~ prints total compilation statistics
- ~--engine.CompilationStatisticDetails~ prints compilation histograms for each node
- ~--engine.TraceMethodExpansion=truffleTier~ prints a tree of all expanded Java methods
- ~--engine.TraceNodeExpansion=truffleTier~ prints a tree of all expanded Nodes
- ~--engine.MethodExpansionStatistics=truffleTier~ prints total Graal nodes produced by a method
- ~--engine.NodeExpansionStatistics=truffleTier~ also includes Graal specializations
- ~--engine.InstrumentBoundaries~ prints info about boundaries encountered (should be minimized)
- ~--engine.InstrumentBranches~ prints info about branch profiles
- ~--engine.SpecializationStatistics~ prints details about specializations performed
- ~--vm.XX:+TraceDeoptimization~ prints all deoptimizations
- ~--vm.XX:+TraceDeoptimizationDetails~ prints all deoptimizations with details

It is also possible to control what is being compiled, add details to IGV graphs dumped, and print the exact assembly produced: see https://github.com/oracle/graal/blob/master/truffle/docs/Optimizing.md.

**** How to debug specializations
*Specialization histogram:* If compiled with ~-Atruffle.dsl.GenerateSpecializationStatistics=true~ and executed with ~--engine.SpecializationHistogram~, Truffle DSL will compile the nodes in a special way and show a table of the specializations performed during the execution of a program.

Example shown at
https://github.com/oracle/graal/blob/master/truffle/docs/SpecializationHistogram.md,
maybe include the table?

*Slow path only:* If compiled with ~-Atruffle.dsl.GenerateSlowPathOnly=true~, Truffle will only execute the last, most generic specialization, and will ignore all fast path specializations.


* Discussion
*Truffle is not primarily aimed at statically-typed languages or functional
languages. Its most easily accessible benefits lie in speculative optimization
of dynamically typed code and inline caches, where generic object-oriented code
can be specialized to a specific value type. Statically-typed languages have a
lot more information regarding the values that will flow through a function, and
e.g. GHC has a specific /specialization/ compiler pass.

However, there is a lot of overlap between the static optimizations done by
e.g. GHC and runtime optimizations done by Graal. An example would be
unfolding/inlining, where the compiler needs to make a single decision of
whether to replace a call to a function with its definition -- a decision that
depends on the size of the definition, whether they are in the same module, and
other heuristics cite:jones02_inliner. A Truffle interpreter would be able to
postpone the decision until execution time, when the definition could be inlined
if the call happened enough times.

* Results


   (A few pages)

One-to-one evaluation and discussion of directly comparable subjects, confidence
intervals, likely causes of improvements/regressions, iterations to
steady-state.

** Discussion
Size of codebase

Effort required

Effect produced

Is this road viable?

** Next work
(A few pages, subsections/mini-headers)

FFI, tooling

RPython, K Framework - exploration

SPMD on Truffle, Array languages

More type extensions OR totality (as a proof assistent)

Finite types, universes, no type in type, HoTT, CoC

Is this useful at all? What's the benefit for the world? (in evaluation)

next work: LF, techniques, extensions, real language


* Bibliography                                                :ignoreheading:
bibliographystyle:abbrv
bibliography:bibliography.bib
* Footnotes
