% Created 2021-04-02 Fri 13:43
% Intended LaTeX compiler: pdflatex
\documentclass[english,zadani,odsaz]{fitthesis}
\renewcommand\title[1]{}
\usepackage{minted}
\usepackage{tikz}
\usepackage{tikz-cd}
\usepackage{bussproofs}
\usepackage[figure,table,listing]{totalcount}
\input{metadata}
\date{\today}
\title{}
\hypersetup{
 pdfauthor={},
 pdftitle={},
 pdfkeywords={},
 pdfsubject={},
 pdfcreator={Emacs 28.0.50 (Org mode 9.3)}, 
 pdflang={English}}
\begin{document}

% * (front matter)                                              :ignoreheading:
\maketitle
\setlength{\parskip}{0pt}
{\hypersetup{hidelinks}\tableofcontents}
\iftotalfigures\listoffigures\fi
\iftotaltables\listoftables\fi
\iftotallistings\listoflistings\fi
\iftwoside\cleardoublepage\fi
\setlength{\parskip}{0.5\bigskipamount}

\chapter{Změnit název: ``Montuno: Efficient elaboration of dependent types using JIT compiler''}
\label{sec:org605e2a9}

\chapter{Introduction}
\label{sec:org44f3e9a}
\section{(Old)}
\label{sec:org2ef14f0}
When creating small experimental or research languages, writing a compiler may
be too much effort for the expected gain. On the other hand an interpreter is
usually not as performant as its creators may require for more computationally
intensive tasks.

There is a potential third way, proposed by Yoshihiko Futamura in the 1970s,
called the Futamura projection (or partial program evaluation), wherein an
interpreter is specialized in conjunction with the source code of a program,
yielding an executable. Some parts of the interpreter may be specialized, some
optimized, some left off entirely. Depending on the quality of the specializer,
the gains may be several orders of magnitude.

The goal of my thesis is to evaluate whether the GraalVM/Truffle platform is
suitable enough to act as a specializer for functional languages, in particular
for the dependently-typed lambda calculus.  To illustrate in Figure
\ref{fig:futamora}, the question is whether the path \textit\{Native
Image\textrightarrow Result\} is fast enough compared to the path
\textit{Executable\textrightarrow Result}.

\begin{figure}
\centering
\begin{tikzcd}
{} & Program
 \arrow[ld, "Compiler" description, bend right]
 \arrow[dd, "Interpreter" description, bend right=67]
 \arrow[rd, "Partial\ Evaluation" description, bend left]
 \arrow[dd, "JIT" description, bend left=67] & {} \\
Executable \arrow[rd, "Run" description, bend right] & {} & Native\ Image \arrow[ld, "Run", bend left]
 \\ {} & Result & {}
\end{tikzcd}
\caption{Methods of program execution}
\label{fig:futamora}
\end{figure}

Truffle has already been used rather successfully for the (mostly) imperative
languages Ruby, Python, R, Java, and WebAssembly, but (purely-)functional
languages differ in their evaluation model and in particular the required
allocation throughput, so it is still an open question whether GraalVM is a good
enough fit.

The desired outcome---at least, of the first part of my thesis---is a set of
implementations, and a set of benchmarks demonstrating a positive or a negative
result.  If the result is positive, there are many potential follow-up tasks:
implementing a different, more complex language, maybe a language to be
interpreted into the dependently-typed lambda calculus to attempt the approach
implemented in the \emph{Collapsing Tower of Interpreters} \cite{amin2017collapsing},
or experimenting with different runtime models - all depending on the results of
this preliminary proof of concept.

In the best case, the JIT-compiled program would be as close in performance to a
program processed by a hand-crafted compiler as possible (not including JIT
warm-up), and I would spend the second half of my thesis on different topics
(like provably-correct program transformations) instead of hand-optimizing the
primitive operations - I should find out which it is going to be as soon in the
second term as possible.

As far as I am aware, there are no other native just-in-time compiled
implementations of the dependently-typed lambda calculus, with the exception of
the preliminary investigations done by the originator of this idea
\cite{kmett_2019}, although there are a few projects implementing a lambda
calculus directly to the Java Virtual Machine byte code..

\section{New}
\label{sec:org87d7d5a}
--- see Excel@FIT start

Follow-up to Cadenza (STLC) on behalf of its creator

Combination of JIT and dependent types is natural (computation on the type-level) but not found in literature

Motivation: JIT for elaboration!!! plus easy prototyping, stepping block for
    other projects (efficient LF/Twelf, \ldots{})

smalltt a step in another direction


\chapter{GraalVM and the Truffle Framework}
\label{sec:org55e978c}
\section{Intro: ecosystem, purpose, benefits}
\label{sec:orgb9a70cc}
\textbf{GraalVM} is a just-in-time optimizing compiler for the Java bytecode. \textbf{Truffle} is
a set of libraries that expose the internals of the GraalVM compiler, intended
for easy implementation of other languages. So far JavaScript, Python, Ruby, R,
and WebAssembly have Truffle implementations, and therefore can run on the JVM.

GraalVM is also intended to allow creating \emph{polyglot applications} easily,
applications that have their parts written in different languages. It is
therefore easy to e.g. call R to create visualizations for the results of a
Python program, or to call any Truffle language from Java.

There is also the option to compile a \emph{Native Image} to eliminate most program
start-up costs associated with a just-in-time compiler, pre-compiling the
program partially (ahead-of-time).

From the point of view of a programmer, Truffle makes it possible to write an
interpreter, and then slowly add optimizations like program graph rewriting,
node specializations, inline instruction caching or others. This seems like a
good middle ground between spending large amounts of time on an optimized
compiler, and just specifying the semantics of a program in an interpreter that,
however, will likely not run quickly.

While GraalVM/Truffle is open-source and released under GPL v2, an
enterprise edition that claims large performance improvements is released
commercially.

\begin{figure}[!htb]
\centering
\includegraphics[width=.9\linewidth]{./img/graalvm.jpg}
\caption{GraalVM and Truffle (source: oracle.com)}
\end{figure}

JIT options - specialization, deoptimization
Use cases
Potential optimizations
\section{GraalVM}
\label{sec:orgcc19adc}
marketing texts

diagrams

Hotspot's JIT vs Graal's

\section{Truffle}
\label{sec:org9dbdbb8}
Intro + motivating multilanguage snippet

features with code samples

benchmarks for other languages

\chapter{Language specification}
\label{sec:orgc59e533}
\section{Dependent types}
\label{sec:org0762d55}
Intro dependent types + motivating example

hot ML research area (cubical, path, \ldots{}) (look in Kovacs' materials)

\section{Lambda Calculus Theory}
\label{sec:org8d4a078}
\subsection{Untyped Lambda Calculus}
\label{sec:orgbd1c553}
The untyped lambda calculus is a simple language consisting of just three kinds
of forms: variables, function application, and abstraction.

\begin{figure}[!htpb]
\[\begin{array}{ccll}
e & ::= & x            & \text{variable} \\
  & |   & e_1~e_2      & \text{application} \\
  & |   & \lambda x. e & \text{abstraction}
\end{array}\]
\caption{Untyped lambda calculus}
\end{figure}

syntax, semantics, usage

\subsection{Simply-Typed Lambda Calculus}
\label{sec:org16662a1}
The simply-typed lambda calculus adds a fourth kind of a term, type annotation,
and its type language:

\begin{figure}[!htpb]
\[\begin{array}{ccll}
e & ::= & x           & \text{variable} \\
  & |   & e_1~e_2      & \text{application} \\
  & |   & \lambda x. e & \text{abstraction} \\
  & |   & x:\tau     & \text{annotation}
\end{array}\]
\[\begin{array}{ccll}
\tau & ::= & \alpha           & \text{base type} \\
     & |   & \tau\rightarrow\tau' & \text{composite type}
\end{array}\]
\caption{Simply typed lambda calculus}
\end{figure}

syntax, semantics, type checking, type inference

\subsection{Dependently-Typed Lambda Calculus}
\label{sec:orgd3dc091}

Lambda cube

\ldots{} image

The dependently typed lambda calculus merges these two languages together,
simplifying the grammar.

\begin{figure}[!htpb]
\[\begin{array}{ccll}
e & ::= & x           & \text{variable} \\
  & |   & e_1~e_2      & \text{application} \\
  & |   & \lambda x. e & \text{abstraction} \\
  & |   & x:\tau      & \text{annotation} \\
  & |   & *           & \text{the type of types} \\
  & |   & \forall x:\rho.\rho' & \text{dependent function space}
\end{array}\]
\caption{Dependently typed lambda calculus}
\end{figure}
\ldots{}

directions, decidability, where is DTLC

implicits, holes, metacontext

\section{Algorithms}
\label{sec:org8c81d3d}
type-checking (bidi, NbE) + motivation

elaboration (what is it, why is it slow, glued, nondet, cite Kovacs)

evaluation techniques (by-need, \ldots{})

eval/apply (in implementation section)

\section{Specification}
\label{sec:org6d14800}
\begin{listing}[!htpb]
\begin{minted}[]{text}
let const = (\ a b x y -> x) :: forall (a :: *) (b :: *) . a -> b -> a
\end{minted}
\caption{The constant function in LambdaPi}
\end{listing}

grammar

semantics (inference rules, evaluation rules)

\chapter{Language Implementations}
\label{sec:orgdf575c8}
\section{Pure Interpreter}
\label{sec:org26ee3d5}
\subsection{Parser}
\label{sec:org7cfeb08}
ANTLR is the parser to use in JVM

Several ways to consume: listener, visitor - I've used AST transformation which
is the most compact and most familiar to other DTLC implementations which are
usually in functional languages

(listener - enter/exit function calls, visitor is similar, toAst uses recursive calls)

\begin{minted}[linenos,firstnumber=1]{antlr}
grammar Montuno;
@header {
package montuno;
}

file : (decls+=top)* EOF;

top
    : id=IDENT ':' type=term '.' #Decl
    | id=binder (':' type=term)? '=' body=term '.' #Defn
    | '%nf' term #Elab
    ;

term
    : 'let' name=binder ':' type=term '=' tm=term 'in' body=term #Let
    | LAMBDA (args+=binder)* '.' body=term #Lam
    | '(' (dom+=binder)+ ':' kind=term ')' ARROW cod=term #PiExpl
    | '{' (dom+=binder)+ (':' kind=term)? '}' ARROW cod=term #PiImpl
    | (spine+=atom)+ (ARROW rest=term)? #App
    ;
atom
    : '(' rec=term ')' #Rec
    | IDENT #Var
    | '*' #Star
    | 'Nat' #Nat
    | NAT #LitNat
    ;
binder
    : IDENT #Ident
    | '_' #Hole
    ;

IDENT : [a-zA-Z] [a-zA-Z0-9']*;
NAT : [0-9]+;

WS : [ \t\r\n] -> skip;
COMMENT : '--' (~[\r\n])* -> skip;
NCOMMENT : '{-'~[#] .*? '-}' -> skip;
LAMBDA : '\\' | 'λ';
ARROW : '->' | '→';
\end{minted}

\subsection{Data shape}
\label{sec:org1cf8b82}

\subsection{Algorithms}
\label{sec:orgc23ee08}
elaboration

\section{Truffle Interpreter}
\label{sec:orgd235983}
I have implemented a dependently typed lambda calculus called LambdaPi based on
the prior work \emph{A tutorial implementation of a dependently typed lambda calculus}
\cite{loh2010tutorial}. The parser and interpreter are written in Kotlin, where
I will also need to write the JIT implementation. This is a pure interpreter
that will serve as a baseline for future benchmarks.

parser shared with the previous implementation

data classes

truffle specifics

inline cache, tail call, trampoline (continuations)

!! show their effect on program graphs

eval/uneval(quote)

evaluation phases - translate to Code, run typecheck, run eval vs glued, ???

native image

\subsection{Evaluation model of function application}
\label{sec:org541f92d}
push-enter - arguments are pushed onto the stack, the function then takes as
many as it requires

eval-apply - the caller sees the arity of the function and then decides whether
it is over-applied (evaluates the function and creates a continuation), appllied
exactly (EVAL), or under-applied (creates a PAP, a closure-like value)

-- exactly describe the rules from eval-apply paper KNOWNCALL, EXACT, CALLK, PAP
-- known application ( = known arity), unknown function


\section{Similar implementations}
\label{sec:orgea74763}
``I've considered LLVM, WASM backends, but we need to evaluate type-checking
perf especially, so not relevant - better to compare with existing and/or
experimental systems''

SmallTT, Coq, Agda, GHC - for comparison

\chapter{Evaluation}
\label{sec:orgf507896}
\section{Benchmarks}
\label{sec:org8a6ee3e}
\begin{itemize}
\item from SmallTT project, from Idris project
\item memory usage (curve)
\item compilation speed (type-heavy test)
\item evaluation speed (compute-heavy test)
\end{itemize}

Nats, pairs, fun types, numerics

\section{Results}
\label{sec:orgea99192}
---

\section{Future work}
\label{sec:orgbec03ed}
good enough?

LF, techniques, extensions, real language

\chapter{Conclusion}
\label{sec:org7079550}




% * (bibliography, start of appendix)                           :ignoreheading:

\makeatletter
\def\@openbib@code{\addcontentsline{toc}{chapter}{Bibliography}}
\makeatother
\begin{flushleft}

\bibliographystyle{bibstyle}
\bibliography{bibliography}

\end{flushleft}
\iftwoside\cleardoublepage\fi
\appendix
\appendixpage
\iftwoside\cleardoublepage\fi
\startcontents[chapters]
% \setlength{\parskip}{0pt}
% \printcontents[chapters]{l}{0}{\setcounter{tocdepth}{2}}
% \setlength{\parskip}{0.5\bigskipamount}
\iftwoside\cleardoublepage\fi

\chapter{Contents of the attached data storage}
\label{sec:org19b42c7}
\ldots{}
\end{document}