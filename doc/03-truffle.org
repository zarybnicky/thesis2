* (front matter)                                              :ignoreheading:
#+LANGUAGE: en
#+OPTIONS: texht:nil toc:nil author:nil ':t H:4 num:3
#+LATEX_CLASS: fitthesis
#+LATEX_CLASS_OPTIONS: [english,zadani,odsaz]
#+EXCLUDE_TAGS: noexport
#+BIND: org-latex-title-command ""
#+BIND: org-latex-prefer-user-labels t
#+BIND: org-latex-default-figure-position "htb"

#+latex: \setcounter{chapter}{2}

* Adding JIT compilation to Montuno: MontunoTruffle
  :PROPERTIES:
  :CUSTOM_ID: jit-interpreter
  :END:
** Introduction
In the first part of this thesis, we introduced the theory of dependent types,
specified a small, dependently typed language, and introduced some of the
specifics of creating an interpreter for this language, under the name
Montuno. The second part is concerned with the Truffle language implementation
framework: we will introduce the framework itself and the features it provides
to language designers, and use it to build a second interpreter.

To reiterate the goal of this thesis, the intent is to create a vehicle for
evaluating whether adding just-in-time compilation produces visible improvements
in the performance of dependently typed languages. Type elaboration is often a
performance bottleneck cite:gross21_performance, and because it involves
evaluation of terms, it should be possible to improve it using JIT compilation; as
optimizing AST evaluation is a good candidate for JIT compilation. We have
designed a language that uses features and constructs that are representative of
state-of-the-art proof assistants and dependently typed languages, so that such
evaluation may be used as a guideline for further work.

This chapter is concerned with building a second interpreter based on
Truffle. First, however, we need to introduce the idea of just-in-time
compilation in general, and see how Truffle implements the concept.

** Just-in-time compilation
Just-in-time (JIT) compilation, in general, is a technique that combines an
interpreter and a compiler into a single runtime. A program is first
interpreted, and later compiled during its runtime. The JIT compiler often
observes the behavior of the interpreted program, so that it can compile it more
efficiently. While a program is running, the JIT compiler optimizes the parts
that run often; using an electrical engineering metaphor, such parts are
sometimes called /"hot loops"/.

The optimizations often rely on assumption that, when executing a program, its
functions (and the functions in the libraries it uses) are only called in a
specific pattern, configuration, or with a specific type of data. When talking
about specific optimizations, the terms /slow path/ and /fast path/ are often
used. The fast path is the one for which the program is currently optimized,
whereas the slow paths are all the other ones, e.g., function calls or branches
that were not used during the specific program execution.

There are several approaches to JIT compilation: /meta-tracing/ and /partial
evaluation/ are the two most common ones.

**** Meta-tracing
A JIT compiler based on meta-tracing records a /trace/ of the path taken during
program execution. Often used paths are then optimized: either rewritten, or
directly compiled to machine code. Tracing, however, adds some overhead to the
runtime of the program, so only some paths are traced. While the programmer can
provide hints to the compiler, meta-tracing may result in unpredictable peak
performance. This technique has been successfully used in projects like PyPy,
that is built using the RPython JIT compiler cite:bolz14_meta, or on GHC with
mixed results cite:schilling13_tracing_jit.

**** Partial evaluation
The second approach to JIT compilation is called /partial evaluation/, also called
the /Futamura projection/. The main principle is as follows: (fully) evaluating a
program using an interpreter produces some output, whereas partially evaluating
a program using an interpreter produces a specialized executable. The
specializer assumes that the program is constant and can, e.g., eliminate parts
of the interpreter that will not be used by the program. This is the approach
taken by Truffle cite:latifi19_futamura.

#+LABEL: partial-eval
#+ATTR_LaTeX: :placement [htb]
#+CAPTION: Partial evaluation with constant folding\protect\footnotemark
[[./img/partial-evaluation.png]]
#+latex: \footnotetext{Source: Graal: High Performance Compilation for Managed Languages \cite{graal_pldi}}

The basic principle is demonstrated in Figure ref:partial-eval, on actual code
produced by Truffle. In its vocabulary, a ~CompilationFinal~ value is assumed to
be unchanging for a single instance of the program graph node (the field ~flag~ in
the figure), and so the JIT compiler can transform a conditional ~if~ statement
into an unconditional one, eliminating the second branch.

There are, in fact, three Futamura projections, referred to by their ordinals:
the /first Futamura projection/ specializes an interpreter with regards to a
program, producing an executable. The /second Futamura projection/ combines the
specializer itself with an interpreter, producing a compiler. The third
projection uses the specializer on itself, producing a compiler maker. As we
will see in later sections, Truffle and GraalVM implement both the first and
second projections cite:latifi19_futamura.

** Truffle and GraalVM
I have mentioned Truffle several times already in previous chapters. To
introduce it properly, we first need to take a look at the Java Virtual machine
(JVM). The JVM is a complex platform that consists of several components: a
number of compilers, a memory manager, a garbage collector, etc., and the entire
purpose of this machinery is to execute ~.class~ files that contain the bytecode
representation of Java, or other languages that run on the JVM platform. During
the execution of a program, code is first translated into generic executable
code using a fast C1 compiler. When a specific piece of code is executed enough
times, it is further compiled by a slower C2 compiler that performs more
expensive optimizations, but also produces more performant code.

The HotSpotVM is one such implementation of this virtual machine. The GraalVM
project, of which Truffle is a part, consists of several components and the main
one is the Graal compiler. It is an Oracle research project that replaces the C2
compiler inside HotSpotVM, to modernize an aging code base written in C++, and
replace it with a modern one built with Java cite:duboscq13_graalir.  The Graal
compiler is used in other ways, though, some of which are illustrated in Figure
ref:graal. We will now look at the main ones.

#+LABEL: graal
#+ATTR_LaTeX: :placement [htb]
#+CAPTION: GraalVM components and Truffle\protect\footnotemark
[[./img/graalvm.png]]
#+latex: \footnotetext{Source: \url{https://www.graalvm.org/community/assets}}

**** Graal
Graal itself is at its core a graph optimizer applied to program graphs. It
processes Java bytecode into a graph of the entire program, spanning across
function calls, and reorders, simplifies, and overall optimizes it.

It actually builds two graphs in one: a data-flow graph, and an instruction-flow
graph. Data-flow describes what data is required for which operation, which can
be reordered or optimized away, whereas the instruction-flow graph stores the
actual order of instructions that will happen on the processor. Figure
ref:graal-graph shows the output of a graph visualization tool provided by
Graal. This specific graph is the execution path of a ~CounterNode~ that simply
reads its internal field, adds a one to it, and stores the result; more complex
program graphs are often very large and hard to read.

**** SubstrateVM
#+LABEL: graal-graph
#+ATTR_LaTeX: :placement [htb] :scale .7
#+CAPTION: Graal program graph, visualized using IGV\protect\footnotemark
[[./img/graal-graph.png]]
#+latex: \footnotetext{Source: Graal: High Performance Compilation for Managed Languages \cite{graal_pldi}}

As Graal is a standalone Java library, it can also be used in contexts other
than the HotSpotVM. SubstrateVM is an alternative virtual machine that executes
Graal-optimized code. It does not perform just-in-time optimizations, though,
but uses Graal as an ahead-of-time compiler. The result is a small stand-alone
executable file that does not depend on a JVM being installed on a machine,
called a /Native Image/. By replacing JIT compilation with ahead-of-time, these
binaries start an order-of-magnitude faster than regular Java programs, and can
be freely copied between machines, similar to Go or Rust binaries
cite:wurthinger13_graal.

**** Truffle
The Graal program graph, Graal IR, is a directed graph structure in static
single assignment form. As it is implemented in Java itself, the graph structure
is extensible cite:duboscq13_graalir.  Truffle exposes this extensibility of the
program to developers. In essence, it is a graph manipulation library and a set
of utilities for creating these graphs. These graphs are the abstract syntax
tree of a language: each node has an ~execute~ method; calling the method returns
the result of evaluating the expression it represents.

**** Interpreter/compiler
When creating a programming language, there is a trade-off between writing an
interpreter and a compiler. An interpreter is usually simpler to implement and
each function in the host language directly encodes the semantics of a language
construct, but the result can be rather slow: compared to the language in which
the interpreter is written, it can often be slower by a factor to 10x to 100x
cite:wurthinger13_graal. A compiler, on the other hand, does not execute a
program directly, but instead translates its semantics onto the semantics of a
different virtual machine, be it the JVM, LLVM, or x86 assembly.

Truffle attempts to side-step this trade-off by making it possible to create an
interpreter that can be compiled on-demand via JIT when interpreted or
ahead-of-time into a Native Image; the result should be an interpreter-based
language implementation with the performance of a compiled language and
access to all JVM capabilities (e.g. memory management). Instead of running an
interpreter inside a host language like Java, the interpreter is embedded one
layer lower, into a program graph that runs directly on the JVM and is
manipulated by the Truffle runtime that runs next to it.

**** Polyglot
Truffle languages can all run next to one another on the JVM. As a side-effect,
communication between languages is possible without the need for usual FFI
(foreign function interface) complications. As all values are JVM objects,
access to object properties uses the same mechanisms across languages, as does
function invocation. In effect, any language from Figure ref:graal can access
libraries and values from any other such language.

**** TruffleDSL
Truffle is a runtime library that manages the program graph and a number of
other concerns like variable scoping, or the object storage model that allows
objects from different languages to share the same layout. TruffleDSL is a
user-facing library in the form of a domain-specific language (DSL) that aids in
simplified construction specialized Truffle node classes, inline caches,
language type systems, and other specifics. This DSL is in the form of Java
/annotations/ that give additional information to classes, methods, or fields, so
that a DSL processor can later use them to generate the actual implementation
details.

**** Instrumentation
The fact that all Truffle languages share the same basis, the program graph,
means that a shared suite of tooling could be built on top of it: a profiler
(VisualVM), a stepping debugger (Chrome Debugger), program graph inspector
(IGV), a language server (Graal LSP). We will use some of these tools in further
sections.

** Truffle in detail
Concluding the general introduction to Truffle and GraalVM, we will now look at
the specifics of how a Truffle language differs from the type of interpreter we
created previously.

The general concept is very similar to the previously created AST interpreter:
there is again a tree data structure at the core, where each node corresponds to
one expression that can be evaluated. The main differences are in a number of
details that were previously implicit, though, like the simple action of
"calling a function", which in Truffle involves the interplay of, at a minimum,
five different classes.

Figure ref:truffle-arch shows the components involved in the execution of a
Truffle language. Most of our work will be in the parts labeled "AST", "AST
interpreter", and "AST rewriting". All of these involve the contents of the
classes that form the abstract syntax tree, as individual graph nodes contain
their data, but also their interpretation and rewriting specifics.

#+LABEL: truffle-arch
#+ATTR_LaTeX: :placement [htb] :scale .5
#+CAPTION: Architecture of a Truffle language, arrows denote program execution flow\protect\footnotemark
[[./img/truffle-typical.png]]

Overall, the implementation of a Truffle language can be divided into a few
categories. Some of the classes to be sub-classed and methods to be implemented
are included in parentheses to give a brief idea of the terminology we will use,
although we will expand on each one momentarily. These blocks are:

#+latex: \footnotetext{Source: Graal: High Performance Compilation for Managed Languages \cite{graal_pldi}}

- language execution (~Launcher~),
- language registration (~Language~, ~Context~, ~ParsingRequest~),
- program entry point (~RootNode~, ~CallTarget~),
- node execution (~VirtualFrame~, ~execute~, ~call~),
- node specialization (~Specialization~, ~Profile~, ~Assumption~),
- value types (~TypeSystem~, ~ValueType~),
- compiler directives (~transferToInterpreter~, ~TruffleBoundary~),
- function calls (~InvokeNode~, ~DispatchNode~, ~CallNode~),
- object model (~Layout~, ~Shape~, ~Object~), and
- others (instrumentation, ~TruffleLibrary~ interfaces, threads).

**** Launcher
The entry point to a Truffle language is a ~Launcher~ (Listing
ref:truffle-launcher). This component handles processing command-line arguments,
and uses them to build a language execution context. A language can be executed
from Java directly without a ~Launcher~, but it handles all GraalVM-specific
options and switches, many of which we will use later, and correctly builds a
language execution environment, including all debugging and other tools that
the user may decide to use.

#+label:truffle-launcher
#+caption:A minimal language \texttt{Launcher}
#+attr_latex: :position [htb]
#+begin_src kotlin
class MontunoLauncher : AbstractLanguageLauncher() {
  companion object {
    @JvmStatic fun main(args: Array<String>) = Launcher().launch(args)
  }
  override fun getDefaultLanguages() = arrayOf("montuno");
  override fun launch(contextBuilder: Context.Builder) {
    contextBuilder.arguments(getLanguageId(), programArgs)
    Context context = contextBuilder.build()
    Source src = Source.newBuilder(getLanguageId(), file).build()
    Value returnVal = context.eval(src)
    return returnVal.execute().asInt()
  }
}
#+end_src

**** Language registration
The programming language is represented by a ~Language~ object, whose primary
purpose is to answer \texttt{ParsingRequest}s with the corresponding program
graphs, and to manage execution \texttt{Context}s that contain global state of a
single language process. It also specifies general language properties like
support for multi-threading, or the MIME type and file extension, and decides
which functions and objects are exposed to other Truffle languages.

#+label:truffle-reg
#+caption:A minimal \texttt{Language} registration
#+attr_latex: :position [htb]
#+begin_src kotlin
  @TruffleLanguage.Registration(
    id = "montuno", defaultMimeType = "application/x-montuno"
  )
  class Language : TruffleLanguage<MontunoContext>() {
    override fun createContext(env: Env) = MontunoContext(this)
    override fun parse(request: ParsingRequest): CallTarget {
      CompilerAsserts.neverPartOfCompilation()
      val root = ProgramRootNode(parse(request.source))
      return Truffle.getRuntime().createCallTarget(root)
    }
  }
#+end_src

**** Program entry point
Listing ref:truffle-reg demonstrates both a language registration and the
creation of a ~CallTarget~. A call target represents the general concept of a
/callable object/, be it a function or a program, and as we will see later, a
single call to a call target corresponds to a single stack ~VirtualFrame~. It
points to the ~RootNode~ at the entry point of a program graph, as shown in Figure
ref:truffle-interop.

A ~CallTarget~ is also the basic optimization unit of Truffle: the runtime tracks
how many times a ~CallTarget~ was entered (called), and triggers optimization
(partial evaluation) of the program graph as soon as a threshold is reached.

**** Node execution
#+LABEL: truffle-interop
#+ATTR_LaTeX: :position [htb]
#+CAPTION: Combination of regular and partially-evaluated code\protect\footnotemark
[[./img/truffle-interop.png]]
#+latex: \footnotetext{Source: Graal: High Performance Compilation for Managed Languages \cite{graal_pldi}}

A ~RootNode~ is a special case of a Truffle ~Node~, the basic building block of the
program graph. Each node has a single way of evaluating the expression it
represents, the ~execute~ method. We may see nodes with multiple ~execute~ methods
later, but they are all ultimately translated by the Truffle DSL processor into
a single method: Truffle will pick the most appropriate one based on the
methods' return type, arguments types, or user-provided /guard/ expressions.

Listing ref:add-lang contains an example of two nodes. They share a parent
class, ~LanguageNode~, whose only method is the most general version of ~execute~:
one that takes a virtual frame and returns anything. An ~IntLiteralNode~ has only
one way of providing a result, it returns the literal value it
contains. ~AddNode~, on the other hand, can add either integers or strings, so it
uses another Truffle DSL option, a ~@Specialization~ annotation, which then
generates the appropriate logic for choosing between the methods ~addInt~,
~addString~, and ~typeError~.

#+label: add-lang
#+caption: Addition with type specialization
#+attr_latex: :position [htb]
#+begin_src kotlin
  abstract class LanguageNode : Node() {
    abstract fun execute(frame: VirtualFrame): Any
  }
  class IntLiteralNode(private val value: Long) : LanguageNode() {
    override fun execute(frame: VirtualFrame): Any = value
  }
  abstract class AddNode(
    @Child val left: LanguageNode, @Child val right: LanguageNode,
  ) : LanguageNode() {
    @Specialization fun addInt(l: Int, r: Int) = l + r
    @Specialization fun addString(l: String, r: String) = l + r
    @Fallback fun typeError(l: Any?, r: Any?): Unit
      = throw TruffleException("type error")
  }
#+end_src

**** Specialization
Node specialization is one of the main optimization capabilities of Truffle. The
~AddNode~ in Listing ref:add-lang can handle strings and integers both, but if it
only ever receives integers, it does not need to check whether its arguments are
strings on the /fast path/ (the currently optimized path). Using node
specialization, the ~AddNode~ can be in one of four states: uninitialized,
integers-only, strings-only, and both generic. Whenever it encounters a
different combination of arguments, a specialization is /activated/. Overall, the
states of a node form a directed acyclic graph: a node can only ever become more
general, as the Truffle documentation emphasizes.

#+LABEL: truffle-deopt
#+ATTR_LaTeX: :placement [htb]
#+CAPTION: Node optimization and deoptimization in Truffle\protect\footnotemark
[[./img/truffle-deopt.png]]
#+latex: \footnotetext{Source: One VM to Rule Them All \cite{wurthinger13_graal}}

**** (De)optimization
Node specialization combined with the optimization of a ~CallTarget~ when called
enough times are sufficient to demonstrate the process of JIT compilation in
Truffle. Figure ref:truffle-deopt demonstrates this process on a node type with
several more state transitions. When all nodes in a program graph reach a stable
state where no more specializations take place, it is may be partially
evaluated. This produces efficient machine code instead of slow
interpreter-based code, specialized for the nodes' current states.

However, this compilation is /speculative/, it assumes that nodes will not
encounter different values, and this is encoded in explicit /assumption/ objects.
When these assumptions are invalidated, the compiled machine code is discarded,
and the nodes revert back to their non-optimized form. This process is called
/deoptimization/ cite:wimmer17_deoptimization, and can be explicitly invoked using
the Truffle method ~transferToInterpreter~.

After a deoptimization, the states of nodes should again stabilize, so that they
may be partially evaluated into efficient machine code once more. Often, this
(de)optimization process repeats multiple times during the execution of a single
program: the period from the start of a program until a stable state is called
the /warm-up/ phase.

**** Value types
Nodes can be specialized based on various criteria, but the above-mentioned
specialization with regards to the type of arguments requires that these types
are all declared and aggregated into a ~TypeSystem~ object and annotation. These
are again processed by Truffle DSL into a class that can check the type of a
value (~isUnit~), and perform implicit conversion between them (~asBoolean~,
~castLong~). Listing ref:truffle-value demonstrates a ~TypeSystem~ with a custom
type ~Unit~ and the corresponding required ~TypeCheck~, and with an implicit
type-cast in which an integer is implicitly convertible into a long integer.

#+label:truffle-value
#+caption: A \texttt{TypeSystem} with an implicit cast and a custom type
#+begin_src kotlin
@CompilerDirectives.ValueType
object Unit

@TypeSystem(Unit::class, Boolean::class, Int::class, Long::class)
open class Types {
  companion object {
    @ImplicitCast
    fun castLong(value: Int): Long = value.toLong()
    @TypeCheck(Unit::class)
    fun isUnit(value: Any): Boolean = value === Unit
  }
}
#+end_src
#+latex: \vspace*{-.5cm}

**** Function invocation
An important part of the implementation of any Truffle language consists of
handling function calls. A common approach in multiple Truffle is as follows:
Given an expression like ~fibonacci(5)~. This expression is evaluated in multiple
steps: an ~InvokeNode~ resolves the function that the expression refers to
(~fibonacci~) into a ~CallTarget~, and evaluates its arguments (~5~). A ~DispatchNode~
creates a ~CallNode~ for the specific ~CallTarget~ and stores it in a
cache. Finally, a ~CallNode~ is what actually performs the switch from one part of
the program graph to another, building a stack ~Frame~ with the function's
arguments, and entering the ~RootNode~ referred to by the ~CallTarget~.

**** Stack frames
\texttt{Frame}s were mentioned several times already: they are Truffle's
abstraction of a stack frame. In general, stack frames contain variables and
values in the local scope of a function, those that were passed as its arguments
and those declared in its body. In Truffle, this is encoded as a ~Frame~ object,
and passed as an argument to all ~execute~ functions. Frame layout is set by
a ~FrameDescriptor~ object, which contains \texttt{FrameSlot}s that refer to parts
of the frame. Listing ref:truffle-frame demonstrates two nodes that interact
with a ~Frame~: a reference to a local variable, and a local variable declaration.

#+label:truffle-frame
#+caption: Basic operations with a \texttt{Frame}
#+attr_latex: :position [!htb]
#+begin_src kotlin
class ReadLocalVarNode(val name: String) : Node {
  fun execute(frame: VirtualFrame): Any {
    val slot: FrameSlot = frame.getFrameDescriptor().findFrameSlot(name)
    return frame.getValue(slot ?: throw TruffleException("not found"));
} }
class WriteLocalVarNode(val name: String, val body: Node) : Node {
  fun execute(frame: VirtualFrame): Unit {
    val slot: FrameSlot = frame.getFrameDescriptor().addFrameSlot(name)
    frame.setObject(slot, body.execute(frame));
} }
#+end_src

There are two kinds of a ~Frame~, virtual and materialized frames. A ~VirtualFrame~
is, as its name suggests, virtual, and its values can be freely optimized
by Truffle, reorganized, or even passed directly in registers without being
allocated on the heap (using a technique called Partial Escape Analysis). A
~MaterializedFrame~ is not virtual, it is an object at the runtime of a program,
and it can be stored in program's values or nodes. A virtual frame is preferable
in almost all cases, but e.g., implementing closures requires a materialized
frame, as it needs to be stored in a ~Closure~ object. This is shown in Listing
ref:truffle-closure, where ~frame.materialize()~ captures a virtual frame and
stores it in a closure.

#+label:truffle-closure
#+caption: A closure value with a \texttt{MaterializedFrame}
#+attr_latex: :position [htb]
#+begin_src kotlin
  @CompilerDirectives.ValueType
  data class Closure(
    val callTarget: CallTarget,
    val frame: MaterializedFrame,
  )
  class ClosureNode(val ct: CallTarget) : Node {
    fun executeClosure(frame: VirtualFrame) {
      return Closure(ct, frame.materialize())
    }
  }
#+end_src

**** Caching
These were the main features required for writing a Truffle language, but there
are several more tools for their optimization, the first one being /inline
caching/. This is an old concept that originated in dynamic languages, where it
is impossible to statically determine the call target in a function invocation,
so it is looked up at runtime. Most function call sites use only a limited
number of call targets, so these can be cached. As the cache is a local one,
placed at the call site itself, it is called an /inline cache/. This concept is
used for a number of other purposes, e.g., caching the ~FrameSlot~ in an
assignment operator, or the ~Property~ slot in an object access operation.

In the case of function dispatch, a ~DispatchNode~ goes through the following
stages: /uninitialized/; /monomorphic/, when it is specialized to a single call
target; /polymorphic/, when it stores a number of call targets small enough that
the cost of searching the cache is smaller than the cost of function lookup; and
/megamorphic/, when the number of call targets exceeds the size of the cache, and
every function call is looked up again. Listing ref:truffle-cache demonstrates
this on a ~DispatchNode~, adding a polymorphic cache with size 3, and also
demonstrates the Truffle DSL annotations ~Cached~. The cache key is the provided
~CallTarget~, based on which a ~DirectCallNode~ is created and cached as well. The
megamorphic case uses an ~IndirectCallNode~: in a ~DirectCallNode~, the call target
can be inlined by the JIT compiler, whereas in the indirect version it can not.

#+label: truffle-cache
#+caption: Polymorphic and megamorphic inline cache on a \texttt{DispatchNode}
#+begin_src kotlin
abstract class DispatchNode : Node {
  abstract fun executeDispatch(
    frame: VirtualFrame, callTarget: CallTarget, args: Array<Any>): Any

  @Specialization(limit="3", guards="callTarget == cachedCallTarget")
  fun doDirect(
    frame: VirtualFrame, callTarget: CallTarget, args: Array<Any>,
    @Cached("callTarget") cachedCallTarget: CallTarget,
    @Cached("create(cachedCallTarget)") callNode: DirectCallNode
  ) = callNode.call(args)

  @Specialization(replaces="doDirect")
  fun doIndirect(
    frame: VirtualFrame, callTarget: CallTarget, args: Array<Any>,
    @Cached("create()") callNode: IndirectCallNode
  ) = callNode.call(callTarget, args)
}
#+end_src

**** Guards
Listing ref:truffle-cache also demonstrates another optimization feature, a
generalization of nodes specializing themselves based on types or arguments. A
~Specialization~ annotation can have arbitrary user-provided /guards/. These are
often used in tandem with a cache, or with complex type specializations. In
general, using a ~Specialization~ makes it possible to choose the most optimal
node implementation based on its situation or configuration.

**** Profiles
Another tool for optimization are /profiles/. These are objects that the developer
can use to track whether a conditional statement was executed: in the
implementation of an ~if~ statement, or when handling an exception. The compiler
will use the information collected during optimization, e.g., when a
~ConditionProfile~ tracks that the condition in an ~if~ statement was true every
time, the compiler will omit the ~else~ branch during compilation.

**** Assumptions
/Assumptions/ are the last tool that a developer can use to provide more
information to the compiler. Unlike profiles and specializations that are local
to a node, assumptions are global objects whose value can be changed from any
part of a program graph. An assumption is /valid/ when created, and it can be
/invalidated/, which triggers deoptimization of any code that
relies on it. A typical use of assumptions is shown in Listing ref:truffle-assume
cite:shopify2020, where TruffleRuby relies on the fact that global variables
are only seldom changed and can be cached. A ~ReadGlobalVarNode~ reads the value
of the global variable only the first time, and relies on two assumptions
afterwards, which are invalidated whenever the value of the variable changes,
and the cached value is discarded.

#+label:truffle-assume
#+caption:Cached reading of a global variable using assumptions \cite{shopify2020}
#+begin_src kotlin
@Specialization(assumptions = [
  "storage.getUnchangedAssumption()",
  "storage.getValidAssumption()"
])
fun readConstant(
  @Cached("getStorage()") storage: GlobalVariableStorage,
  @Cached("storage.getValue()") value: Any
) = value
#+end_src

**** Inlining
During optimization, the Graal compiler replaces \texttt{DirectCallNode}s with
the contents of the call target they refer to, performing function /inlining/
cite:wurthinger17_partial_eval. Often, this is the optimization with the most
impact, as replacing a function call with the body of the callee means that many
other optimizations can be applied. For example, if a ~for~ loop contains only a
function call and the function is inlined, then the optimizer could further
analyze the data flow, and potentially either reduce the loop to a constant, or
to a vector instruction.

There are potential drawbacks, and Truffle documentation warns developers to
place ~TruffleBoundary~ annotations on functions that would be expanded to large
program graphs, like ~printf~, as Graal will not ever inline a function through a
~TruffleBoundary~.

**** Splitting
Related to inlining, a call target can also be /split/ into a number of
/monomorphic/ call targets. Previously, we saw an ~AddNode~ that could add either
integers or strings. If this was a global or built-in function that was called
from different places with different configurations of arguments, then this node
could be split into two: one that only handles integers and one for
strings. Only the monomorphic version would then be inlined at a call site,
leading to even better possibility of optimizations.

Both of these two techniques, inlining and splitting, are guided by Graal
heuristics, and they are generally one of the last optimization techniques to be
checked when there are no more gains to be gained from caching or
specializations.

**** Object model
#+label:frames
#+caption:Accessing an object property using a \texttt{Shape} and a \texttt{Property} \cite{vergu19_scopes}
#+begin_src kotlin
@Specialization(guards=[
  "addr.key() == keyCached",
  "shapeCached.check(addr.frame())"
], limit="20")
fun doSetCached(
  addr: FrameAddr, value: Any,
  @Cached("addr.key()") keyCached: Occurrence,
  @Cached("addr.frame().getShape()") shapeCached: Shape,
  @Cached("shapeCached.getProperty(keyCached)") slotProperty: Property
): Unit {
  slotProperty.set(addr.frame(), value, shapeCached)
}
#+end_src

Truffle has a standard way of structuring data with fixed layout, called the
Object Storage Model cite:grimmer15_polyglot. It is primarily intended for class
instances that have a user-defined data layout, but e.g., the meta-interpreter
project DynSem cite:vergu19_scopes uses it for variable scopes, and TruffleRuby
uses it to make C \texttt{struct}s accessible from Ruby as if they were
objects. Similar to \texttt{Frame}s, an empty ~DynamicObject~ is instantiated from
a ~Shape~ (corresponds to a ~FrameDescriptor~) that contains several instances of a
~Property~ (corresponds to a ~FrameSlot~). Listing ref:frames shows the main method
of a node that accesses an object property, also utilizing a polymorphic cache.

**** Interop
As previously mentioned, it is possible to evaluate /foreign/ code from other
languages using functions like /eval/, referred to as /polyglot/. However, Truffle
also makes it possible to use other languages' /values/: to define a foreign
function and use it in the original language, to import a library from a
different language and use it as if it was native. This is referred to as an
interoperability message protocol or /interop/, for short.

Truffle uses /libraries/ to accomplish this. They play a role similar to
/interfaces/ in object-oriented languages cite:grimmer15_polyglot, and describe
capabilities of \texttt{ValueType}s. A library /message/ is an operation that a
value type can support, and it is implemented as a special node in the program
graph, as a nested class inside the value type. The \texttt{ValueType}s of a
foreign language then need to be mapped based on these libraries into a
language: a value that implements an ~ArrayLibrary~ can be accessed using array
syntax, see Listing ref:libraries. Libraries are also used for polymorphic
operations inside a language if there is a large amount of value types, to
remove duplicate code that would otherwise be spread over multiple
\texttt{Specialization}s.

#+label:libraries
#+caption:Array access using a \texttt{Library} interface\protect\footnotemark
#+begin_src kotlin
  class ArrayReadNode : Node {
    @Specialization(guards="arrays.isArray(array)", limit="2")
    fun doDefault(
      array: Object, index: Int,
      @CachedLibrary("array") arrays: ArrayLibrary
    ): Int = arrays.read(array, index)
  }
#+end_src
#+latex: \footnotetext{Source: \url{https://www.graalvm.org/graalvm-as-a-platform/language-implementation-framework/TruffleLibraries/}}

** Mapping concepts to Truffle
**** Where to use Truffle?
Truffle uses JIT compilation, and optimizes repeatedly executed parts of a
program. Many parts of the previously implemented interpreter are only one-off
computations, though, e.g., the elaboration process itself that processes a
pre-term once and produces a corresponding term, while discarding the
pre-term. Only the evaluation of terms to values runs multiple times, as
(top-level) functions are stored in the form of terms. It is possible that the
elaboration process might benefit as well, by implementing /infer/, /check/, and
/unify/ as Truffle nodes and using those in place of functions.

**** Inspiration
For inspiration, I have looked at a number of other functional languages that
use Truffle: several theses (TruffleClojure cite:feichtinger15_clojure,
TrufflePascal cite:flimmel17_truffle_pascal, Mozart-Oz
cite:istasse17_oz_truffle), two Oracle projects (FastR cite:stadler16_fastr,
TruffleRuby cite:shopify2020), and other projects (Cadenza cite:kmett_2019,
DynSem cite:vergu19_scopes, Mumbler cite:mumbler, Truffled PureScript
cite:purescript).

In the last phases of writing this thesis, I have encountered the project Enso
cite:enso that was released April 13th, a month before my thesis deadline. It is
a visual programming language that uses dependent types at its core. In
particular, compared to my previous approach, it strictly delineates between
elaboration, compilation, and evaluation, introducing a special /compiler/
component. This allows them to apply optimizations in the spirit of a compiled
language, gradually performing optimization /passes/, and refining the code that
is transformed into a Truffle graph in the end.

After comparing their approach to mine, I have found theirs significantly more
viable with regards to optimization: my original approach was to transform the
entire elaboration algorithm into a Truffle graph containing nodes like
/QuoteNode/, /InferNode/, etc. The entire system contained a large amount of
components, and the program graph did not usually manage to stabilize enough
that it would be compiled.

I have adopted this separation of elaboration and evaluation, meaning that only
/closures/ are compiled into machine code. This significantly simplifies both the
implementation and any optimization efforts (profiling and reading of Truffle
graphs), and this is the approach that I will describe in this section. This
approach is very similar to the implementations of other proof assistants that
also use an evaluation platform other than the host language for evaluation,
e.g., Coq supports several backends (~native_compute~, ~vm_compute~),

*** Approach
Out of the many changes that are required, the largest one is in the encoding of
functions and closures, and replacing closure implementation with call
targets. Environments and variable references need to be rewritten to use
explicit stack frames, and lazy evaluation cannot use Kotlin's ~lazy~ abstraction,
but instead needs to be encoded as an explicit ~Thunk~ object.

#+LABEL:flow-truffle
#+CAPTION: Component overview of the Truffle interpreter
#+ATTR_LATEX: :options [!htb]
#+begin_figure latex
\centering
\begin{tikzpicture}[
  line/.style={-latex},
  block/.style={draw,thick,text width=2cm,minimum height=1cm,align=center}
]
\node[block](repl){REPL};
\node[block,below=.5cm of repl](cli){CLI};
\node[block,below=.5cm of cli](file){File};
\node[block,right=of cli](driver){Launcher\\Language};
\node[block,above right=0 and 1cm of driver](elab){Elaboration};
\node[block,below=of elab](eval){Evaluation};
\node[block,right=4cm of driver](truffle){Truffle};

\node[draw,inner xsep=3mm,inner ysep=2mm,fit=(repl)(cli)(file),label={90:Frontend}](front){};
\node[draw,inner xsep=3mm,inner ysep=2mm,fit=(elab)(eval)(truffle),label={90:Backend}](back){};

\draw[line] (repl.east) to (driver);
\draw[line] (cli.east) to (driver);
\draw[line] (file.east) to (driver);
\draw[line] (driver) to[bend right=10] (elab);
\draw[line] (elab) to[bend right=10] (driver);
\draw[line] (eval) to[bend right=10] (elab);
\draw[line] (elab) to[bend right=10] (eval);
\draw[line] (eval) to[bend right=10] (truffle.south);
\draw[line] (truffle.north) to[bend right=10] (elab);
\end{tikzpicture}
#+end_figure

Figure ref:flow-truffle demonstrates the components of the new interpreter. The
~Launcher~ is the same as in the previous interpreter, only now we use the
~Context~ that it prepares based on user-provided options. The ~Language~
object initializes a different ~Context~ object, a ~MontunoContext~, which is an
internal object containing the top-level variable scope, the meta-variable
scope, and other global state variables. ~Language~ then dispatches parsing
requests to the parser, and the pre-terms it produces are then wrapped into a
~ProgramRootNode~.

Executing the ~ProgramRootNode~ starts the elaboration process, calling into the
code of the original interpreter. During evaluation of a term into a value, any
closures produced are compiled into a Truffle graph, saving the current
environment in the form of an array. The closure is then evaluated when supplied
with an argument into a value, which can again be compared, unified, or built
back up into a ~Term~ using ~quote~.

Elaboration and evaluation both access the ~MontunoContext~ object to resolve
top-level variables and meta-variables into the corresponding \texttt{Term}s.
The REPL needs to obtain lists of bound variables to produce suggestions and
process other REPL commands from the context as well, but all interaction
between the host code of the ~Launcher~ and the internals of the language need to
happen via the polyglot interface. This means that any REPL commands that need
to access the language context now need to be implemented as language pragmas:
e.g., to reset the language state, we now need the pragma ~{-# RESET #-}~, which
is then also accessible in user-provided code as well.

The data flow in Figure ref:dataflow-truffle, if compared with the previous data
flow diagram, only adds the data representation /Code/, and the operation /Close/,
which represents the construction of a closure that /closes/ over the current
environment.  Otherwise, other parts of the system can stay the same, at least
on first glance.

#+LABEL:dataflow-truffle
#+CAPTION: Data flow overview. Cyan is elaboration, red normalization-by-evaluation
#+ATTR_LATEX: :options [!htb]
#+begin_figure latex
\centering
\begin{tikzpicture}[
  line/.style={-latex},
  elabl/.style={-latex,draw=cyan},
  elabn/.style={-latex,color=cyan},
  evall/.style={-latex,draw=red},
  evaln/.style={-latex,color=red},
  block/.style={draw,thick,text width=2cm,minimum height=1cm,align=center}
]
\node[block](s){String};
\node[block,right=1.5cm of s](p){Pre-term};
\node[block,right=1.5cm of p](t){Term};
\node[block,right=1.5cm of t](v){Value};
\node[block,above=1cm of v](c){Code};

\draw[line] (s) to node[midway,above]{Parse} (p);
\draw[elabl] (p) to node[elabn,midway,above]{Infer} node[elabn,midway,below]{Check} (t);
\draw[evall] (t) to[bend left=10] node[evaln,midway,above]{Eval} (v);
\draw[evall] (t) to[bend left=10] node[evaln,midway,above left]{Close} (c);
\draw[evall] (c) to[bend left=10] node[evaln,midway,right]{Eval} (v);
\draw[evall] (v) to[bend left=10] node[evaln,midway,below]{Quote} (t);
\draw[line] (t) to[bend left=20] node[midway,below]{Pretty-print} (s);
\draw[elabl] (v) to[loop right] node[elabn,midway,left]{Unify} (v);
\end{tikzpicture}
#+end_figure

*** Implementation notes
While I have sketched the changes that would be required when creating a new
Truffle interpreter based on the previous, non-Truffle one, the actual
implementation process was slightly different. Instead of creating a entirely
separate interpreter that would share some library code, as with my first
attempts at creating a Truffle interpreter for Montuno, the effort to
incorporate the improvements of Enso resulted in the two codebases merging.

Unlike my previous implementations, the non-Truffle and Truffle versions share
the entire elaboration component. The only difference is in a single pluggable
component, a ~Compiler~. This is an interface with a single method, ~buildClosure~,
that produces the original ~VClosure~ value in the non-Truffle version, and in the
Truffle version it transforms the body of a closure into a Truffle graph.

*** Values
#+label:truffle-typesystem
#+caption:A \texttt{TypeSystem} and two simple \texttt{ValueType}s from the Truffle interpreter
#+attr_latex: :position [htb]
#+begin_src kotlin
@TypeSystem(
  VUnit::class, VThunk::class,
  VLam::class, VPi::class,
  VPair::class, VSg::class,
  VMeta::class, VLocal::class, VTop::class,
  VNat::class, VBool::class,
)
class Types {
  @TypeCheck(VUnit::class)
  fun isVUnit(value: Any) = value === VUnit
}

@ValueType
object VUnit : TruffleObject
@ValueType
class VPair(val left: Any, val right: Any) : TruffleObject
#+end_src

Not including constants, we have only two main value types: a Π-type (equivalent
to a λ-abstraction), and a Σ-type. A Π-type maps onto a closure and will be
discussed momentarily. A Σ-type can be expressed as a pair, or a linked list of
nested pairs, to use the simplest representation. Then there are neutral terms,
unresolved variables that accumulate a spine of unapplied operands and
projections: these are expressed as a head containing a variable reference,
and a spine with an array of spine values.

Each of these values needs to be a separate ~ValueType~ class, and an entry in the
Truffle type system. A snippet in Listing ref:truffle-typesystem shows the
~TypeSystem~ and two simple value types. Other than the above-mentioned types of
values, there is a number of literal types, and a type ~VThunk~. This type needs
to be explicitly mentioned here, so that we can implement lazy evaluation in
Truffle; its interface is exactly the same as a Kotlin ~lazy~, but a ~VThunk~
inherits from the class ~TruffleObject~ so that it can be used inside compiled
code.

*** Closures
#+label:truffle-closure-sketch
#+caption: Sketch of the closure implementation
#+attr_latex: :position [htb]
#+begin_src kotlin
  @CompilerDirectives.ValueType
  @ExportLibrary(InteropLibrary::class)
  data class TruffleClosure(
      val env: VEnv, val callTarget: CallTarget
  ) : TruffleObject {
    override val arity: Int = 1
    @ExportMessage fun isExecutable() = true
    @ExportMessage override fun execute(vararg args: Any?): Val {
      return callTarget.call(*concat(env, args))
    }
  }
#+end_src

A closure needs to store the function to execute, which was a ~Term~ in the
non-Truffle implementation, together with the execution environment. The Truffle
version replaces the ~Term~ with a ~CallTarget~ that points to a Truffle graph.  A
~CallTarget~ can only be called with an array of objects, so this is what a
~Closure~ stores in the place of an environment. The ~CallTarget~ points to a
~ClosureRootNode~, which first copies the array of arguments it was given into the
local scope, and then executes the body. Reports from the Cadenza project
cite:kmett_2019 show that this is more efficient than simply accessing the
environment as an array, as the virtual frame can be optimized by Truffle. The
closure object itself can be seen in Listing
ref:truffle-closure-sketch. Notably, it also uses the ~InteropLibrary~ mentioned
in the previous chapter, meaning that it can also be invoked from outside of the
interpreter internals.

Changing the definition of a closure also means that all ~VPi~ or ~VLam~ values now
contain a closure that was processed by the previously mentioned ~Compiler~
component. My original intent was to compile only top-level definitions
(globally defined functions or constants) and meta-variables, but the processes
of meta-variable solving and elaborating a top-level definition both produce a
value, so it is simpler to produce closures globally, across all the entire
interpreter.

A ~Term~ is processed by the ~Compiler~ into a ~Code~ object, which is then wrapped
into the ~ClosureRootNode~, and converted into a ~CallTarget~. The compilation
process produces the Truffle graph as another tree structure, only this one will
be interpreted by Truffle and not our algorithms. A snippet of the compilation
code can be seen in Listing ref:truffle-compiler. It converts the ~Term~ type into
a version of the ~eval~ function: a Π-type produces a closure, therefore we need
to start a new compilation process with its body. a local variable reads from
the ~Frame~, therefore we find the ~FrameSlot~ that corresponds to the de Bruijn
index, and compile it into a /read/ operation. Finally, a unit term is compiled
into a constant expression.

#+label:truffle-compiler
#+caption: Snippet from the \texttt{Compiler}
#+attr_latex: :position [htb]
#+begin_src kotlin
  class TruffleCompiler(val ctx: MontunoContext) : Compiler() {
    override fun buildClosure(t: Term, env: VEnv): Closure {
      val fd = FrameDescriptor()
      val code = compileTerm(bodyTerm, Lvl(env.size + 1))
      val root = TruffleRootNode(code, ctx.top.lang, fd)
      return TruffleClosure(ctx, env, root.callTarget)
    }
    private fun compileTerm(
      t: Term, depth: Lvl, fd: FrameDescriptor
    ): Code = when (t) {
      is TPi -> CClosure(t.name, t.type,
                         compileTerm(t.bound, depth, fd),
                         compileClosure(body, depth + 1))
      is TApp -> CApp(compileTerm(t.l, depth, fd),
                      compileTerm(t.r, depth, fd))
      is TLocal -> CReadLocal(fd.findFrameSlot(t.ix.toLvl(depth).it))
      is TMeta -> CDerefMeta(t.slot)
      TUnit -> CConstant(VUnit)
    } //...
  }
#+end_src

*** Elaboration
Given the previously mentioned approach, changing the elaboration would not be
necessary at all. However, in the course of building the second version, I had
attempted to perform type-directed optimization, meaning that to optimize a term
well, I needed its type. In the course of performing this change, I had changed
the meta-context from untyped to typed, meaning that each meta-variable now has
a type that the solution needs to conform to. This means that all binders in
terms and values can now store the type of their argument, which allows the
optimization process to use this information.

The actual implementation of meta-variables, as used by the compiler is shown in
Figure ref:truffle-metas. The Truffle graph node stores a reference to the
meta-variable. If it has been solved between the time of the compilation and
execution, this node is replaced with the value of the solution. If it has not,
then this node produces a neutral value with an empty spine.

#+label:truffle-metas
#+caption:Meta-variable reference node in Truffle
#+attr_latex: :position [htb]
#+begin_src kotlin
open class CDerefMeta(val slot: MetaEntry) : Code() {
  override fun execute(frame: VirtualFrame): Val = when {
    !slot.solved -> VMeta(slot.meta, VSpine(), slot)
    else -> {
      replace(CConstant(slot.value!!, loc))
      slot.value!!
    }
  }
}
#+end_src

*** Normalization
Given the approach taken, there are no actual changes to the normalization
algorithm, only the references to closures in Π- and λ-terms have been changed
to be an opaque interface, which offers only a single operation ~instantiate~.
The non-Truffle implementation needed to slightly change as well to conform to
this interface. The interface and the non-Truffle implementation of a closure
are demonstrated in Listing ref:truffle-metas.

#+label:truffle-metas
#+caption:Non-Truffle closure implementation
#+attr_latex: :position [htb]
#+begin_src kotlin
  interface Closure {
    fun inst(v: Val): Val
  }
  @CompilerDirectives.ValueType
  @ExportLibrary(InteropLibrary::class)
  data class PureClosure(val env: VEnv, val body: Term) : Closure {
    override val arity: Int = 1
    @ExportMessage fun isExecutable() = true
    @ExportMessage override fun execute(vararg args: Any?): Val
      = body.eval(concat(env, args))
  }
#+end_src

*** Built-ins
Built-in constants and types need to be implemented as special nodes. The
resolution of a built-in name to its corresponding node happens during
compilation. Each built-in term has its arity, the number of expected arguments.
The compiler produces the correct number of closures (~VLam~) nodes around the
built-in node invocation that corresponds to the arity of the operation. These
are passed to a ~BuiltinRootNode~ that uses them directly, unlike the
~ClosureRootNode~, that first copies them to the local scope

This is shown on the example of a ~Succ~ node in Listing ref:truffle-succ. This
node has the arity 1, it expects a single argument, which can be either an
already evaluated integer or a ~Thunk~ that will produce an integer, which is then
/forced/, and coerced to a integer using a function generated by the ~TypeSystem~.

#+label:truffle-succ
#+caption:Example of a built-in node, a \texttt{Succ} node
#+attr_latex: :position [htb]
#+begin_src kotlin
class Succ : BuiltinTerm(1) {
  @Specialization
  fun doInt(n: Int) = n + 1
  @Specialization
  fun doThunk(t: Thunk) = TypesGen.asVNat(t.force()) + 1
}
#+end_src

*** Driver
The previously-mentioned decision to share elaboration code was also partly
motivated by the experience of working on the original Truffle code base: while
it is possible to pause the execution of a Truffle program, and inspect it
inside a debugger, it is impossible to access the language context from outside
the language code. This made any attempts at incremental development using unit
tests impossible, as I was only able to test the complete process by providing
input from the outside.

While this also means that the polyglot interface of the language is
well-tested, it discouraged me from trying to adapt the existing Truffle
codebase when attempting to integrate the concepts from Enso. However, this
limitation places additional demands on the driver code that handles user
interaction. Each user command that is impossible to express using the polyglot
interface needs to be implemented as an interpreter directive, a pragma. As
processing of parsed input happens in the course of elaboration, this means that
the function ~checkTopLevel~ not only handles elaboration of top-level
definitions, but also these commands. They are parsed into ~RTerm~ nodes, as the
same mechanism is used for pragmas like ~NORMALIZE~, that expect a well-formed
expression. An snippet of the code that processes these commands is shown in
Listing ref:truffle-symbols.

#+label:truffle-symbols
#+caption:Processing REPL commands inside elaboration
#+attr_latex: :position [htb]
#+begin_src kotlin
  fun checkTopLevel(top: MontunoContext, e: TopLevel): Any? {
    top.metas.newMetaBlock()
    val ctx = LocalContext(top, LocalEnv(top.ntbl))
    return when (e) {
      // ...
      is RTerm -> when (e.cmd) {
        Pragma.NOTHING -> top.pretty(ctx.infer(e.tm))
        Pragma.RESET -> { top.reset(); null }
        Pragma.SYMBOLS -> top.getSymbols()
        Pragma.BUILTIN -> { top.registerBuiltins(e.loc, e.tm); null }
        Pragma.PRINT -> top.printElaborated()
      }
    }
  }
#+end_src

One other benefit of the two interpreters sharing their code base is that
switching between backends is possible by simply issuing a command in the REPL
interface, as demonstrated in Listing ref:repl-engine. Extending the compiler
backend with different or better optimizations passes, in the manner of Montuno,
would be possible by simply extending the ~Compiler~ interface. Any algorithmic
improvements to the elaboration process would then be shared between all
implementations.

#+label:repl-engine
#+caption:Switching compilation engines in REPL
#+attr_latex: :position [htb]
#+begin_src text
  $> montuno --truffle
  Mt> :engine
  montuno-truffle
  Mt> id : {A} -> A -> A = \x.x
  Mt> :normalize id id
  λ x. x
  Mt> :engine montuno-pure
  Mt> id : {A} -> A -> A = \x.x
  Mt> :normalize id id
  λ x. x
#+end_src

* Bibliography                                                :ignoreheading:
bibliographystyle:abbrv
bibliography:bibliography.bib
* Footnotes
